=============================================================================================================
üîπ Q1. What is the difference between process and thread? Why is Java considered a multi-threaded language?
=============================================================================================================
üéØDifference Between Process and Thread
	üîπ Process: A process is an independent program in execution with its own memory space and system resources.
	üîπ Thread: A thread is a lightweight execution unit inside a process that shares the process‚Äôs memory and resources.
	
	üîß Code Perspective
		JVM Process
		 ‚îú‚îÄ‚îÄ Main Thread
		 ‚îú‚îÄ‚îÄ GC Thread
		 ‚îú‚îÄ‚îÄ JIT Compiler Thread
		 ‚îú‚îÄ‚îÄ HTTP Worker Threads
		All these threads share heap memory but have separate stacks.

üéØWhy Is Java Considered a Multi-Threaded Language?
	üîπ Reason 1: Built-in Thread Support: 
		Java provides first-class APIs for multithreading: 
			Thread, Runnable, Callable, Executor Framework, ForkJoinPool, CompletableFuture

	üîπ Reason 2: JVM Itself Is Multi-Threaded
		Even a simple Java program creates multiple threads internally:
			public class Demo {
				public static void main(String[] args) {
					System.out.println("Hello");
				}
			}
		Behind the scenes, JVM runs: Main thread, Garbage Collector thread, JIT compiler thread, Signal handler threads
		üëâ Multithreading is intrinsic to JVM, not optional.

	üîπ Reason 3: Shared Memory Model
		Java supports shared-memory concurrency, enabling:
			Fast inter-thread communication, High-performance parallelism
			With built-in synchronization: synchronized, volatile, Locks, Atomic variables

	üîπ Reason 4: Rich Concurrency Utilities
		Java‚Äôs java.util.concurrent package provides:
			Thread pools, Concurrent collections, Lock-free algorithms, High-level async programming
		This makes Java suitable for:
			High-throughput servers, Real-time systems, Distributed systems

	üîπ Reason 5: Platform Independence: 
		Java threads map to native OS threads, so Java programs: 
			Scale across CPU cores, Use true parallelism, 	Behave consistently across platforms

=============================================================================================================
üîπ Q2. What are the benefits and drawbacks of multithreading?
=============================================================================================================
‚úÖ Benefits of Multithreading
	1Ô∏è‚É£ Better CPU Utilization & Parallelism
		Multithreading allows applications to use multiple CPU cores simultaneously, instead of wasting idle CPU time.
		üëâ Example: One thread waits for DB, Another processes data, Another handles user requests
			ExecutorService executor = Executors.newFixedThreadPool(4);
			executor.submit(() -> processOrder());
			executor.submit(() -> sendEmail());
			executor.submit(() -> generateInvoice());
		‚úî Result: Higher throughput

	2Ô∏è‚É£ Improved Application Responsiveness
		Time-consuming tasks can run in background, keeping the main flow responsive.
		üëâ Common use cases: UI applications, Web servers, Microservices, CompletableFuture.runAsync(() -> generateReport());
		‚úî Users don‚Äôt feel the delay

	3Ô∏è‚É£ Higher Throughput for Server Applications
		Web servers handle thousands of concurrent requests using thread pools.
			1 JVM Process
			 ‚îî‚îÄ‚îÄ 200+ Worker Threads
		‚úî More requests per second, Better scalability

	4Ô∏è‚É£ Efficient Resource Sharing
		Threads share: Heap memory, Open files, Network connections
		This makes inter-task communication faster than inter-process communication.
			synchronized(cache) {
				cache.put(key, value);
			}
		‚úî Less overhead compared to processes

	5Ô∏è‚É£ Better Design for Asynchronous Tasks
		Multithreading naturally fits: Producer‚ÄìConsumer, Event-driven systems, Non-blocking workflows
			BlockingQueue<Task> queue = new LinkedBlockingQueue<>();

	6Ô∏è‚É£ JVM-Level Optimization
		The JVM is optimized for concurrency: Adaptive thread scheduling, Optimized locks, 
		Lock elision & biased locking (earlier versions)
		‚úî Performance tuned by JVM itself

‚ùå Drawbacks of Multithreading: This is where senior engineers shine üëá
	1Ô∏è‚É£ Increased Complexity
		Multithreaded code is: Harder to write, Harder to reason about, Harder to debug
		üëâ Bugs are often non-deterministic.
	
	2Ô∏è‚É£ Concurrency Bugs (Very Dangerous)
		| Bug Type          | Description                     |
		| ----------------- | ------------------------------- |
		| Race condition    | Incorrect results due to timing |
		| Deadlock          | Threads wait forever            |
		| Livelock          | Threads active but no progress  |
		| Starvation        | Thread never gets CPU           |
		| Visibility issues | Stale data due to CPU cache     |

	3Ô∏è‚É£ Performance Overhead
		Threads introduce overhead: Context switching, Lock contention, Cache invalidation
		‚ö† Too many threads can degrade performance.

	4Ô∏è‚É£ Difficult Debugging & Testing
		Problems may: Appear only in production, Disappear during debugging, Depend on CPU timing
		‚úî Hard to reproduce consistently

	5Ô∏è‚É£ Risk of Data Corruption: 	Since threads share memory: One bad thread can corrupt shared state, Crash entire JVM
			sharedList.add(item); // unsafe without synchronization

	6Ô∏è‚É£ Deadlocks & Resource Contention: Improper locking can halt entire application.
			Thread A ‚Üí lock1 ‚Üí waiting for lock2  
			Thread B ‚Üí lock2 ‚Üí waiting for lock1

	7Ô∏è‚É£ Memory Issues: ThreadLocal leaks in thread pools, Stack memory per thread, Increased GC pressure
		‚ö† Too many threads = OutOfMemoryError

=============================================================================================================
üîπ Q3. Explain Thread vs Runnable. Which one do you prefer and why?
=============================================================================================================
1Ô∏è‚É£ Thread vs Runnable ‚Äî Core Difference
	| Aspect           | Thread                      | Runnable             |
	| ---------------- | --------------------------- | -------------------- |
	| Type             | Class                       | Functional interface |
	| Inheritance      | Cannot extend another class | Can extend any class |
	| Responsibility   | Task + execution            | Task only            |
	| Reusability      | Poor                        | High                 |
	| Design           | Tight coupling              | Loose coupling       |
	| Executor support | ‚ùå Not directly   	         | ‚úÖ Preferred  	    |
	| Lambda support   | ‚ùå                	         | ‚úÖ        	        |
	| Testability      | Low                         | High                 |

2Ô∏è‚É£ Why Runnable Is Better (Senior Perspective)
	‚úÖ 1. Supports Inheritance: Java supports single inheritance.
			class OrderService extends BaseService implements Runnable {
				public void run() {
					processOrder();
				}
			}
		üëâ Impossible with Thread.

	‚úÖ 2. Separation of Concerns (Very Important)
		Runnable separates: What to run ‚Üí Runnable, How to run ‚Üí Thread / Executor, This is clean OOP design.

	‚úÖ 3. Works with Executor Framework: Executors accept Runnable, not Thread.
			ExecutorService executor = Executors.newFixedThreadPool(5);
			executor.submit(() -> processOrder());
		üëâ This is how real production systems work.

	‚úÖ 4. Better Reusability
		Same Runnable can be executed by: Multiple threads, Different executors, Async frameworks

	‚úÖ 5. Lambda Friendly (Java 8+): Cleaner, concise, modern Java.
			new Thread(() -> System.out.println("Lambda Runnable")).start();

	‚úÖ 6. Easier Testing
		Runnable logic can be: Unit tested without threads, Mocked and reused

3Ô∏è‚É£ When Would You Use Thread?
	Rare, but valid cases:
		‚úî When you need to: Override thread behavior, Customize thread lifecycle, Create specialized thread types
		Example:
				class LoggingThread extends Thread {
					public void run() {
						log();
						super.run();
					}
				}																
		Even then, many teams avoid this.

=============================================================================================================
üîπ Q4. Difference between start() and run() methods.
=============================================================================================================
1Ô∏è‚É£ start() vs run() ‚Äî Core Difference
	‚úÖ start(): Creates a new thread, Tells the JVM to schedule the thread, Internally calls run() in a new call stack
	‚úÖ run(): Contains the task logic, Executes like a normal method call, Runs in the current thread

2Ô∏è‚É£ Code Example (Most Important): üëâ This single example is enough to explain the entire concept in an interview.
		class Demo extends Thread {
			public void run() {
				System.out.println(Thread.currentThread().getName());
			}

			public static void main(String[] args) {
				Demo t = new Demo();

				t.run();   // ‚ùå NOT a new thread
				t.start(); // ‚úÖ New thread created
			}
		}
	üîç Output: 
		main
		Thread-0

3Ô∏è‚É£ What Happens Internally?
	‚ñ∂ When start() is called: 
		JVM creates a new native thread, Allocates a new call stack
		Registers thread with OS scheduler
		JVM invokes run() asynchronously
				main thread stack
				new thread stack ‚Üí run()

	‚ñ∂ When run() is called: No new thread, No new stack, Just a normal method call
		main thread stack ‚Üí run()

4Ô∏è‚É£ Why You Should Never Call run() Directly
	new Thread(task).run(); // ‚ùå common beginner mistake
	Problems: No parallel execution, No concurrency, Misleading code, Performance bugs in production

5Ô∏è‚É£ Why Can‚Äôt We Call start() Twice?
		Thread t = new Thread();
		t.start();
		t.start(); // ‚ùå IllegalThreadStateException

	‚ñ∂ Reason: A thread‚Äôs lifecycle can start only once, JVM does not allow reusing terminated threads


=============================================================================================================
üîπ Q5. What are daemon threads? Give real-world use cases.
=============================================================================================================
1Ô∏è‚É£ What Are Daemon Threads?
	A daemon thread is a background service thread in Java that:
		Runs in the background, Provides supporting services to user threads, Does not prevent JVM from exiting
		üëâ When all user (non-daemon) threads finish, the JVM terminates automatically, even if daemon threads are still running.

2Ô∏è‚É£ How to Create a Daemon Thread
		Thread t = new Thread(() -> {
			while (true) {
				System.out.println("Daemon running...");
			}
		});
		t.setDaemon(true); // must be before start()
		t.start();
		‚ö† Calling setDaemon(true) after start() throws IllegalThreadStateException.

3Ô∏è‚É£ JVM Behavior with Daemon Threads
	User Threads running ‚Üí JVM alive  
	Only Daemon Threads running ‚Üí JVM exits
	Daemon threads: Are abruptly stopped, Do not execute finally blocks reliably, Should not manage critical resources
	
4Ô∏è‚É£ User Thread vs Daemon Thread (Comparison)	
	| Aspect          | User Thread           | Daemon Thread             |
	| --------------- | --------------------- | ------------------------- |
	| Purpose         | Application logic     | Background support        |
	| JVM exit        | Keeps JVM alive       | JVM ignores it            |
	| Lifecycle       | Controlled by app     | Controlled by JVM         |
	| Reliability     | Guaranteed completion | May stop anytime          |
	| Resource safety | Safer                 | Unsafe for critical tasks |

5Ô∏è‚É£ Real-World Use Cases (Very Important)
	üîπ 1. Garbage Collector (GC): 	GC runs as daemon threads. ‚úî JVM cleans memory, Stops automatically when app exits
	üîπ 2. JIT Compiler Threads: Compiles bytecode to native code, Improves performance at runtime, Runs in background
	üîπ 3. Background Monitoring & Cleanup Tasks
			Examples: Cache cleanup, Session expiration, Health checks, Metrics collection

				ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(r -> {
					Thread t = new Thread(r);
					t.setDaemon(true);
					return t;
				});

	üîπ 4. Logging Framework Internals
			Some async loggers use daemon threads for: Flushing logs, Buffer management
			‚ö† Logs may be lost if JVM exits suddenly.

	üîπ 5. Timer Threads: Timer timer = new Timer(true); // daemon timer
			Used for: Periodic background tasks, Cleanup jobs
	üîπ 6. Framework Background Threads
			Frameworks like: Spring, Netty, Kafka clients
			Use daemon threads for: Heartbeats, Watchdogs, Housekeeping

6Ô∏è‚É£ When NOT to Use Daemon Threads
	‚ùå For: File writing, Database updates, Payment processing, Critical business workflows
		Because: JVM can terminate them anytime, No guarantee of completion
		
=============================================================================================================
üîπ Q6. How does JVM decide thread scheduling? Is thread scheduling platform dependent?
=============================================================================================================
1Ô∏è‚É£ JVM does not directly schedule threads. It delegates scheduling to the underlying operating system.
	‚ñ∂  What JVM Actually Does
		The JVM: Creates Java threads, Maps them to native OS threads
		Sets thread properties like: Priority, Daemon status, Hands control to the OS scheduler
		üëâ After that, OS decides: Which thread runs, For how long, On which CPU core

	‚ñ∂ JVM Uses Preemptive Scheduling
		Modern JVMs rely on preemptive multitasking, meaning:
			A running thread can be paused by OS, CPU time is shared via time slicing, Threads don‚Äôt cooperate voluntarily
		This prevents one thread from monopolizing the CPU.
		
2Ô∏è‚É£ What Factors Influence Thread Scheduling?
	Even though JVM doesn‚Äôt schedule directly, these factors matter:
		‚úî Thread Priority
				Thread t = new Thread(task);
				t.setPriority(Thread.MAX_PRIORITY);
			JVM passes priority to OS, OS may honor it, No guarantee
			‚ö† Priority is a hint, not a command.

		‚úî Thread State : Only threads in RUNNABLE state are eligible. Blocked / waiting threads are ignored.
		‚úî CPU Availability: Single-core ‚Üí time slicing, Multi-core ‚Üí true parallel execution
		‚úî OS Scheduling Algorithm
			Examples: Linux ‚Üí Completely Fair Scheduler (CFS), Windows ‚Üí Priority-based scheduler

3Ô∏è‚É£ Is Thread Scheduling Platform Dependent?
	‚úÖ YES ‚Äî Absolutely: This is the correct and expected answer.
	üîπ Why It‚Äôs Platform Dependent? Because: JVM relies on native OS threads, Different OSes use different scheduling algorithms
		Priority handling varies across platforms
		üî∏ Example: Thread Priority Behavior
			t1.setPriority(Thread.MIN_PRIORITY);
			t2.setPriority(Thread.MAX_PRIORITY);
			
=============================================================================================================
üîπ Q7. What is context switching? Explain green threads vs native threads.
=============================================================================================================
1Ô∏è‚É£  Context switching is the process of saving the state of one running thread and restoring the state of another,
   so the CPU can switch execution between them.

	üîß What Gets Saved & Restored?
		When a context switch happens, the system saves: CPU registers, Program counter, Stack pointer, Thread state
		Then loads the state of the next thread.
			Thread A running
			   ‚Üì (context switch)
			Thread A state saved
			Thread B state restored
			Thread B running

	üîπ Why Context Switching Is Needed?
		Because: CPU cores are limited, Threads are more than CPU cores, OS must share CPU time fairly
				Even on single-core systems, context switching creates the illusion of parallelism.

	üîπ Cost of Context Switching (Very Important): Context switching is expensive:
		CPU cycles wasted, Cache invalidation, Pipeline flush
		‚ö† Too many threads ‚Üí too many context switches ‚Üí performance degradation

2Ô∏è‚É£ Green Threads vs Native Threads: This is about who manages the threads ‚Äî JVM or OS.
	üü¢ Green Threads: 
		üîπ What Are Green Threads? Green threads are user-level threads managed entirely by the JVM, not by the operating system.
			JVM schedules threads, OS sees only ONE process
		
		üîπ Characteristics: 
			JVM handles scheduling, OS is unaware of individual threads
			No true parallelism on multi-core CPUs
			Context switching is faster (no kernel involvement)
		
		üîπ Pros: ‚úî Low context switching cost, Fast thread creation, Portable
		üîπ Cons: ‚ùå No true parallelism, One blocking call blocks all threads, Poor scalability
		üîπ Example Early Java versions on Solaris supported green threads.


	üîµ Native Threads
		üîπ What Are Native Threads? Native threads are OS-level threads. Each Java thread maps to a native OS thread.
			Java Thread ‚Üî OS Thread
		
		üîπ Characteristics
			OS scheduler controls execution, True parallelism on multi-core CPUs
			Blocking in one thread doesn‚Äôt block others, Higher context switching cost
		
		üîπ Pros: ‚úî Uses multiple CPU cores, Better scalability, Better I/O handling
		üîπ Cons: ‚ùå Higher overhead, OS-dependent behavior, More context switching cost

4Ô∏è‚É£ What Does Modern Java Use?
	‚úÖ Native Threads ONLY: Since Java 1.3+, Java uses native threads on all major platforms.
		üëâ This is why: Java scales well on multi-core CPUs, Servers handle thousands of requests
		GC, JIT, application threads run independently

=============================================================================================================
üîπ Q8. What is thread priority? Does it really work?
=============================================================================================================
1Ô∏è‚É£  Thread priority is an integer hint (1‚Äì10) that suggests to the thread scheduler how important a thread is relative to others.
	In Java:
			Thread.MIN_PRIORITY  = 1
			Thread.NORM_PRIORITY = 5
			Thread.MAX_PRIORITY  = 10

			Thread t = new Thread(task);
			t.setPriority(Thread.MAX_PRIORITY);

2Ô∏è‚É£ What Is the Purpose of Thread Priority?
	The idea is: Higher priority ‚Üí more CPU time, Lower priority ‚Üí less CPU time
	üëâ But this is only a suggestion, not a guarantee.

3Ô∏è‚É£ Does Thread Priority Really Work?
	üîπ Honest Senior Answer: Not reliably.
	üîπ Correct Technical Answer: Thread priority behavior is platform dependent and should not be relied upon for correctness.

4Ô∏è‚É£ Why Thread Priority Is Unreliable
	üî∏ 1. JVM Delegates Scheduling to OS: The JVM maps Java threads to native OS threads and passes the priority to the OS scheduler.
		 What happens next depends on: OS scheduling algorithm, OS priority mapping, CPU availability

	üî∏ 2. Platform-Dependent Behavior:
			| OS      | Priority Handling           |
			| ------- | --------------------------- |
			| Linux   | Often ignored or normalized |
			| Windows | More likely to respect      |
			| macOS   | Limited effect              |
		üëâ Same Java code ‚Üí different behavior on different OSes.

	üî∏ 3. No Execution Order Guarantee
			Thread t1 = new Thread(() -> System.out.println("Low"));
			Thread t2 = new Thread(() -> System.out.println("High"));
			t1.setPriority(Thread.MIN_PRIORITY);
			t2.setPriority(Thread.MAX_PRIORITY);
			t1.start();
			t2.start();
		‚úî Output is non-deterministic, ‚ùå No guarantee t2 runs first

	üî∏ 4. Risk of Starvation: 
			High-priority threads can starve lower-priority threads
			causing: Performance degradation, Unresponsive systems, This is one reason JVMs often ignore or minimize priority effects.

5Ô∏è‚É£ When Thread Priority Can Help (Rare Cases)
	‚úî Best-effort background tasks, Non-critical monitoring threads, CPU-heavy background computations
	Example: 
		Thread monitoring = new Thread(task);
		monitoring.setPriority(Thread.MIN_PRIORITY);
		monitoring.setDaemon(true);
	Even then ‚Üí don‚Äôt depend on it.

6Ô∏è‚É£ What Should Be Used Instead (Senior Best Practice)
	Instead of thread priority, use: ExecutorService, Thread pools, Proper task queues, Back-pressure mechanisms, Concurrency utilities
	Example: ExecutorService executor = Executors.newFixedThreadPool(4);
	This gives: Predictable behavior, Better scalability, OS-agnostic execution
	
=============================================================================================================
üîπ Q9. Explain thread lifecycle with states. Difference between NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED.
=============================================================================================================
1Ô∏è‚É£  A thread lifecycle represents the different states a thread goes through from creation to termination
	as defined by Thread.State in Java.
	Java defines 6 thread states: NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED

2Ô∏è‚É£ Thread Lifecycle Diagram (Mental Model)
	NEW
	 ‚îÇ start()
	 ‚ñº
	RUNNABLE  <‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
	 ‚îÇ   ‚îÇ                    ‚îÇ
	 ‚îÇ   ‚îÇ lock acquired      ‚îÇ notify/timeout
	 ‚îÇ   ‚ñº                    ‚îÇ
	BLOCKED              WAITING / TIMED_WAITING
	 ‚îÇ                         ‚îÇ
	 ‚îÇ lock released           ‚îÇ
	 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ RUNNABLE
				 ‚îÇ
				 ‚ñº
			TERMINATED

3Ô∏è‚É£ Thread States Explained (With Causes & Examples)
	üîµ 1. NEW: Meaning => Thread object is created, start() has not been called
			Thread t = new Thread(() -> {}); // t is in NEW state
			Key Point: No system resources allocated, Cannot be scheduled
	
	üü¢ 2. RUNNABLE: Meaning => Thread is ready to run or running, JVM has handed thread to OS scheduler
			t.start(); // enters RUNNABLE
			Important: Java does NOT distinguish between: Ready and Running, Both are RUNNABLE.
			Interview Tip: ‚ÄúRUNNABLE means eligible to run, not necessarily running.‚Äù

	üî¥ 3. BLOCKED: Meaning => Thread is waiting to acquire a monitor lock, Happens only with synchronized
			synchronized(lock) {
				// another thread holding lock
			}
			Causes: Thread tries to enter synchronized block/method, Lock held by another thread
			Key Difference: BLOCKED ‚Üí waiting for lock, WAITING ‚Üí waiting for signal

	üü° 4. WAITING: Meaning => Thread waits indefinitely for another thread‚Äôs action
			Causes
					obj.wait();
					thread.join();
					LockSupport.park();
			How It Leaves WAITING: notify(), notifyAll(), Target thread finishes (for join)

	üü† 5. TIMED_WAITING: Meaning => Thread waits for a specified time
			Causes
					Thread.sleep(1000);
					obj.wait(1000);
					thread.join(1000);
					lock.tryLock(1, TimeUnit.SECONDS);

			After Timeout: Thread returns to RUNNABLE automatically

	‚ö´ 6. TERMINATED: Meaning => Thread execution is complete, run() method exits, // run() finished
			Important: Cannot be restarted, Calling start() again ‚Üí IllegalThreadStateException

4Ô∏è‚É£ When Does a Thread Enter the BLOCKED State?
	A thread enters the BLOCKED state only when it is trying to acquire an intrinsic (monitor) lock that is already held by another thread. üëâ BLOCKED = waiting for a monitor lock
	üîπ The Only Cause of BLOCKED State
		‚úî Attempting to enter a synchronized block or method, Lock is already owned by another thread
				synchronized (lock) {
					// critical section
				}
		If another thread already holds lock, the current thread becomes BLOCKED.

5Ô∏è‚É£ BLOCKED vs WAITING (Very Common Trap)
	| Aspect        | BLOCKED          | WAITING           |
	| ------------- | ---------------- | ----------------- |
	| Reason        | Lock unavailable | Signal required   |
	| Triggered by  | synchronized     | wait(), join()    |
	| Timeout       | ‚ùå No   	       | ‚ùå No   	       |
	| Controlled by | JVM monitor      | Application logic |

6Ô∏è‚É£ WAITING vs TIMED_WAITING
	| Aspect   | WAITING    | TIMED_WAITING    |
	| -------- | ---------- | ---------------- |
	| Duration | Indefinite | Fixed time       |
	| Wake-up  | notify     | timeout / notify |
	| Example  | wait()     | sleep(1000)      |


=============================================================================================================
üîπ Q10. Can a thread move from WAITING to RUNNABLE without notify? What happens if notify is called before wait?
=============================================================================================================
1Ô∏è‚É£ Can a Thread Move from WAITING ‚Üí RUNNABLE Without notify()?
	‚úÖ YES ‚Äî A thread in WAITING state can move back to RUNNABLE without notify() in the following cases:
	üîπ 1. Thread Interruption: If a waiting thread is interrupted:
				synchronized (lock) {
					lock.wait();   // WAITING
				}
				thread.interrupt();

			‚û° The thread: Wakes up, Throws InterruptedException, Moves to RUNNABLE
			‚úî No notify() involved
	
	üîπ 2. Spurious Wakeups: The JVM is allowed to wake up a waiting thread without any notify.
			This is rare, but legal per Java spec. üëâ That‚Äôs why this is mandatory:
				synchronized (lock) {
					while (!condition) {
						lock.wait();
					}
				}
			‚ùå Using if is wrong, ‚úÖ while rechecks the condition safely

	üîπ 3. Target Thread Completion (join): If a thread is waiting on join():
			t.join();  // WAITING
			When thread t finishes: Waiting thread wakes up, Moves to RUNNABLE, No notify is required
	‚ùå What Does NOT Wake a WAITING Thread: sleep(), Lock release, Thread priority, CPU availability


2Ô∏è‚É£ What Happens If notify() Is Called Before wait()?
	üîπ Short Answer: The notification is lost.
	üîç Explanation: Consider this code:
			synchronized (lock) {
				lock.notify(); // nobody is waiting
			}
		Later:
			synchronized (lock) {
				lock.wait();   // waits forever
			}
		What happens? No thread was waiting at notify time, JVM does not store notifications, The waiting thread blocks indefinitely
		‚û° Dead wait situation
		üî• Key Rule (Must Say in Interview): wait/notify has no memory ‚Äî notify does not queue signals.

3Ô∏è‚É£ Why This Design Exists
	Java‚Äôs wait/notify is low-level and signal-based, not condition-based.
	It assumes: 
		The programmer controls the condition, Notifications are not persistent,
		That‚Äôs why higher-level constructs exist:
				BlockingQueue, CountDownLatch, Semaphore, Condition

4Ô∏è‚É£ Correct Pattern to Avoid Lost Notifications
	‚úÖ Always use a condition check
			synchronized (lock) {
				while (!condition) {
					lock.wait();
				}
			}
		And:
			synchronized (lock) {
				condition = true;
				lock.notifyAll();
			}

5Ô∏è‚É£ notify() vs notifyAll() in This Context
	| Method      | Risk                         |
	| ----------- | ---------------------------- |
	| notify()    | Can wake wrong thread        |
	| notifyAll() | Safer, avoids missed signals |
üëâ In real systems, notifyAll is usually safer.


=============================================================================================================
üîπ Q11. How does JVM clean up a terminated thread? What happens if a thread throws an exception?
=============================================================================================================
1Ô∏è‚É£ How Does JVM Clean Up a Terminated Thread?
	üîπ What Is a Terminated Thread? 
		A thread enters TERMINATED state when: run() method finishes normally, or An uncaught exception escapes run()
		Once terminated: The thread can never be restarted, It becomes eligible for garbage collection
		üîß What JVM Cleans Up (Internally): When a thread terminates, the JVM:
			‚úÖ 1. Releases Thread Stack Memory
					Each thread has its own stack, Stack frames are discarded immediately
					This memory is returned to the OS / JVM
			
			‚úÖ 2. Releases Native OS Thread: Java threads map to native OS threads, JVM informs OS to clean up native thread resources
			‚úÖ 3. Releases Locks Automatically
					If thread exits a synchronized block or method:
						JVM automatically releases the monitor lock
									synchronized (lock) {
										throw new RuntimeException();
									} // lock released automatically
						‚úî No deadlock due to terminated thread

			‚úÖ 4. Clears Thread-Local Stack Data: Local variables, Method frames, Program counter

‚ùå What JVM Does NOT Clean Automatically
	‚ö† ThreadLocal values: Stored in ThreadLocalMap inside the thread
		If thread lives long (e.g., thread pool), values may cause memory leaks
		Must be cleaned manually using remove(): threadLocal.remove();
	üß† Important GC Point: The Thread object itself is garbage collected only if no references remain
	Termination ‚â† immediate GC

üéØ Interview Line : When a thread terminates, the JVM releases its stack memory, native thread resources, and any acquired intrinsic locks, and the Thread object becomes eligible for garbage collection if no references remain.

2Ô∏è‚É£ What Happens If a Thread Throws an Exception?
	This depends on whether the exception is caught or uncaught.
	üîπ Case 1: Exception Is Caught Inside run()
						public void run() {
							try {
								int x = 10 / 0;
							} catch (Exception e) {
								System.out.println("Handled");
							}
						}
			‚úî Thread continues execution, Thread terminates normally after run() completes, No impact on other threads
	
	üîπ Case 2: Uncaught Runtime Exception (Most Important)
						public void run() {
							int x = 10 / 0; // ArithmeticException
						}
			What happens? Exception propagates out of run(), JVM invokes UncaughtExceptionHandler, Thread terminates immediately
			Other threads continue normally, ‚ùó JVM does NOT crash because of one thread

			‚ñ∂  Default Behavior: If no handler is set:
					JVM prints stack trace to stderr, Thread dies silently after logging
			
			‚ñ∂  Custom UncaughtExceptionHandler (Senior-Level)
							Thread.setDefaultUncaughtExceptionHandler((t, e) -> {
								System.out.println("Thread " + t.getName() + " failed: " + e);
							});

					Or per thread:
							t.setUncaughtExceptionHandler((thread, ex) -> {
								log.error("Thread crashed", ex);
							});

					‚úî Used in production systems for: Logging, Alerts, Restarting tasks,
	üîπ Case 3: Checked Exception in run()
			‚ùå Not allowed directly
				public void run() throws IOException { // compile-time error
				}
			‚úî Must be handled inside run()

3Ô∏è‚É£ What About Locks & finally Blocks?
	‚úî Locks: JVM releases intrinsic locks automatically

	‚úî finally Blocks
			try {
				risky();
			} finally {
				cleanup();
			}
		‚úî finally executes before thread termination, ‚ùå Except in extreme cases like System.exit()

4Ô∏è‚É£ What Happens to Other Threads?
	| Scenario                     | Impact                                      |
	| ---------------------------- | ------------------------------------------- |
	| One thread throws exception  | Other threads unaffected                    |
	| Main thread throws exception | JVM exits **only if no other user threads** |
	| Daemon thread crashes        | Ignored by JVM                              |

=============================================================================================================
üîπ Q12. What is synchronization and why is it needed? What problem does synchronization solve?
=============================================================================================================
1Ô∏è‚É£  Synchronization is a mechanism in Java that controls access to shared mutable resources so that only one thread executes a critical section at a time, while also ensuring memory visibility between threads.
‚ñ∂ In Java, synchronization is achieved using:
	synchronized keyword, Intrinsic (monitor) locks, Locks in java.util.concurrent

2Ô∏è‚É£ Why Is Synchronization Needed? 
	‚ñ∂ Because multiple threads share memory.
		Without synchronization: 
			Threads may read stale data, Updates may overwrite each other, Program behavior becomes non-deterministic

3Ô∏è‚É£ What Problem Does Synchronization Solve?
	Synchronization primarily solves three core concurrency problems:
	üî¥ Problem 1: Race Condition (Atomicity Issue)
		‚ùå Without Synchronization
				class Counter {
					int count = 0;

					void increment() {
						count++; // read ‚Üí modify ‚Üí write
					}
				}

		If two threads execute count++ concurrently: Both read the same value, Both write back the same result, One update is lost
		This is a race condition.
		‚úÖ With Synchronization
			synchronized void increment() {
				count++;
			}
		‚úî Ensures only one thread updates count at a time, Makes count++ atomic

	üî¥ Problem 2: Memory Visibility (Stale Data)
		‚ùå Without Synchronization
				boolean running = true;
				void stop() {
					running = false;
				}
				void run() {
					while (running) {
						// may loop forever
					}
				}
		Why? Each thread may cache running, Update by one thread may not be visible to others

		‚úÖ With Synchronization
				synchronized void stop() {
					running = false;
				}

				synchronized void run() {
					while (running) {}
				}

			‚úî Synchronization enforces happens-before relationship, Changes become visible across threads

	üî¥ Problem 3: Instruction Reordering
		Without synchronization: JVM & CPU may reorder instructions, Leads to partially initialized objects being visible
		‚úÖ Synchronization prevents unsafe reordering.

4Ô∏è‚É£ What Synchronization Guarantees (Very Important)
	| Guarantee        | Explanation                             |
	| ---------------- | --------------------------------------- |
	| Mutual exclusion | Only one thread enters critical section |
	| Atomicity        | Compound operations execute as one      |
	| Visibility       | Writes by one thread visible to others  |
	| Ordering         | Prevents harmful reordering             |

5Ô∏è‚É£ What Synchronization Does NOT Guarantee
	‚ùå No fairness guarantee, No deadlock prevention, No performance guarantee

6Ô∏è‚É£ Real-World Example (Senior-Level)
	In a Spring Boot application: Multiple HTTP requests update the same in-memory cache
	Without synchronization ‚Üí corrupted cache
	With synchronization ‚Üí consistent state
	
=============================================================================================================
üîπ Q13. Difference between synchronized method vs synchronized block. Why is synchronized block preferred?
=============================================================================================================
1Ô∏è‚É£ synchronized Method vs synchronized Block
	üîπ Synchronized Method: When you mark a method as synchronized, the entire method body is locked.
				class Counter {
					public synchronized void increment() {
						count++;
						log();          // also synchronized
						validate();     // also synchronized
					}
				}
			Lock acquired on: this (for instance method), ClassName.class (for static method)

	üîπ Synchronized Block: Only a specific section of code is synchronized.
				class Counter {
					public void increment() {
						// non-critical code
						validate();

						synchronized (this) {
							count++;
						}

						// non-critical code
						log();
					}
				}
			Lock acquired on: Explicit object (this, or any other lock object)
			
2Ô∏è‚É£ Key Differences (Interview Table)
	| Aspect      | Synchronized Method    | Synchronized Block    |
	| ----------- | ---------------------- | --------------------- |
	| Lock scope  | Entire method          | Only critical section |
	| Flexibility | Low                    | High                  |
	| Performance | Lower (coarse-grained) | Better (fine-grained) |
	| Lock object | Implicit               | Explicit              |
	| Readability | Simpler                | Slightly more verbose |
	| Control     | Less                   | More                  |
	| Best for    | Simple logic           | Complex logic         |

3Ô∏è‚É£ Why Is Synchronized Block Preferred?
	üî• Reason 1: Fine-Grained Locking (Most Important): 
		Synchronized blocks allow locking only the critical section, not the entire method.
			synchronized (this) {
				updateSharedState();
			}
		‚úî Less lock contention, Better concurrency, Higher throughput

	üî• Reason 2: Better Performance
		Synchronized methods: Hold the lock longer, Block other threads unnecessarily
		Synchronized blocks: Reduce time spent holding locks, Improve scalability in multi-threaded apps
	
	üî• Reason 3: Explicit Lock Object: 	private final Object lock = new Object();
			synchronized (lock) {
				// critical section
			}
		‚úî Avoids exposing intrinsic lock (this), Prevents accidental external locking, Cleaner encapsulation

	üî• Reason 4: Avoids Over-Synchronization
			public synchronized void process() {
				validate();
				callRemoteService();  // ‚ùå slow but locked
				updateCache();
			}
			Bad design. 
			
		‚úîUsing block: Lock only where required
			public void process() {
				validate();
				callRemoteService(); // no lock
				synchronized (this) {
					updateCache();
				}
			}

	üî• Reason 5: Multiple Locks in Same Method
			synchronized (lockA) {
				updateA();
			}

			synchronized (lockB) {
				updateB();
			}
		‚úî Impossible with synchronized method, Enables better concurrency

4Ô∏è‚É£ When Is Synchronized Method Acceptable?
	‚úî Very small methods, Simple getters/setters, No performance concern, Single shared resource
	Example:
		public synchronized int getCount() {
			return count;
		}

5Ô∏è‚É£ Common Interview Traps (Avoid These)
	‚ùå ‚ÄúSynchronized method is faster‚Äù, ‚ÄúBlock is always better‚Äù, ‚ÄúMethod uses different lock‚Äù
	‚úî Correct understanding matters.
	
=============================================================================================================
üîπ Q14. Can we synchronize a static method? What is the lock associated with static synchronization?
=============================================================================================================
1Ô∏è‚É£  ‚úÖ YES: A static method can be declared synchronized just like an instance method.
			class Counter {
				private static int count = 0;

				public static synchronized void increment() {
					count++;
				}
			}

2Ô∏è‚É£ What Lock Is Used for Static Synchronization?
	üî• Key Answer : Static synchronized methods use the Class level as the lock.
		In the example above, the lock is: Counter.class

3Ô∏è‚É£ static vs instance synchronized code example
	class Test {
		public static synchronized void staticMethod() {
			System.out.println("Static method");
		}

		public synchronized void instanceMethod() {
			System.out.println("Instance method");
		}
	}
	staticMethod() ‚Üí lock on Test.class, instanceMethod() ‚Üí lock on instance object
	üëâ These do NOT block each other unless explicitly synchronized on the same lock.

4Ô∏è‚É£ Static Synchronized Block (Explicit Lock)
	You can also synchronize a static block explicitly:
		class Test {
			static void update() {
				synchronized (Test.class) {
					// critical section
				}
			}
		}
	‚úî Same lock as static synchronized method, More flexible (preferred in complex logic)

5Ô∏è‚É£ Why Would We Use Static Synchronization?
	‚úî Protect shared static data, Enforce class-wide mutual exclusion, Prevent concurrent modification of static state
	Example: private static Map<String, String> cache = new HashMap<>();

6Ô∏è‚É£ Common Interview Traps (Avoid These ‚ùå)
	‚ùå ‚ÄúStatic synchronized uses object lock‚Äù, ‚ÄúStatic and instance synchronization block each other‚Äù,  ‚ÄúStatic synchronized is per object‚Äù
	‚úî Only class-level locking is used.

=============================================================================================================
üîπ Q15. What happens if multiple threads synchronize on different objects? Is synchronization reentrant?
=============================================================================================================
1Ô∏è‚É£ What Happens If Multiple Threads Synchronize on Different Objects?
	üîπ Short, Correct Answer: They do NOT block each other.
	Synchronization in Java is per lock object, not per method or per code.
	üîç Why? Each object in Java has its own intrinsic (monitor) lock.
		If threads synchronize on different objects, they acquire different locks, so they can execute in parallel.
		üß™ Code Example
					class Test {
						private final Object lock1 = new Object();
						private final Object lock2 = new Object();

						public void method1() {
							synchronized (lock1) {
								System.out.println("Thread in lock1");
								sleep();
							}
						}

						public void method2() {
							synchronized (lock2) {
								System.out.println("Thread in lock2");
								sleep();
							}
						}

						private void sleep() {
							try { Thread.sleep(2000); } catch (Exception ignored) {}
						}
					}

		If: Thread A calls method1(), Thread B calls method2() üëâ Both threads run concurrently because: lock1 ‚â† lock2
		
üî• Interview Line (Important): ‚ÄúSynchronization only blocks threads competing for the same lock object.‚Äù
‚ö† Common Trap
			synchronized (new Object()) {
				// useless synchronization
			}
	‚ùå Every thread gets a new lock, No mutual exclusion, Synchronization becomes meaningless

2Ô∏è‚É£ Is Synchronization Reentrant? ‚úÖ YES ‚Äî Java synchronization is reentrant.
	üîπ What Does Reentrant Mean? A thread that already holds a lock can re-acquire the same lock again without getting blocked.
		The JVM keeps a lock hold count for each thread.
		üß™ Code Example (Very Important)
					class ReentrantDemo {

						public synchronized void methodA() {
							System.out.println("Inside methodA");
							methodB(); // re-acquires same lock
						}

						public synchronized void methodB() {
							System.out.println("Inside methodB");
						}
					}
		‚úî Same thread, Same object lock (this), No deadlock, Works smoothly
	üîç What Happens Internally?
		Thread acquires lock ‚Üí hold count = 1
		Calls another synchronized method
		Lock re-acquired ‚Üí hold count = 2
		Exits methodB() ‚Üí count = 1
		Exits methodA() ‚Üí count = 0 ‚Üí lock released
		‚ùå If Synchronization Were NOT Reentrant: This code would deadlock ‚Äî but it doesn‚Äôt. That‚Äôs why reentrancy is essential.

3Ô∏è‚É£ ReentrantLock vs synchronized (Quick Note)
	| Feature    | synchronized | ReentrantLock |
	| ---------- | ------------ | ------------- |
	| Reentrant  | ‚úÖ Yes 	    | ‚úÖ Yes  	    |
	| Lock count | JVM-managed  | Explicit  	|
	| Try lock   | ‚ùå     	    | ‚úÖ    	    |

=============================================================================================================
üîπ Q16. Can a constructor be synchronized? Can we synchronize on null?
=============================================================================================================
1Ô∏è‚É£  ‚ùå No, a constructor cannot be synchronized.
		class Test {
			public synchronized Test() { } // ‚ùå compile-time error
		}
	‚ùì Why Not? Because synchronization requires a lock, and: Instance synchronization uses this as the lock
	During constructor execution, the object is not fully constructed, this is not safely available as a lock yet
	üëâ Locking an object that isn‚Äôt fully initialized would break JVM safety guarantees.

üîπ JVM Design Reason (Senior Insight)
	Constructors are used to create the object, Synchronization is used to protect shared state
	Before construction completes, the object: Should not be shared, Should not be locked
	That‚Äôs why Java forbids synchronized constructors at language level.

‚úÖ How to Achieve Constructor-like Synchronization (Correct Way)
	If you need thread safety during initialization:
	‚úî 1: Use synchronized block inside constructor (rare but allowed)
				class Test {
					private static final Object LOCK = new Object();

					public Test() {
						synchronized (LOCK) {
							// thread-safe initialization
						}
					}
				}
	‚úî 2 (Better): Use factory methods
				class Test {
					private Test() {}

					public static synchronized Test create() {
						return new Test();
					}
				}

2Ô∏è‚É£ Can We Synchronize on null? ‚ùå No ‚Äî synchronizing on null causes a runtime exception.
	Object lock = null;
		synchronized (lock) {   // ‚ùå
			// code
		}
	üí• What Happens? ‚û° JVM throws NullPointerException immediately.
	‚ùì Why? Because: synchronized requires a real object, JVM needs the object‚Äôs monitor, null has no monitor
		So the JVM cannot acquire a lock.
	üîπ Important Clarification: This is not a compile-time error ‚Äî it fails at runtime.

=============================================================================================================
üîπ Q17. What happens if an exception occurs inside synchronized block?
=============================================================================================================
‚úÖ The lock is automatically released when the thread exits the synchronized block, even if it exits due to an exception.
	This is guaranteed by the JVM.
	1Ô∏è‚É£ Simple Example : 	Object lock = new Object();
		synchronized (lock) {
			System.out.println("Lock acquired");
			int x = 10 / 0;   // ArithmeticException
		}

		üîç What Happens Step by Step? 
			Thread acquires the monitor lock on lock, Exception occurs, JVM executes monitorexit automatically, Lock is released
			Exception propagates up the call stack
		‚úî No deadlock, Other threads can acquire the lock

	2Ô∏è‚É£ Why Lock Is Always Released (JVM Guarantee)
		At bytecode level, synchronized uses:
			monitorenter
			   // critical section
			monitorexit
		The JVM ensures monitorexit runs: On normal exit, On exceptional exit
		üëâ Similar to how finally works.

	3Ô∏è‚É£ With try‚Äìcatch‚Äìfinally (Common Interview Confusion)
			synchronized (lock) {
				try {
					riskyOperation();
				} catch (Exception e) {
					handle(e);
				}
			}
		or even:

			synchronized (lock) {
				riskyOperation();
			}
		‚úî In both cases, the lock is released.

	4Ô∏è‚É£ What If the Exception Is Uncaught?
			synchronized (lock) {
				throw new RuntimeException("Boom");
			}
		
		Lock is released: Thread terminates, JVM may invoke UncaughtExceptionHandler, Other threads continue normally
		üëâ Lock release is independent of exception handling.

	5Ô∏è‚É£ Important Clarification: What Is Not Released Automatically
		‚ö† Only intrinsic (monitor) locks are auto-released.
		‚ùå With ReentrantLock
					lock.lock();
					doSomething();
					throw new RuntimeException(); // ‚ùå lock NOT released automatically
				You must unlock explicitly:

					lock.lock();
					try {
						doSomething();
					} finally {
						lock.unlock();
					}
				This contrast is something interviewers love.

	6Ô∏è‚É£ Can an Exception Cause Deadlock? 	‚ùå Not with synchronized alone, because:
		JVM always releases the monitor 
		‚úî Deadlocks come from: Multiple locks, Circular wait, Bad lock ordering
=============================================================================================================
üîπ Q18. Does synchronized guarantee fairness? What is intrinsic lock / monitor lock?
=============================================================================================================
1Ô∏è‚É£ ‚ùå No ‚Äî synchronized does NOT guarantee fairness
	üîπ What ‚Äúfairness‚Äù means? Threads acquire the lock in the order they requested it, No thread is starved indefinitely
	üîπ Why synchronized Is Not Fair: 
		synchronized uses intrinsic (monitor) locks
		JVM does not maintain a FIFO queue for threads waiting on a monitor
		When a lock is released, any waiting thread may acquire it
		Scheduling is influenced by: OS thread scheduler, JVM optimizations, Timing and CPU availability
		üëâ Result: No ordering guarantee
		üß™ Example (Starvation Possible)
			synchronized (lock) {
				// critical section
			}
		If: Thread A releases the lock, Threads B, C, D are waiting
		‚û° JVM may allow any of them to enter next, ‚û° Thread B could starve if unlucky
		üî• Interview Line: ‚Äúsynchronized provides mutual exclusion and visibility, but it does not guarantee fairness or ordering of lock acquisition.‚Äù
	
	üîπ Why Java Designed It This Way (Senior Insight)
		Fair locking: Requires maintaining queues, Adds overhead, Reduces throughput
		synchronized is optimized for: Performance, Low contention, Fast uncontended paths
		‚úÖ If You Need Fairness
			Use ReentrantLock with fairness enabled:
				Lock lock = new ReentrantLock(true); // fair lock
				‚ö† Even fair locks: Reduce throughput, Still depend on OS scheduling

2Ô∏è‚É£ What Is an Intrinsic Lock / Monitor Lock?
	üîπ An intrinsic lock (also called a monitor lock) is the built-in lock that every Java object has, which is used by the synchronized keyword. üëâ Every object in Java has exactly one monitor

	üîπ How It Works
		synchronized (obj) {
			// critical section
		}
		Thread tries to acquire obj‚Äôs monitor, If free ‚Üí thread enters, If held ‚Üí thread becomes BLOCKED
		On exit ‚Üí monitor is released automatically

	üîπ Types of Intrinsic Locks
		| Synchronization Type         | Lock Used         |
		| ---------------------------- | ----------------- |
		| Instance synchronized method | `this`            |
		| Static synchronized method   | `ClassName.class` |
		| Synchronized block           | Specified object  |

	üîπ Monitor Responsibilities
		A monitor provides: Mutual exclusion (only one thread inside), Memory visibility guarantees, wait/notify mechanism
		That‚Äôs why: wait(), notify(), notifyAll() are in Object
		They operate on the object‚Äôs monitor
		üîç JVM Internals (Senior-Level Detail): Monitor metadata stored in object header (Mark Word)
		Implemented using: Biased locks, Lightweight locks (CAS), Heavyweight OS monitors (under contention)

3Ô∏è‚É£ Intrinsic Lock vs Explicit Lock (Quick Contrast)
	| Aspect       | Intrinsic Lock (`synchronized`) | Explicit Lock (`ReentrantLock`) |
	| ------------ | ------------------------------- | ------------------------------- |
	| Fairness     | ‚ùå No             	             | ‚úÖ Optional      	           |
	| Timeout      | ‚ùå No               	         | ‚úÖ Yes                  	       |
	| Try lock     | ‚ùå No                    	     | ‚úÖ Yes                 	       |
	| Auto release | ‚úÖ Yes                	         | ‚ùå Manual                	   |
	| wait/notify  | Object methods                  | Condition objects               |

=============================================================================================================
üîπ Q19. How does JVM implement synchronized internally?
=============================================================================================================
1Ô∏è‚É£ High-Level Answer (Start Like This in Interview)
‚ÄúJVM implements synchronized using monitor locks associated with every object. At bytecode level it uses monitorenter and monitorexit, and internally it optimizes locking through biased locks, lightweight locks, and heavyweight (OS) monitors depending on contention.‚Äù

2Ô∏è‚É£ Every Object Has a Monitor 
	In Java: Every object is associated with a monitor (intrinsic lock)
		synchronized means acquire the object‚Äôs monitor
				synchronized (obj) {
					// critical section
				}
		üëâ Lock = monitor of obj

3Ô∏è‚É£ Bytecode Level: monitorenter / monitorexit
	When you compile synchronized code, the JVM inserts bytecode instructions:
		monitorenter
		  // critical section
		monitorexit
	For synchronized methods: JVM implicitly adds these instructions at method entry & exit
	‚úî JVM guarantees monitorexit is executed,Even if an exception occurs

4Ô∏è‚É£ Where Is Lock Information Stored? (Important)
	üîπ Object Header (Mark Word): Each object has a header containing a Mark Word, which stores:
		Lock state, Thread ID (in some modes), Hash code, GC info
		This is where synchronization metadata lives.

5Ô∏è‚É£ Lock States & Optimizations (Most Important Part)
	JVM does NOT always use OS-level locks. It uses adaptive locking based on contention.
	üî• Lock States (Evolution)
				No Lock
				  ‚Üì
				Biased Lock
				  ‚Üì
				Lightweight Lock
				  ‚Üì
				Heavyweight Lock (OS Monitor)

6Ô∏è‚É£ Biased Locking (Fast Path ‚Äì No Contention)
	üîπ What It Is: Lock is biased toward one thread, No atomic instructions needed, No context switching
	üîπ When Used: Same thread repeatedly enters synchronized block, Single-threaded access pattern
	‚úî Very fast ‚ùå Breaks when another thread tries to acquire lock
	Note: Biased locking is disabled/removed in newer Java versions, but conceptually important.

7Ô∏è‚É£ Lightweight Lock (CAS-Based)
	üîπ What Happens: JVM uses CAS (Compare-And-Swap), Thread tries to acquire lock without blocking, Uses spin locking
		‚úî No OS involvement, ‚úî Efficient under low contention

8Ô∏è‚É£ Heavyweight Lock (Monitor / OS Mutex)
	üîπ When It Happens: High contention, CAS/spin fails repeatedly
	üîπ What JVM Does: Lock is inflated, Thread is blocked, OS scheduler manages the lock
	‚ùå Context switching, ‚ùå Kernel involvement ‚ùå Expensive

9Ô∏è‚É£ Lock Inflation & Deflation
	JVM inflates lock when contention increases, JVM may deflate lock later if contention drops
	Happens dynamically at runtime, This is why modern synchronized is much faster than older Java versions.

üîü How Reentrancy Is Implemented
	JVM tracks: Owning thread, Lock hold count, Same thread can re-enter the lock: Hold count increments, Lock released only when count reaches zero

1Ô∏è‚É£1Ô∏è‚É£ Memory Visibility Guarantees: synchronized provides happens-before guarantees:
	On monitorenter: Thread reads fresh values from main memory
	On monitorexit: Thread flushes changes to main memory
	‚úî Solves visibility, Solves ordering, Solves atomicity

1Ô∏è‚É£2Ô∏è‚É£ What synchronized Does NOT Do
	‚ùå No fairness guarantee, ‚ùå No timeout, ‚ùå No deadlock prevention
	That‚Äôs why ReentrantLock exists.

1Ô∏è‚É£3Ô∏è‚É£ Why synchronized Became Fast (Senior Insight)
	Before Java 6: Always heavyweight locks, Very slow, After Java 6+: Biased locking, Thin locks, Lock coarsening, Lock elimination
	üëâ Modern synchronized is highly optimized and often good enough.
	
=============================================================================================================
üîπ Q20. Why are wait(), notify(), notifyAll() in Object class, not Thread? Why must wait/notify be called inside synchronized block?
=============================================================================================================
1Ô∏è‚É£  Because wait/notify/notifyAll work on object monitors (locks), not on threads.
	üîç The Core Idea: Coordination Is About Shared Objects
		Threads do not wait on threads ‚Äî threads wait on shared resources (objects).
			synchronized (queue) {
				while (queue.isEmpty()) {
					queue.wait();   // waiting on queue‚Äôs monitor
				}
			}
		Here: The object (queue) is the coordination point, Multiple threads can wait on the same object, The lock + condition belongs to the object, üëâ That‚Äôs why these methods live in Object.

	üîπ What a Monitor Actually Provides: Every object‚Äôs monitor provides:
		Mutual exclusion (via synchronized), A wait set (threads waiting on this object), Notification mechanism (notify/notifyAll)
		Since these are object-level features, the methods belong to Object.
	
	‚ùå Why Putting Them in Thread Would Be Wrong
			If they were in Thread:
				Which thread would you notify?
				How would multiple threads wait on the same condition?
				How would you associate a condition with a shared resource?
				It breaks the shared-resource coordination model.

2Ô∏è‚É£ Why Must wait() / notify() Be Called Inside a synchronized Block?
	üîπ Because the calling thread must own the object‚Äôs monitor.
		If not ‚Üí JVM throws IllegalMonitorStateException.
	
3Ô∏è‚É£ What Happens Internally (This Is the Key)
	‚ñ∂ When wait() is called: Thread must already hold the object‚Äôs monitor
		JVM: Releases the monitor, Moves thread to the object‚Äôs wait set, Puts thread in WAITING or TIMED_WAITING
			synchronized (lock) {
				lock.wait(); // releases lock + waits atomically
			}
		This atomic release + wait is crucial.

	‚ñ∂ When notify() / notifyAll() is called
		Thread must hold the same monitor
		JVM: Moves waiting thread(s) from wait set ‚Üí entry set, They compete to re-acquire the lock
		Lock is released only when synchronized block exits

4Ô∏è‚É£ Why Synchronization Is Mandatory (Critical Reason)
	üî• To Prevent Race Conditions & Lost Notifications
		Without synchronization: 
			A thread could call notify() before another thread actually starts wait()
			Notifications could be lost, State could be inconsistent
		Synchronization ensures:
			Visibility of shared state, Atomicity of ‚Äúcheck condition ‚Üí wait‚Äù, Correct coordination

5Ô∏è‚É£ What Happens If You Don‚Äôt Use synchronized?
	lock.wait();   // ‚ùå
	lock.notify(); // ‚ùå
	‚û° JVM throws: IllegalMonitorStateException
	Because: Thread does not own the monitor, JVM cannot safely release/acquire the lock

6Ô∏è‚É£ Correct Usage Pattern (Must Know)
	‚úÖ Always use wait in a loop
		synchronized (lock) {
			while (!condition) {
				lock.wait();
			}
		}
	And notify after state change:
		synchronized (lock) {
			condition = true;
			lock.notifyAll();
		}
	This handles: Spurious wakeups, Missed signals, Visibility issues
	
=============================================================================================================
üîπ Q21. Difference between wait() and sleep(). What happens internally when wait() is called? Why should wait always be used inside a loop?
=============================================================================================================
1Ô∏è‚É£ Difference Between wait() and sleep()
	| Aspect                | `wait()`                       | `sleep()`               |
	| --------------------- | ------------------------------ | ----------------------- |
	| Class                 | `Object`                       | `Thread`                |
	| Purpose               | Inter-thread communication     | Pause execution         |
	| Releases lock         | ‚úÖ Yes              	         | ‚ùå No          	       |
	| Requires synchronized | ‚úÖ Yes               	         | ‚ùå No           	       |
	| Wakes up by           | notify / notifyAll / interrupt | Time expiry / interrupt |
	| Thread state          | WAITING / TIMED_WAITING        | TIMED_WAITING           |
	| Coordination          | Yes                            | No                      |

	üîç Code Comparison
	sleep() example
		synchronized (lock) {
			Thread.sleep(1000); // holds lock
		}
	‚ùå Other threads cannot enter synchronized block, ‚úî Used for timing, delays

	wait() example
		synchronized (lock) {
			lock.wait(); // releases lock
		}
	‚úî Other threads can acquire lock, ‚úî Used for coordination

2Ô∏è‚É£ What Happens Internally When wait() Is Called?
	üîπ Preconditions: Thread must own the object‚Äôs monitor, Otherwise ‚Üí IllegalMonitorStateException
	üîß Internal Steps (Very Important)
			When a thread calls wait():
				Thread verifies ownership of the monitor, JVM releases the monitor lock, Thread is placed into the object‚Äôs wait set
				Thread state becomes: WAITING (wait), TIMED_WAITING (wait(timeout))
				Thread stays dormant until: notify(), notifyAll(), interrupt, spurious wakeup
					RUNNABLE
					   ‚Üì wait()
					releases lock
					   ‚Üì
					WAITING (wait set)
	
	üîπ On notify / notifyAll: 
		Thread moves from wait set ‚Üí entry set, Thread must re-acquire the same lock
		Only then it continues execution, ‚ö† notify does NOT release the lock immediately
		Lock is released when notifier exits synchronized block.

3Ô∏è‚É£ Why Must wait() Always Be Used Inside a Loop?
	üî• This is a MUST-KNOW correctness rule.
	‚ùå Problem with if
			synchronized (lock) {
				if (!condition) {
					lock.wait();
				}
				// unsafe
			}
		This code is broken.

	üî¥ Reason 1: Spurious Wakeups: 	JVM is allowed to wake a waiting thread without notify.
				If that happens: if condition is not rechecked, Thread continues with invalid state

	üî¥ Reason 2: Lost / Wrong Notification
				With multiple waiting threads: notify() may wake wrong thread, Condition may still be false

	üî¥ Reason 3: Race Conditions
				Between: notify, re-acquiring lock, State may change again.

	‚úÖ Correct Pattern: while
		synchronized (lock) {
			while (!condition) {
				lock.wait();
			}
			// safe to proceed
		}
		‚úî Condition is rechecked, Handles spurious wakeups, Handles missed notifications, Thread-safe and correct
=============================================================================================================
üîπ Q22. Difference between notify() vs notifyAll(). When should notifyAll be preferred? What happens if notify is called without any waiting thread?
=============================================================================================================
1Ô∏è‚É£ Difference Between notify() and notifyAll()
	| Aspect           | `notify()`                       | `notifyAll()`              |
	| ---------------- | -------------------------------- | -------------------------- |
	| Wakes up         | **One arbitrary waiting thread** | **All waiting threads**    |
	| Thread selection | JVM chooses (no order guarantee) | All are moved to entry set |
	| Risk             | Can wake wrong thread            | Safer                      |
	| Performance      | Slightly better                  | More overhead              |
	| Correctness      | Risky                            | Correct in most cases      |

üîç Important Detail
	notify() wakes one thread from the object‚Äôs wait set
	notifyAll() wakes all waiting threads
	Woken threads still must re-acquire the lock before running
	‚ö† Neither method releases the lock immediately ‚Äî that happens only when the synchronized block exits.

2Ô∏è‚É£ When Should notifyAll() Be Preferred?
	Prefer notifyAll() whenever multiple conditions or multiple types of threads may be waiting on the same lock.
	üîπ Real Reasons (Very Important)
	‚úÖ 1. Multiple Conditions on Same Lock
			Example: Producer threads waiting for space, Consumer threads waiting for data
				Using notify(): JVM may wake wrong type of thread, Condition still false ‚Üí thread waits again
					Risk of livelock / dead wait
					
				notifyAll(): All threads wake, Only threads with true condition proceed, Others go back to waiting safely

	‚úÖ 2. Avoiding Lost Signals
			With notify(): Wrong thread may be notified, Correct thread remains waiting indefinitely
			With notifyAll(): No missed signals, Correctness over performance
	
	‚úÖ 3. Simpler & Safer Code
			notifyAll() + while(condition) is the recommended pattern.
				synchronized (lock) {
					while (!condition) {
						lock.wait();
					}
				}

3Ô∏è‚É£ Performance Concern with notifyAll()
	Yes: Wakes all threads, Context switching overhead
	But: Correctness > performance, JVMs handle this efficiently, Over-optimizing with notify() causes bugs

4Ô∏è‚É£ What Happens If notify() Is Called When No Thread Is Waiting?
	Nothing happens ‚Äî the notification is lost.
	üîç Explanation
			synchronized (lock) {
				lock.notify(); // nobody waiting
			}
		Later:
			synchronized (lock) {
				lock.wait();   // waits forever
			}

		Why? Java does not queue notifications, notify has no memory, If no thread is waiting, signal is dropped
		‚ö† Result: Potential dead wait, Common concurrency bug

5Ô∏è‚É£ Correct Pattern to Avoid This Bug
	Always use: Condition variable, Loop, notifyAll
		synchronized (lock) {
			while (!condition) {
				lock.wait();
			}
		}
		synchronized (lock) {
			condition = true;
			lock.notifyAll();
		}


=============================================================================================================
üîπ Q23. What is spurious wakeup?
=============================================================================================================
üëâA spurious wakeup occurs when a thread wakes up from wait() without: notify(), notifyAll(), timeout, interrupt
 It‚Äôs rare, but explicitly allowed by the Java specification.
	‚ùì Why Do Spurious Wakeups Exist?
		Because: JVM implementations use low-level OS primitives, For performance reasons, For safety under complex scheduling scenarios
		The JVM is allowed to wake a thread at any time.

	üî• Key Rule: You must assume wait() can return at any time.

üëâWhy Spurious Wakeups Are Dangerous
	‚ùå Broken Code (Using if)
		synchronized (lock) {
			if (!condition) {
				lock.wait(); // may wake spuriously
			}
			// ‚ùå condition may still be false
		}
	If a spurious wakeup happens: Condition is not rechecked, Thread proceeds incorrectly, Data corruption / bugs

	‚úÖ Correct Code (Using while)
		synchronized (lock) {
			while (!condition) {
				lock.wait();
			}
			// ‚úÖ safe
		}

	‚úî Rechecks condition, Handles spurious wakeups, Handles wrong notifications

üëâRelationship Between notify & Spurious Wakeups
	Even without spurious wakeups:
		notify() may wake the wrong thread, That thread must recheck the condition, This is another reason while is mandatory.
		
=============================================================================================================
üîπ Q24. What problem does volatile solve? Does volatile guarantee atomicity? Can volatile replace synchronization?
		Difference between volatile and synchronized.
=============================================================================================================
volatile solves the memory visibility problem, not atomicity.
	üîç The Actual Problem: Visibility
			In multithreaded programs: 
				Each thread may cache variables in CPU registers / core-local caches
				Updates made by one thread may not be visible to others immediately
							class Worker {
								boolean running = true;

								void stop() {
									running = false;
								}

								void work() {
									while (running) {
										// may loop forever!
									}
								}
							}
				Even after stop() is called: work() thread may keep seeing cached running = true

		‚úÖ How volatile Fixes This
			volatile boolean running = true;
			volatile guarantees: Reads always come from main memory, Writes are immediately flushed to main memory
			üëâ All threads see the latest value.
			üî• Interview Line : ‚Äúvolatile ensures visibility of changes across threads by preventing caching and reordering.‚Äù

2Ô∏è‚É£ Does volatile Guarantee Atomicity?
	‚ùå NO ‚Äî volatile does NOT guarantee atomicity: This is one of the most important points.
	‚ùå Example (Broken Even with volatile)
		volatile int count = 0;
		count++; // NOT atomic
	
	Why? count++ is actually: Read count, Increment, Write back, Multiple threads can interleave these steps ‚Üí lost updates.

	‚úÖ What volatile is Atomic For: ‚úî Single read, ‚úî Single write
		volatile int x = 10; // atomic write
		int y = x;           // atomic read
		But not compound operations.

3Ô∏è‚É£ Can volatile Replace Synchronization?
	Sometimes ‚Äî but only in very limited cases.
	‚úÖ When volatile Is Enough
		Use volatile when: Only one thread writes, Other threads only read, No compound operations, No invariants to protect
		Example: volatile boolean shutdown = false;
		‚úî Safe, Efficient, No locking overhead
		‚ùå When volatile Is NOT Enough: 
			Multiple writers, Increment/decrement, Check-then-act logic, Protecting invariants, Multiple shared variables
			Example:
				if (initialized) {   // check
					useResource();   // act
				}
			Needs synchronization.
		üî• Senior Insight: 	‚ÄúIf you need mutual exclusion, volatile is the wrong tool.‚Äù

4Ô∏è‚É£ Difference Between volatile and synchronized
	| Aspect            | `volatile`   | `synchronized`      |
	| ----------------- | -------------| ------------------- |
	| Solves visibility | ‚úÖ Yes       | ‚úÖ Yes    	         |
	| Solves atomicity  | ‚ùå No        | ‚úÖ Yes       	     |
	| Mutual exclusion  | ‚ùå No        | ‚úÖ Yes        	     |
	| Thread blocking   | ‚ùå No        | ‚úÖ Yes         	 |
	| Performance       | Very fast    | Slower (lock-based) |
	| Use case          | State flags  | Critical sections   |
	| Fairness          | ‚ùå No        | ‚ùå No       	     |
	| Deadlock risk     | ‚ùå No        | ‚úÖ Yes    	         |

	üîπ Memory Semantics Difference
		volatile: Prevents instruction reordering, Establishes happens-before on read/write
		synchronized: Prevents reordering, Establishes happens-before on lock acquire/release, Also provides mutual exclusion

5Ô∏è‚É£ Visual Mental Model (Senior-Level)
	volatile ‚Üí visibility + ordering
	synchronized ‚Üí visibility + ordering + atomicity

6Ô∏è‚É£ Common Interview Traps (Avoid These ‚ùå)
	‚ùå ‚Äúvolatile makes code thread-safe‚Äù, ‚Äúvolatile is lighter synchronized‚Äù, ‚Äúvolatile can replace locks everywhere‚Äù
	‚úî Each tool has a specific purpose.


=============================================================================================================
üîπ Q25. How volatile works internally (CPU cache + memory barriers)? Why volatile int i++ is not thread safe?
=============================================================================================================
1Ô∏è‚É£ How volatile Works Internally (CPU Cache + Memory Barriers)
	üîπ The Core Problem: CPU Caches
		Modern CPUs don‚Äôt read/write variables directly from RAM.
		Each core has: Registers, L1 / L2 / L3 caches
		So in multithreading: volatile boolean flag = true;
		Thread A may update flag in its CPU cache, Thread B may keep reading a stale cached value
		üëâ This is the visibility problem.

2Ô∏è‚É£ What volatile Actually Does
	When you mark a variable as volatile, the JVM enforces special memory semantics using memory barriers (fences).
	üî• Two Guarantees Provided by volatile: Visibility, Ordering (no dangerous reordering)

3Ô∏è‚É£ Memory Barriers (Key Concept)
	A memory barrier is a CPU instruction that restricts how memory operations can be reordered and cached.
	üîπ Volatile Write (volatileVar = x)
		JVM inserts a Store Barrier: Flushes the value to main memory, Invalidates cached copies in other cores
		Prevents previous writes from being reordered after the volatile write
			[Normal writes]
				‚Üì
			StoreStore Barrier
				‚Üì
			volatile write
				‚Üì
			StoreLoad Barrier

	üîπ Volatile Read (x = volatileVar)
		JVM inserts a Load Barrier: Forces read from main memory, Prevents subsequent reads from being reordered before the volatile read
			volatile read
				‚Üì
			LoadLoad Barrier
				‚Üì
			[Normal reads]

4Ô∏è‚É£ Happens-Before Guarantee (Very Important)
	The Java Memory Model defines: A write to a volatile variable happens-before every subsequent read of that same variable.
	This is why volatile ensures visibility + ordering.

5Ô∏è‚É£ What Volatile Does NOT Do: 
	‚ùå Does NOT lock, Does NOT block threads, Does NOT provide mutual exclusion, Does NOT make compound operations atomic
	This leads us to the next question üëá

6Ô∏è‚É£ Why volatile int i++ Is NOT Thread Safe
	üîπ The Myth Many think:
		volatile int i; i++; is thread-safe because i is volatile.
		‚ùå This is wrong.

	üîç What i++ Actually Means: i++ is equivalent to:
		int temp = i;   // volatile read
		temp = temp + 1;
		i = temp;       // volatile write

	That‚Äôs 3 separate operations.

üî• The Problem: Lost Updates
		Consider two threads:
				volatile int i = 0;
				Thread A reads i = 0, increments to 1
				Thread B reads i = 0, increments to 1
				Both write back 1.
		üëâ Final value = 1, not 2, 	üëâ One update is lost
		Even though: Each read/write is visible, Operations are ordered, There is no mutual exclusion.
		
		8Ô∏è‚É£ Correct Ways to Fix i++
			‚úÖ Option 1: synchronized
					synchronized void increment() {
						i++;
					}

			‚úÖ Option 2: AtomicInteger (Best Practice)
					AtomicInteger i = new AtomicInteger(0);
					i.incrementAndGet();
					Uses CAS (Compare-And-Swap) + memory barriers.

			‚úÖ Option 3: LongAdder (High Contention)
					LongAdder adder = new LongAdder();
					adder.increment();

		9Ô∏è‚É£ When Volatile IS the Right Tool: ‚úî State flags, Shutdown signals, Configuration reload flags, One writer, many readers
		Example: volatile boolean shutdown;

=============================================================================================================
üîπ Q26. Can volatile be used with objects? Difference between visibility vs atomicity.
=============================================================================================================
‚úÖ Yes, volatile can be used with object references
	volatile MyObject obj; But here‚Äôs the critical catch üëá
	üî• What volatile Guarantees for Objects
		volatile on an object does NOT make the object thread-safe.
		It guarantees only that: The reference is visible to all threads
		Updates to the reference are immediately visible, The reference is safely published
		üëâ It does NOT protect the object‚Äôs internal state.

		üß™ Example: Safe Reference Visibility, Unsafe State
		class Holder {
			int x;
		}
		volatile Holder holder;
		Thread A:
				holder = new Holder();
				holder.x = 10;
		Thread B:
				Holder h = holder; // guaranteed to see latest reference
				System.out.println(h.x); // ‚ùå may still be unsafe without synchronization

		‚úî Thread B sees the latest object reference, 
		‚ùå No guarantee that fields inside the object are safely updated unless:
				Fields are final, Or access is synchronized, Or object is immutable

‚úÖ When volatile with objects IS useful
	‚úî Safe publication of immutable objects, One-time initialization, Configuration reloads, Replacing whole object atomically
	volatile Config config;
	config = new Config(...); // safe publication

‚ùå When volatile is NOT enough
		‚ùå Mutating object fields, Multiple threads modifying same object, Protecting invariants

2Ô∏è‚É£ Difference Between Visibility vs Atomicity
	| Aspect                 | Visibility   | Atomicity         |
	| ---------------------- | ------------ | ----------------- |
	| Problem                | Stale data   | Lost updates      |
	| Solved by volatile     | ‚úÖ Yes  	    | ‚ùå No   	        |
	| Solved by synchronized | ‚úÖ Yes	    | ‚úÖ Yes    	    |
	| Needs locking          | ‚ùå Not always | ‚úÖ Usually 	    |
	| Example                | Stop flag    | Counter increment |



=============================================================================================================
üîπ Q27. Explain heap, stack, registers, CPU cache in multithreading.
=============================================================================================================
1Ô∏è‚É£ Heap (Shared Memory): 
	Heap is a shared memory area which Stores: Objects, Instance variables, Static variables, Managed by Garbage Collector
		class User {
			int age;
		}
		User u = new User(); // object on heap

	üîπ Heap in Multithreading: ‚úî Shared across all threads, ‚ùå Not thread-safe by default
		u.age = 30; // Thread A
		System.out.println(u.age); // Thread B
		Without synchronization: Thread B may see stale value Or partial state (unsafe publication)
	üî• Key Interview Line: ‚ÄúHeap is shared among threads, which is why synchronization is required.‚Äù

2Ô∏è‚É£ Stack (Thread-Local Memory)
	Each thread has its own stack. Stack contains: Method call frames, Local variables, References to heap objects
		void process() {
			int x = 10;     // stack
			User u = new User(); // reference on stack, object on heap
		}
	
	üîπ Stack in Multithreading: ‚úî Stack is thread-confined, ‚úî No synchronization needed, ‚úî Fast access
		int counter = 0; // local variable ‚Üí thread-safe
	üî• Key Interview Line: ‚ÄúLocal variables live on the stack and are inherently thread-safe.‚Äù

3Ô∏è‚É£ CPU Registers (Per Thread, Ultra Fast)
	Smallest, fastest memory, Located inside CPU core
	Hold: Temporary variables, Instruction operands, Loop counters, JVM & JIT may keep variables in registers.
	
	üîπ Registers in Multithreading: ‚úî Private to a CPU core / thread, ‚ùå Other threads cannot see register values
		while (running) { } // running may stay in register!
		This is why visibility problems happen.
	üî• Key Interview Line: ‚ÄúThreads may read variables from registers instead of main memory.‚Äù

4Ô∏è‚É£  CPU Cache (Root Cause of Visibility Bugs)
	Faster than RAM, slower than registers
	Levels: L1 (per core), L2 (per core), L3 (shared)
	Each core may cache memory independently.
	
	üîπ CPU Cache in Multithreading
		Thread A: sharedFlag = false;
		Thread B: while (sharedFlag) { }
		Problem: Thread B may keep reading cached true, Never sees update by Thread A

	üî• This Is the Visibility Problem, Solved by: volatile, synchronized, Locks, Atomic variables

5Ô∏è‚É£  How Java Fixes This (JMM Rules)
	üîπ synchronized: Flushes thread‚Äôs writes to heap on exit, Invalidates other threads‚Äô caches on entry
	üîπ volatile: Forces reads/writes to main memory, Inserts memory barriers
	üîπ Happens-Before: Defines when writes become visible to other threads

7Ô∏è‚É£ Putting It All Together (Very Important)
	| Memory Area | Shared?       | Thread-Safe? | Used For                |
	| ----------- | --------------| ------------ | ------------------------|
	| Heap        | ‚úÖ Yes        | ‚ùå No    	 | Objects, shared state   |
	| Stack       | ‚ùå No         | ‚úÖ Yes     	 | Local variables         |
	| Registers   | ‚ùå No         | ‚úÖ Yes     	 | Fast execution          |
	| CPU Cache   | ‚ùå (per core) | ‚ùå No      	 | Performance optimization|

8Ô∏è‚É£ Why Multithreading Bugs Happen (Senior Insight)
	Because: 
		Threads don‚Äôt read from heap every time
		CPU caches + registers cause stale reads
		JVM allows reordering for performance
	üëâ Java gives tools to restore correctness, but you must use them correctly.

=============================================================================================================
üîπ Q28. What are memory barriers / fences? Explain happens-before rules.
=============================================================================================================
1Ô∏è‚É£  Memory barriers (or fences) are low-level CPU instructions that restrict reordering of memory operations and control visibility of reads/writes across threads.
	üëâ They tell the CPU and compiler: ‚ÄúDo NOT move memory operations across this point.‚Äù

2Ô∏è‚É£ Why Memory Barriers Exist
	Modern systems optimize aggressively: CPUs reorder instructions, Compilers reorder instructions, CPU cores cache values locally
	These optimizations can break multithreaded correctness.
	Memory barriers restore order and visibility when required.

3Ô∏è‚É£ Types of Memory Barriers (Conceptual)
	You‚Äôll hear these names in JVM discussions:
		üîπ 1. LoadLoad Barrier: Prevents loads after the barrier from moving before it
				Load A, LoadLoad, Load B   // cannot move before Load A

		üîπ 2. StoreStore Barrier: Prevents stores after the barrier from moving before it
				Store A, StoreStore, Store B  // cannot move before Store A
		
		üîπ 3. LoadStore Barrier: Prevents stores after the barrier from moving before earlier loads
		üîπ 4. StoreLoad Barrier (Strongest & Most Expensive): Prevents loads after the barrier from moving before earlier stores
				Store A, StoreLoad, Load B  // cannot move before Store A
			‚ö† This is the most expensive fence and is critical for volatile.

4Ô∏è‚É£ How Java Uses Memory Barriers: Java does not expose memory barriers directly, but inserts them under the hood for:
	‚úî volatile
			Volatile write ‚Üí StoreStore + StoreLoad barriers
			Volatile read ‚Üí LoadLoad + LoadStore barriers

	‚úî synchronized
			Lock acquire ‚Üí Load barriers
			Lock release ‚Üí Store barriers
	
	‚úî Atomic classes (CAS): Use volatile semantics + CPU fences

5Ô∏è‚É£ What Is the Happens-Before Relationship?
	üîπ Definition : If action A happens-before action B, then all effects of A are visible to B, and A is ordered before B.
		Happens-before is a guarantee, not a timing statement.

6Ô∏è‚É£ Core Happens-Before Rules (Must Know)
	These rules explain when visibility is guaranteed.
		üî• 1. Program Order Rule: Within a single thread, actions happen in program order.
				x = 10;
				y = 20;
				‚úî x = 10 happens-before y = 20 (in same thread)

		üî• 2. Monitor Lock Rule (synchronized): An unlock on a monitor happens-before every subsequent lock on the same monitor
				synchronized (lock) {
					shared = 42;
				} // unlock

				synchronized (lock) {
					System.out.println(shared); // guaranteed 42
				}
				‚úî Visibility guaranteed, ‚úî Ordering guaranteed

		üî• 3. Volatile Variable Rule: A write to a volatile variable happens-before every subsequent read of that variable
				volatile boolean ready = false;
				Thread A: data = 100; ready = true;
				Thread B: while (!ready) {} System.out.println(data); // guaranteed 100
				‚úî This is the classic safe publication pattern.

		üî• 4. Thread Start Rule: Calling thread.start() happens-before any action in the started thread
				t.start();
				// everything before start() visible to t

		üî• 5. Thread Termination Rule: All actions in a thread happen-before another thread detects termination via: join()
				isAlive() == false

				t.join();
				// all t‚Äôs writes visible here

		üî• 6. Transitivity Rule
				If: A happens-before B, B happens-before C Then: A happens-before C
				This allows chaining guarantees.

7Ô∏è‚É£ Why Happens-Before Matters (Senior Insight)
	Without happens-before: Threads may see stale values, Objects may appear partially constructed, Bugs become non-deterministic
	Happens-before defines what is legal, not what is fast.

8Ô∏è‚É£ Happens-Before vs Synchronization (Clarification)
	| Concept        | Purpose                    |
	| -------------- | -------------------------- |
	| Memory barrier | Low-level CPU constraint   |
	| Happens-before | High-level JMM rule        |
	| synchronized   | Establishes happens-before |
	| volatile       | Establishes happens-before |

=============================================================================================================
üîπ Q29. How does synchronization affect memory visibility? Why double-checked locking was broken earlier?
=============================================================================================================
1Ô∏è‚É£ Synchronization establishes a happens-before relationship that guarantees memory visibility between threads.
	üîç What Problem Exists Without Synchronization?
		Threads may: Read values from CPU registers / caches, See stale data, Observe partially constructed objects
		Because the JVM and CPU are free to: Cache variables, Reorder instructions
	
	üîë What Synchronization Guarantees
		‚úÖ 1. Visibility: Writes by one thread become visible to other threads
		‚úÖ 2. Ordering: Instructions are not reordered across synchronization boundaries

	üî¨ What Happens Internally (Very Important)
		‚ñ∂ On lock acquisition (monitorenter): Thread invalidates its local caches, Reads fresh values from main memory
		‚ñ∂ On lock release (monitorexit): Thread flushes all writes to main memory
			üß™ Example
						int shared = 0;

						synchronized (lock) {
							shared = 42;
						} // unlock ‚Üí writes flushed to main memory

						synchronized (lock) {
							System.out.println(shared); // guaranteed to see 42
						}
			This works because: Unlock happens-before subsequent lock on the same monitor

	üéØ Interview Line :	‚ÄúSynchronization provides memory visibility by flushing writes on lock release and invalidating caches on lock acquisition.‚Äù

2Ô∏è‚É£ Why Was Double-Checked Locking Broken Earlier?
	This is where deep JVM knowledge shows.

3Ô∏è‚É£ What Is Double-Checked Locking?
class Singleton {
    private static Singleton instance;

    public static Singleton getInstance() {
        if (instance == null) {              // 1st check
            synchronized (Singleton.class) {
                if (instance == null) {      // 2nd check
                    instance = new Singleton();
                }
            }
        }
        return instance;
    }
}


Intent: Avoid synchronization after initialization, Improve performance

4Ô∏è‚É£ Why It Was Broken (Before Java 5)
	üî• Root Cause: Instruction Reordering + Visibility
	Object creation is not atomic.
	This line: instance = new Singleton(); can be reordered internally as: Allocate memory, Assign reference to instance, Run constructor

‚ö† Steps 2 and 3 could be reordered.

üß™ Broken Scenario (Very Important)
		Thread A
		instance = new Singleton();

		Reordering occurs: instance points to memory, Constructor not finished yet
		
		Thread B
		if (instance != null) {
			return instance; // sees partially constructed object
		}


		üëâ Thread B observes: instance != null, But object fields are not initialized
		This caused: Random crashes, Weird null values, Impossible-looking bugs
	
	üéØ Key Insight: Synchronization alone did NOT prevent reordering before Java 5.

5Ô∏è‚É£ Why Java 5+ Fixed It
	Java 5 introduced: A stronger Java Memory Model, clear happens-before guarantees, Proper volatile semantics

6Ô∏è‚É£ Correct Double-Checked Locking (Java 5+)
		class Singleton {
			private static volatile Singleton instance;

			public static Singleton getInstance() {
				if (instance == null) {
					synchronized (Singleton.class) {
						if (instance == null) {
							instance = new Singleton();
						}
					}
				}
				return instance;
			}
		}
	üîë Why volatile Fixes It: volatile guarantees: No instruction reordering, Safe publication, Visibility of fully constructed object
	Specifically: Constructor must complete before reference is visible

7Ô∏è‚É£ Why synchronized Alone Was Not Enough
	Before Java 5: JVM allowed reordering across synchronization boundaries, Memory model was weak and underspecified
	After Java 5: Lock release + acquire creates strong happens-before, Volatile adds StoreLoad memory barriers

8Ô∏è‚É£ Modern Best Practice (Senior Tip)
	Instead of double-checked locking:
		‚úÖ Use Initialization-on-Demand Holder Idiom
		class Singleton {
			private Singleton() {}

			private static class Holder {
				static final Singleton INSTANCE = new Singleton();
			}

			public static Singleton getInstance() {
				return Holder.INSTANCE;
			}
		}

		‚úî Thread-safe, ‚úî Lazy, ‚úî No explicit synchronization, ‚úî Clean and fast


=============================================================================================================
üîπ Q30. How does volatile affect instruction reordering? What is out-of-order execution?
=============================================================================================================
1Ô∏è‚É£ What Is Instruction Reordering?
	Instruction reordering is an optimization where the compiler, JVM, or CPU changes the execution order of instructions to improve performance, as long as single-threaded semantics are preserved.

	Key point: Reordering is legal within a thread, but dangerous across threads.

	üß™ Simple Example
					a = 1;
					b = 2;
			The JVM/CPU may reorder this to:
					b = 2;
					a = 1;
			‚úî Same result in one thread, ‚ùå Can break visibility in multi-threading

2Ô∏è‚É£ What Is Out-of-Order Execution?
	Out-of-order execution is a CPU-level optimization where the processor executes instructions as soon as their operands are ready, rather than strictly following program order.
	This is done to: Keep CPU pipelines busy, Reduce stalls, Improve throughput
	
	üîç Important Distinction
		| Term                   | Level                 |
		| ---------------------- | --------------------- |
		| Instruction reordering | Compiler / JVM / CPU  |
		| Out-of-order execution | CPU microarchitecture |

	üëâ Out-of-order execution is one cause of instruction reordering.

	üß† Example (CPU Perspective)
					x = y + 1;   // depends on y
					a = 5;       // independent
			
			CPU may execute: 
					a = 5
					x = y + 1
			Even if written in opposite order.

3Ô∏è‚É£ Why Reordering Is a Problem in Multithreading
	Consider this classic pattern:
			int data;
			boolean ready;

			Thread A: data = 42; ready = true;
			Thread B: if (ready) {System.out.println(data);}
		
		‚ùå Without proper memory barriers: Reordering may happen:
			ready = true;
			data = 42;

			Thread B sees: ready == true, but data == 0
		üëâ Visibility + ordering bug

4Ô∏è‚É£ How Does volatile Affect Instruction Reordering?
	üî• Core Answer: volatile prevents specific types of instruction reordering by inserting memory barriers around volatile reads and writes.

5Ô∏è‚É£ Memory Barriers Inserted by volatile
	üîπ Volatile Write: volatileVar = value;
		JVM inserts: 
			StoreStore barrier ‚Üí prevents previous writes from moving after
			StoreLoad barrier ‚Üí prevents subsequent reads from moving before
		
		Guarantee: All writes before the volatile write become visible before the volatile write.

	üîπ Volatile Read: value = volatileVar;
		JVM inserts:
			LoadLoad barrier ‚Üí prevents later reads from moving before
			LoadStore barrier ‚Üí prevents later writes from moving before
		
		Guarantee: All reads after the volatile read see effects after the volatile read.

6Ô∏è‚É£ Happens-Before Rule (Volatile) : 
	A write to a volatile variable happens-before every subsequent read of that same variable.
	This is the formal JMM rule that volatile enforces.

7Ô∏è‚É£ Visual Timeline (Senior-Level)
	Without volatile (broken)
		Thread A: data = 42   ready = true
		Thread B: if (ready) print(data) // may print 0

	With volatile (correct)
		Thread A: data = 42  ‚Üí volatile write ready = true
		Thread B: volatile read ready ‚Üí print(data) // guaranteed 42

8Ô∏è‚É£ What volatile Does NOT Prevent
	‚ùå Does not prevent: All reordering everywhere, Race conditions, Lost updates, Non-atomic compound operations
			volatile int i;
			i++; // still unsafe

9Ô∏è‚É£ Why Out-of-Order Execution Still Exists with volatile
	Important clarification: 
		CPUs still execute out-of-order internally
		Memory barriers ensure results appear ordered to other threads
	üëâ Volatile controls visibility and ordering, not CPU internals.

=============================================================================================================
üîπ Q31. How was it fixed in Java 5+? What is safe publication?
=============================================================================================================
1Ô∏è‚É£ Java 5 fixed concurrency issues by introducing a stronger Java Memory Model that clearly defined happens-before rules and strengthened volatile semantics. This ensured proper memory visibility, prevented illegal instruction reordering, and made patterns like double-checked locking safe when used with volatile. Safe publication refers to the practice of making an object visible to other threads only after it is fully constructed, ensuring that all threads see a consistent and initialized state. This can be achieved using volatile references, synchronization, final fields, static initialization, or thread start semantics.

2Ô∏è‚É£ What Is Safe Publication?
üîπ Ensuring that an object is made visible to other threads only after it is fully constructed, so that all threads see a consistent and initialized state.
	Ways to Achieve Safe Publication (Must Know)
	‚úÖ 1. Using volatile Reference: 
		volatile Holder holder; holder = new Holder(); // safely published
		‚úî Fully constructed before visible, ‚úî Very common pattern

	‚úÖ 2. Using synchronized
			synchronized void init() {
				holder = new Holder();
			}

			synchronized Holder get() {
				return holder;
			}
		‚úî Lock release ‚Üí lock acquire establishes happens-before

	‚úÖ 3. Using final Fields (Immutable Objects)
			class Config {
				final int port;

				Config(int port) {
					this.port = port;
				}
			}
		‚úî Java 5+ guarantees visibility of final fields, ‚úî Best practice

	‚úÖ 4. Static Initialization (Class Loading)
			class Singleton {
				static final Singleton INSTANCE = new Singleton();
			}

		‚úî Class initialization is thread-safe, ‚úî Happens-before guaranteed by JVM

	‚úÖ 5. Thread Start Rule
			Holder h = new Holder();
			new Thread(() -> use(h)).start();
		‚úî Everything before start() is visible to the new thread


=============================================================================================================
üîπ Q32. What is deadlock? Conditions required for deadlock. Real-world deadlock example. How to prevent deadlock?
=============================================================================================================
1Ô∏è‚É£ Deadlock is a situation where two or more threads are permanently blocked, each waiting for a resource held by another, so none of them can make progress.
	üëâ The system is alive but stuck forever.
	
2Ô∏è‚É£ Conditions Required for Deadlock (Coffman Conditions)
	Deadlock occurs only if ALL four conditions are true.
		üî¥ 1. Mutual Exclusion: At least one resource is non-shareable, Only one thread can hold it at a time
				synchronized (lock) { }
		
		üî¥ 2. Hold and Wait: Thread holds one resource, While waiting for another
				lockA acquired ‚Üí waiting for lockB
		
		üî¥ 3. No Preemption: Resource cannot be forcibly taken, Thread releases it voluntarily only, Java locks cannot be preempted.

		üî¥ 4. Circular Wait: Circular chain of threads waiting for resources
				T1 ‚Üí lockA ‚Üí lockB
				T2 ‚Üí lockB ‚Üí lockA
			üéØ Interview Line: ‚ÄúDeadlock occurs only when all four Coffman conditions are satisfied simultaneously.‚Äù

3Ô∏è‚É£ Real-World Deadlock Example
	üö¶ Traffic Intersection (Classic)
			Car A blocks road 1, waiting for road 2
			Car B blocks road 2, waiting for road 1
		Neither can move ‚û° Deadlock

	üí≥ Banking System Example
			Thread A locks Account X, waits for Account Y
			Thread B locks Account Y, waits for Account X
		Money transfer system freezes

	üß† Database Example
			Transaction 1 locks row A, waits for row B
			Transaction 2 locks row B, waits for row A
		DB detects deadlock and kills one transaction
	(Java does not auto-resolve deadlocks.)

4Ô∏è‚É£ How to Prevent Deadlock (Most Important): Deadlock prevention = break at least ONE Coffman condition.
	
	‚úÖ 1. Lock Ordering (Best & Most Common): Always acquire locks in a fixed global order.
			synchronized (lockA) {
				synchronized (lockB) {
					// safe
				}
			}
		‚úî If everyone follows same order ‚Üí no circular wait
		üéØ Interview Line: ‚ÄúConsistent lock ordering is the most effective deadlock prevention technique.‚Äù

	‚úÖ 2. Use tryLock() with Timeout (Very Important): 
			if (lockA.tryLock(1, TimeUnit.SECONDS)) {
				try {
					if (lockB.tryLock(1, TimeUnit.SECONDS)) {
						try {
							// work
						} finally {
							lockB.unlock();
						}
					}
				} finally {
					lockA.unlock();
				}
			}
		‚úî Breaks hold and wait, ‚úî Avoids infinite blocking

	‚úÖ 3. Avoid Nested Locks
		‚ùå Bad:
			synchronized (A) {
				synchronized (B) { }
			}
		‚úî Better: Reduce lock scope, Refactor design

	‚úÖ 4. Use High-Level Concurrency Utilities
		Instead of manual locking: BlockingQueue, ConcurrentHashMap, Semaphore, ExecutorService
		They are designed to avoid deadlocks.

	‚úÖ 5. Use Single Lock Where Possible
		synchronized (globalLock) {
			// protect related resources together
		}
		‚úî Fewer locks ‚Üí fewer deadlocks

	‚úÖ 6. Deadlock Detection (Not Prevention)
		In production: Use jstack, JVM thread dump, Monitoring tools
		But: Detection is recovery, not prevention

=============================================================================================================
üîπ Q33. Difference between deadlock and livelock.
=============================================================================================================
1Ô∏è‚É£ What Is Deadlock? 
	Deadlock occurs when two or more threads are permanently blocked, each waiting for a resource held by another.
	üß† Key Behavior: Threads are blocked, No CPU usage, System appears frozen
	üß™ Example (Deadlock)
					synchronized (lockA) {
						synchronized (lockB) { }
					}

					synchronized (lockB) {
						synchronized (lockA) { }
					}
		Thread-1 holds lockA ‚Üí waits for lockB
		Thread-2 holds lockB ‚Üí waits for lockA

2Ô∏è‚É£ What Is Livelock? 
	Livelock occurs when threads are not blocked, but they keep responding to each other in a way that prevents progress.
	üß† Key Behavior: Threads are running, CPU usage is high, System is active but useless
	üß™ Example (Livelock with tryLock)
				while (true) {
					if (lockA.tryLock()) {
						if (lockB.tryLock()) {
							// work
							break;
						}
						lockA.unlock();
					}
					// retry politely
				}
		Both threads: Acquire one lock, Release it to avoid deadlock, Retry at the same time, Repeat forever

3Ô∏è‚É£ Real-World Analogy (Interview Gold)
	üö¶ Deadlock: Two cars meet head-on on a narrow road, neither can move
	üö∂ Livelock:
			Two people try to pass each other in a hallway
			Both step left, then right, repeatedly
			Both are moving, but no one passes
4Ô∏è‚É£ Deadlock vs Livelock ‚Äî Comparison Table
	| Aspect             | Deadlock         | Livelock                    |
	| ------------------ | ---------------- | --------------------------- |
	| Thread state       | BLOCKED          | RUNNABLE                    |
	| CPU usage          | Low / none       | High                        |
	| Progress           | ‚ùå None   	    | ‚ùå None           	      |
	| Threads doing work | ‚ùå No   	        | ‚úÖ Yes           	          |
	| Cause              | Circular waiting | Over-reaction / retry logic |
	| Visibility         | Easy to detect   | Harder to detect            |

5Ô∏è‚É£ How to Prevent Them
	üîπ Prevent Deadlock: Consistent lock ordering, tryLock() with timeout, Reduce nested locks
	üîπ Prevent Livelock: Add random backoff / delay, Limit retries, Introduce priority or fairness
			Thread.sleep(randomDelay());

=============================================================================================================
üîπ Q34. What is starvation? Can synchronized cause starvation? How ReentrantLock helps avoid starvation?
=============================================================================================================
1Ô∏è‚É£ Starvation happens when a thread is perpetually denied access to a shared resource, so it never gets a chance to execute, even though the system as a whole keeps running.
	üëâ The thread is alive, but never makes progress.
	üß† Key Characteristics: 
		Thread waits indefinitely, Other threads keep acquiring the resource, 
		No circular dependency (unlike deadlock), Often caused by unfair scheduling or locking

	üß™ Simple Example (Conceptual)
			High-priority threads keep acquiring a lock
			Low-priority thread keeps waiting
			Low-priority thread starves

2Ô∏è‚É£ Can synchronized Cause Starvation?
	‚úÖ Yes ‚Äî synchronized can cause starvation
	Because synchronized: Uses intrinsic (monitor) locks, Does NOT guarantee fairness, Does NOT enforce FIFO ordering
	Lock acquisition depends on: JVM implementation, OS thread scheduler, Timing and CPU availability
	üëâ A thread that has been waiting longer has no priority over a newly arrived thread.

		üß™ Example Scenario
				synchronized (lock) {
					// critical section
				}
		Thread A repeatedly acquires/releases the lock quickly
		Thread B is slower and keeps getting bypassed
		Thread B may never acquire the lock

	üéØ Interview Line: ‚Äúsynchronized ensures mutual exclusion, but it does not guarantee fairness, so starvation is possible.‚Äù

3Ô∏è‚É£ Other Common Causes of Starvation
	Thread priorities (high-priority threads dominate), Infinite loops holding locks, Excessive contention, Unfair locks, Busy retry loops

4Ô∏è‚É£ How Does ReentrantLock Help Avoid Starvation?
	üî• Key Advantage: Fair Locking
			ReentrantLock allows you to create a fair lock.
				Lock lock = new ReentrantLock(true); // fair lock

	üîπ What Is a Fair Lock? 
		A fair lock grants access in FIFO order, The longest-waiting thread gets the lock next, Prevents newer threads from ‚Äúcutting in line‚Äù
		üëâ This directly reduces starvation.

	üß† How It Works (High Level)
		Threads are queued internally
		When lock is released: JVM gives it to the next waiting thread, No barging (or minimal barging)
		
	üß™ Fair vs Unfair ReentrantLock
	| Aspect          | Fair Lock      | Unfair Lock  |
	| --------------- | -------------- | ------------ |
	| Starvation risk | ‚ùå Very low    | ‚úÖ Possible |
	| Throughput      | Lower          | Higher       |
	| Ordering        | FIFO           | No guarantee |
	| Use case        | Predictability | Performance  |

	‚ö† Important Trade-off (Must Mention): Fair locks reduce starvation but also reduce throughput.
		Because: More context switching, Less opportunity for CPU-local optimizations
		That‚Äôs why: synchronized and default ReentrantLock are unfair by default

5Ô∏è‚É£ Why synchronized Has No Fair Option
	Because it is designed for: Simplicity, Performance, JVM-level optimizations
	Fairness: Adds overhead, Reduces scalability, Is not always required
	So Java leaves fairness control to explicit locks.

=============================================================================================================
üîπ Q35. How do you detect deadlock in Java?  Tools to detect deadlock in production?
=============================================================================================================
1Ô∏è‚É£ How Do You Detect Deadlock in Java?
	üîπ Core Idea: A deadlock is detected by analyzing thread states and lock ownership to find a circular wait.
		In Java, this is typically done using: Thread dumps, JVM tools, Monitoring systems

2Ô∏è‚É£ Detecting Deadlock Using Thread Dump (Most Important)
	‚úÖ Using jstack (Primary Tool) : jstack <pid> or jstack -l <pid>
		üîç What to Look for in Thread Dump
			JVM clearly reports deadlocks like this:
				Found one Java-level deadlock:
				=============================
				"Thread-1":
				  waiting to lock monitor 0x000000001 (object A)
				  which is held by "Thread-2"

				"Thread-2":
				  waiting to lock monitor 0x000000002 (object B)
				  which is held by "Thread-1"

			üëâ If you see ‚ÄúFound one Java-level deadlock‚Äù, that‚Äôs a confirmed deadlock.
			üß† Thread States Involved: Threads usually in BLOCKED state, Each thread holds one lock and waits for another

	üéØ Interview Line : ‚ÄúThe first step to detect a deadlock is taking a thread dump and looking for circular lock dependencies.‚Äù

3Ô∏è‚É£ Using jconsole (Live Detection)
	üîπ Steps: Start jconsole, Connect to JVM, Go to Threads tab, Click Detect Deadlock
	‚úî JVM automatically checks monitor ownership, ‚úî Shows exact threads and locks involved
	üß† When to Use: Non-production, Lower environments, Live debugging with minimal impact

4Ô∏è‚É£ Using jvisualvm
	üîπ Features:Visual thread states, Lock contention graphs, Deadlock detection button
		Steps: Open jvisualvm, Attach to process, Go to Threads, Click Thread Dump / Detect Deadlock
		‚úî Easier visualization, ‚úî Useful during analysis

5Ô∏è‚É£ Production-Grade Deadlock Detection Tools
	üî• 1. Application Performance Monitoring (APM)
			Common tools: Dynatrace, New Relic, AppDynamics, Datadog APM
			They provide: Automatic deadlock detection, Thread contention analysis, Alerts when deadlock occurs
		üéØ Interview Tip: ‚ÄúIn production, deadlocks are usually detected via APM tools or automated thread dump analysis.‚Äù

	üî• 2. JVM Diagnostic Commands (Low Impact)
						jcmd
						jcmd <pid> Thread.print
		‚úî Preferred over jstack in production, ‚úî Lower impact, ‚úî Safe to run on live systems

6Ô∏è‚É£ Programmatic Deadlock Detection (Advanced)
	üîπ Using ThreadMXBean
				ThreadMXBean bean = ManagementFactory.getThreadMXBean();
				long[] ids = bean.findDeadlockedThreads();

				if (ids != null) {
					ThreadInfo[] infos = bean.getThreadInfo(ids);
					for (ThreadInfo info : infos) {
						System.out.println(info);
					}
				}
		‚úî Used in: Health checks, Monitoring agents, Self-diagnosing systems

8Ô∏è‚É£ Best Practices in Production
	‚úÖ What to Do: 
			Enable thread dumps on alerts, Capture multiple thread dumps, Use APM tools
			Log lock acquisition time, Prefer tryLock with timeout

	‚ùå What NOT to Do:
			Kill JVM blindly, Assume high CPU = deadlock
			Ignore BLOCKED threads

=============================================================================================================
üîπ Q36. Why was java.util.concurrent introduced? Difference between synchronized vs Lock interface.
=============================================================================================================
1Ô∏è‚É£ Why Was java.util.concurrent Introduced?
	Because synchronized alone was too low-level, inflexible, and hard to use correctly for complex concurrent programs.
	üîç Problems with synchronized (Pre‚ÄìJava 5 Era)
		Before Java 5, concurrency relied mainly on: synchronized, wait() / notify()
	
	This caused real production issues:
		‚ùå 1. No Fairness Control: Threads could starve, No FIFO ordering
		‚ùå 2. No Timeout or Try Lock: Threads could block forever, Deadlocks were hard to avoid
		‚ùå 3. Single Condition Queue: Only one wait set per monitor, Hard to model multiple conditions (producer/consumer)
		‚ùå 4. Hard to Compose Correctly: wait/notify easy to misuse, Lost notifications, Spurious wakeups, subtle bugs
		‚ùå 5. Poor Scalability: Coarse-grained locking, High contention, Bad performance on multi-core CPUs

	üî• What java.util.concurrent Solved: Introduced in Java 5
		it provided: Higher-level abstractions, Better performance, More expressive APIs, Safer concurrency patterns

	‚úÖ What It Added: 
			Lock, ReentrantLock, ReadWriteLock, Semaphore, CountDownLatch, CyclicBarrier
			ExecutorService, thread pools
			Concurrent collections (ConcurrentHashMap, BlockingQueue)
			Atomic variables (AtomicInteger, etc.)
		üëâ These reduce error-prone low-level locking.

üéØ Interview Line : ‚Äújava.util.concurrent was introduced to provide higher-level, safer, and more scalable concurrency abstractions than synchronized and wait/notify.‚Äù

2Ô∏è‚É£ Difference Between synchronized vs Lock Interface
	| Aspect              | `synchronized`   | `Lock` Interface             |
	| ------------------- | ---------------- | ---------------------------- |
	| Type                | Language keyword | API (library)                |
	| Lock acquisition    | Implicit         | Explicit                     |
	| Lock release        | Automatic        | Manual                       |
	| Fairness            | ‚ùå No    	     | ‚úÖ Optional        	        |
	| Try lock            | ‚ùå No    	     | ‚úÖ Yes             	        |
	| Timeout             | ‚ùå No    	     | ‚úÖ Yes              	        |
	| Multiple conditions | ‚ùå No    	     | ‚úÖ Yes (`Condition`) 	    |
	| Interruptible lock  | ‚ùå No    	     | ‚úÖ Yes                	    |
	| Risk of misuse      | Low              | Higher (if unlock forgotten) |

3Ô∏è‚É£ Key Differences Explained (With Insight)
	üî• 1. Lock Acquisition & Release
		synchronized
				synchronized (lock) {
					// lock acquired & released automatically
				}
		‚úî Simple ‚úî Exception-safe ‚ùå No flexibility
		
		Lock
			lock.lock();
			try {
				// critical section
			} finally {
				lock.unlock();
			}

		‚úî Flexible ‚ùå Developer must unlock manually

	üî• 2. Fairness (Big Advantage of Lock)
		Lock lock = new ReentrantLock(true); // fair lock
		Threads acquire lock in FIFO order, Helps avoid starvation, synchronized has no fairness guarantee.
	
	üî• 3. Try Lock & Timeout (Deadlock Avoidance)
			if (lock.tryLock(1, TimeUnit.SECONDS)) {
				try {
					// work
				} finally {
					lock.unlock();
				}
			}

		‚úî Prevents infinite blocking ‚úî Helps avoid deadlocks, Not possible with synchronized.
	
	üî• 4. Multiple Conditions (Very Important)
		With synchronized: Only one wait set, All threads use same wait()/notify()
		With Lock:
			Condition notEmpty = lock.newCondition();
			Condition notFull  = lock.newCondition();
		‚úî Separate condition queues, ‚úî Cleaner producer‚Äìconsumer logic

	üî• 5. Interruptible Lock Acquisition
			lock.lockInterruptibly();
		‚úî Thread can respond to interruption while waiting, synchronized threads cannot be interrupted while blocked.

4Ô∏è‚É£ Performance & Scalability: 
	synchronized is highly optimized in modern JVMs
		For simple use cases ‚Üí perfectly fine
		For complex coordination ‚Üí Lock scales better
		üëâ This is not about speed alone ‚Äî it‚Äôs about control.

5Ô∏è‚É£ When to Use What (Practical Rule)
	‚úÖ Use synchronized when: 
			Simple critical section, No timeout / fairness required
			Low contention, You want safety & simplicity
	‚úÖ Use Lock when:
			Need fairness, Need timeout / tryLock
			Need multiple conditions, Complex coordination, Avoiding deadlock
	
=============================================================================================================
üîπ Q37. Explain ReentrantLock. What is fair vs non-fair lock?
=============================================================================================================
1Ô∏è‚É£ What Is ReentrantLock?
	ReentrantLock is an explicit, reentrant mutual-exclusion lock provided by java.util.concurrent.locks.
	Reentrant means the same thread can acquire the lock multiple times without blocking itself.
	It is a more flexible and powerful alternative to synchronized.

2Ô∏è‚É£ Why Is It Called Reentrant?
	Because: If a thread already holds the lock, It can lock it again, JVM maintains a hold count
			 Lock is released only when unlock() is called the same number of times
	üß™ Example
			ReentrantLock lock = new ReentrantLock();
			void methodA() {
				lock.lock();
				try {
					methodB(); // re-enters same lock
				} finally {
					lock.unlock(); // MUST be in finally
				}
			}

			void methodB() {
				lock.lock();
				try {
					// safe
				} finally {
					lock.unlock(); // MUST be in finally
				}
			}
		‚úî No deadlock, ‚úî Same thread re-acquires lock safely ‚ö† Forgetting unlock() ‚Üí deadlock risk

3Ô∏è‚É£ Key Features of ReentrantLock (Why It Exists)
	| Feature             | ReentrantLock | synchronized  |
	| ------------------- | ------------- | --------------|
	| Reentrant           | ‚úÖ Yes  	  | ‚úÖ Yes        |
	| Fairness            | ‚úÖ Optional	  | ‚ùå No         |
	| tryLock()           | ‚úÖ Yes 	      | ‚ùå No         |
	| Timeout             | ‚úÖ Yes  	  | ‚ùå No         |
	| Interruptible lock  | ‚úÖ Yes	      | ‚ùå No         |
	| Multiple conditions | ‚úÖ Yes 	      | ‚ùå No         |
	| Auto release        | ‚ùå Manual 	  | ‚úÖ Automatic  |

4Ô∏è‚É£ Fair vs Non-Fair Lock (Very Important)
	A fair lock: ReentrantLock lock = new ReentrantLock(true);
		Grants access in FIFO order, Longest-waiting thread gets the lock next, Prevents starvation
	‚úî Characteristics: Predictable ordering, Reduced starvation, Lower throughput

	A non-fair lock: ReentrantLock lock = new ReentrantLock(); // default
		Allows barging, A thread can acquire the lock even if others are waiting, No ordering guarantee
	‚úî Characteristics; Higher throughput, Better CPU locality, starvation is possible

6Ô∏è‚É£ Fair vs Non-Fair ‚Äî Comparison Table
	| Aspect            | Fair Lock     | Non-Fair Lock |
	| ----------------- | ------------- | ------------- |
	| Ordering          | FIFO          | No guarantee  |
	| Starvation        | Very unlikely | Possible      |
	| Performance       | Slower        | Faster        |
	| Context switching | More          | Less          |
	| Default           | ‚ùå    	    | ‚úÖ   	        |

7Ô∏è‚É£ Why Non-Fair Is the Default?
	Because: Fairness adds queue management overhead, More context switching, Lower throughput
	üëâ Most applications prefer performance over strict fairness.
	üéØ Interview Line: ‚ÄúFair locks reduce starvation but at the cost of throughput.‚Äù

8Ô∏è‚É£ How ReentrantLock Avoids Starvation (When Needed)
	Using a fair lock: ReentrantLock lock = new ReentrantLock(true);
		Threads acquire lock in arrival order, Long-waiting threads eventually run
		Useful in: Banking systems, Billing systems, SLA-sensitive workflows

=============================================================================================================
üîπ Q38. Difference between ReentrantLock and synchronized.
=============================================================================================================
| Aspect              | `synchronized`   | `ReentrantLock`                         |
| ------------------- | ---------------- | --------------------------------------- |
| Type                | Language keyword | Java API (`java.util.concurrent.locks`) |
| Lock acquisition    | Implicit         | Explicit (`lock()`)                     |
| Lock release        | Automatic        | Manual (`unlock()`)                     |
| Reentrant           | ‚úÖ Yes           | ‚úÖ Yes             	                   |
| Fairness            | ‚ùå No guarantee  | ‚úÖ Optional       	                   |
| tryLock             | ‚ùå No            | ‚úÖ Yes             	                   |
| Timeout             | ‚ùå No            | ‚úÖ Yes                 	               |
| Interruptible lock  | ‚ùå No            | ‚úÖ Yes                    	           |
| Multiple conditions | ‚ùå One wait set  | ‚úÖ Multiple `Condition`s   	           |
| Risk of misuse      | Low      	     | Higher (forget unlock)                  |
| Simplicity          | High       	     | Moderate                                |
| Flexibility         | Low        	     | High                                    |

Key Differences Explained (Interviewers Care About These)
	üîπ 1. Lock Acquisition & Release
		synchronized:
					synchronized (lock) {
						// lock acquired and released automatically
					}
				‚úî Simple, ‚úî Exception-safe, ‚ùå No flexibility
		
		ReentrantLock:
					lock.lock();
					try {
						// critical section
					} finally {
						lock.unlock(); // must be explicit
					}
				‚úî More control, ‚ùå Developer must ensure unlock()
	
	üîπ 2. Fairness (Very Important)
		synchronized: No fairness guarantee, Threads may starve
		ReentrantLock: FIFO ordering, Reduces starvation, Slight performance cost
			ReentrantLock lock = new ReentrantLock(true); // fair lock

		üéØ Interview line: ‚ÄúReentrantLock supports fairness; synchronized does not.‚Äù

	üîπ 3. tryLock & Timeout (Deadlock Avoidance)
				if (lock.tryLock(2, TimeUnit.SECONDS)) {
					try {
						// work
					} finally {
						lock.unlock();
					}
				}
			‚úî Avoids indefinite blocking, Helps prevent deadlock, ‚ùå Impossible with synchronized
	
	üîπ 4. Interruptible Lock Acquisition
			lock.lockInterruptibly(); Thread can be interrupted while waiting
			synchronized: BLOCKED thread cannot be interrupted

	üîπ 5. Multiple Conditions (Producer‚ÄìConsumer Use Case)
			synchronized: Only one wait set (wait/notify)
			ReentrantLock:
					Condition notEmpty = lock.newCondition();
					Condition notFull = lock.newCondition();
				‚úî Cleaner design, ‚úî Fewer unnecessary wakeups

4Ô∏è‚É£ Performance Consideration (Senior Insight)
	Modern JVMs heavily optimize synchronized
	For simple, low-contention cases ‚Üí synchronized is perfectly fine
	ReentrantLock is not always faster, Choice is about control, not just speed
	
	üéØ Interview line: ‚ÄúThis is not a performance choice; it‚Äôs a control and design choice.‚Äù

5Ô∏è‚É£ When to Use Which (Very Practical)
	‚úÖ Use synchronized when: Simple critical sections, Low contention, No fairness or timeout needed, Prefer safety and readability
	‚úÖ Use ReentrantLock when:
		You need fairness, You need tryLock or timeout, You need interruptible waits,
		You need multiple conditions, You want to avoid starvation

=============================================================================================================
üîπ Q39. What is ReadWriteLock? When to use ReadWriteLock?
=============================================================================================================
1Ô∏è‚É£ What Is ReadWriteLock?
	ReadWriteLock is a lock that maintains two separate locks:
		a read lock for read-only access
		a write lock for exclusive write access
		üëâ Multiple threads can read concurrently, but writes are exclusive.

	In Java, the main implementation is: ReentrantReadWriteLock

2Ô∏è‚É£ Why Do We Need ReadWriteLock?
	With a normal lock (synchronized or ReentrantLock):
		Readers block each other, Even if they are only reading shared data, This limits scalability.
	
	üî• ReadWriteLock improves throughput when: Reads are frequent, Writes are rare

3Ô∏è‚É£ How ReadWriteLock Works (Core Idea)
	Rules:
		Multiple threads can hold the read lock simultaneously
		Only one thread can hold the write lock
		While a writer holds the lock: No readers allowed
		While readers hold the lock: Writers must wait

4Ô∏è‚É£ Simple Example Using ReentrantReadWriteLock

		class Cache {
			private final ReadWriteLock rwLock = new ReentrantReadWriteLock();
			private final Lock readLock = rwLock.readLock();
			private final Lock writeLock = rwLock.writeLock();

			private int data;

			public int read() {
				readLock.lock();
				try {
					return data;
				} finally {
					readLock.unlock();
				}
			}

			public void write(int value) {
				writeLock.lock();
				try {
					data = value;
				} finally {
					writeLock.unlock();
				}
			}
		}

	‚úî Behavior: Many threads can call read() at the same time, Only one thread can call write(), Writes block both readers and writers

5Ô∏è‚É£ ReadWriteLock vs Normal Lock
| Aspect             | ReentrantLock   | ReadWriteLock           |
| ------------------ | --------------- | ----------------------- |
| Concurrent readers | ‚ùå No    	   | ‚úÖ Yes                  |
| Writers exclusive  | ‚úÖ Yes   	   | ‚úÖ Yes                  |
| Best for           | Mixed workloads | Read-heavy workloads    |
| Complexity         | Low             | Higher                  |
| Risk               | Low             | Starvation (if misused) |

6Ô∏è‚É£ When Should You Use ReadWriteLock?
	‚úÖ Use ReadWriteLock When: 
		Read operations dominate, Writes are infrequent, Data structure is shared and mutable, Read operations do not modify state
	
		üß™ Real-World Examples: 
				In-memory cache, Configuration store, Reference data (country codes, feature flags)
				Lookup tables, Metadata repositories

	üéØ Interview line: ‚ÄúReadWriteLock is ideal when reads vastly outnumber writes.‚Äù

7Ô∏è‚É£ When You Should NOT Use ReadWriteLock
	‚ùå Avoid ReadWriteLock When: 
		Write frequency is high, Reads are short-lived and cheap, Lock upgrade (read ‚Üí write) is required
		Simplicity is more important than throughput
		‚ö† Lock upgrading can cause deadlock if not handled carefully.

8Ô∏è‚É£ Starvation Risk (Important Senior Insight)
	With non-fair ReadWriteLock: Continuous readers may starve writers
	You can enable fairness: ReadWriteLock rwLock = new ReentrantReadWriteLock(true);
	‚úî Writers eventually get chance ‚ùå Slight performance hit

9Ô∏è‚É£ ReadWriteLock vs Concurrent Collections
	Often, you don‚Äôt need ReadWriteLock at all.
		Example: ConcurrentHashMap already implements internal read/write optimizations
	üéØ Interview line: ‚ÄúPrefer concurrent collections over manual ReadWriteLock when possible.‚Äù

=============================================================================================================
üîπ Q40. Explain StampedLock. Difference between optimistic and pessimistic locking.
=============================================================================================================
1Ô∏è‚É£  StampedLock (Java 8+) is a high-performance lock designed for read-mostly workloads.
	It supports three modes of locking: Write lock (exclusive), Read lock (shared), Optimistic read (non-blocking)
	üëâ It improves throughput by allowing lock-free reads when there‚Äôs no contention.

2Ô∏è‚É£ Why Was StampedLock Introduced?
	ReadWriteLock improved concurrency but: Readers still block writers, Writers still block readers, Lock acquisition still has overhead
	StampedLock solves this by introducing optimistic reads, which: Don‚Äôt block writers, Don‚Äôt block readers, Avoid locking unless necessary

	üéØ Interview line: ‚ÄúStampedLock is optimized for scenarios where reads are frequent and writes are rare.‚Äù

3Ô∏è‚É£ How StampedLock Works (Key Idea)
	Instead of returning a Lock object, StampedLock returns a stamp (long value):
		long stamp = lock.readLock();
		The stamp: Represents a version of the lock state, Must be used to unlock
		Helps detect concurrent modifications

4Ô∏è‚É£ Locking Modes in StampedLock
	üî¥ 1. Write Lock (Exclusive)
			long stamp = lock.writeLock();
			try {
				// modify shared data
			} finally {
				lock.unlockWrite(stamp);
			}

		‚úî Only one writer, ‚úî Blocks readers and writers

	üü° 2. Read Lock (Pessimistic Read)
			long stamp = lock.readLock();
			try {
				// read shared data
			} finally {
				lock.unlockRead(stamp);
			}

		‚úî Multiple readers, ‚úî Blocks writers. This is similar to ReadWriteLock.

	üü¢ 3. Optimistic Read (Most Important)
			long stamp = lock.tryOptimisticRead();
			// read shared data (NO LOCK!)
			if (!lock.validate(stamp)) {
				// a write occurred ‚Äî fall back to read lock
			}
		
		‚úî No blocking, ‚úî No locking, ‚úî Fastest read path
		But: Data may change, Must be validated

5Ô∏è‚É£ Optimistic vs Pessimistic Locking (Core Difference)

	üîπ Optimistic Locking: Assume no conflict, Proceed without locking, Validate later, Retry if conflict occurs
		‚úî Characteristics: Non-blocking, High throughput, Best for read-heavy systems
		‚ùå Risks: Retry cost if contention is high
		üß™ Example (StampedLock)
					long stamp = lock.tryOptimisticRead();
					int value = data;
					if (!lock.validate(stamp)) {
						stamp = lock.readLock();
						try {
							value = data;
						} finally {
							lock.unlockRead(stamp);
						}
					}
	
	üîπ Pessimistic Locking: Assume conflict will happen, Lock first, Then operate
		‚úî Characteristics: Safe, Simple, Predictable
		‚ùå Drawbacks: Blocking, Lower concurrency
		üß™ Example
				synchronized (lock) {
					readOrWriteData();
				}

6Ô∏è‚É£ Optimistic vs Pessimistic ‚Äî Comparison Table
	| Aspect            | Optimistic Locking          | Pessimistic Locking             |
	| ----------------- | --------------------------- | ------------------------------- |
	| Locking           | None initially              | Immediate                       |
	| Blocking          | ‚ùå No           	          | ‚úÖ Yes              	        |
	| Conflict handling | Validation & retry          | Wait                            |
	| Performance       | High (low contention)       | Lower                           |
	| Best for          | Read-heavy workloads        | Write-heavy / critical sections |
	| Example           | StampedLock optimistic read | synchronized / ReentrantLock    |

7Ô∏è‚É£ Important Limitations of StampedLock (Must Mention)
	‚ö† StampedLock is NOT reentrant // Same thread cannot re-acquire lock ‚Üí deadlock
	‚ö† No Condition support, No automatic unlocking, More complex to use, Not suitable for simple cases
	üéØ Interview line: ‚ÄúStampedLock trades simplicity for performance.‚Äù

8Ô∏è‚É£ When Should You Use StampedLock?
	‚úÖ Use StampedLock When: Reads vastly outnumber writes, Read operations are short, You need maximum throughput, Contention is low
	‚ùå Avoid StampedLock When: Code must be simple, Reentrancy is required, Write contention is high, You need condition variables
	
9Ô∏è‚É£ StampedLock vs ReadWriteLock (Quick Contrast)
	| Aspect           | ReadWriteLock | StampedLock         |
	| ---------------- | ------------- | ------------------- |
	| Optimistic reads | ‚ùå No  	   | ‚úÖ Yes   	         |
	| Reentrant        | ‚úÖ Yes 	   | ‚ùå No      	     |
	| Complexity       | Low           | High                |
	| Performance      | Good          | Better (read-heavy) |
	| Java version     | 5+            | 8+                  |
	
=============================================================================================================
üîπ Q41. Explain Executor framework. Why should we avoid creating threads manually?
=============================================================================================================
1Ô∏è‚É£  The Executor Framework (java.util.concurrent) is a high-level abstraction for asynchronous task execution that decouples task submission from task execution. Instead of Creating threads, Starting threads, Managing thread lifecycle, You submit tasks, and the framework manages threads for you.

üîë Core Idea : You tell what to execute; the Executor decides how and when to execute it.

2Ô∏è‚É£ Core Components of Executor Framework
	üîπ 1. Executor: Basic interface with one method: void execute(Runnable command);
	üîπ 2. ExecutorService extends Executor and adds Lifecycle management, Task submission with results, Graceful shutdown
			ExecutorService executor = Executors.newFixedThreadPool(5);

	üîπ 3. ThreadPoolExecutor: The actual implementation behind most executors.
			Controls: Pool size, Queue, Thread creation, Rejection policy
	
	üîπ 4. Task Types
			Runnable ‚Üí no result
			Callable<V> ‚Üí returns result, can throw exception
			Future<Integer> f = executor.submit(() -> 42);

3Ô∏è‚É£ Common Executor Types (Must Know)
	Executors.newFixedThreadPool(n);      // fixed number of threads
	Executors.newCachedThreadPool();     // dynamic threads
	Executors.newSingleThreadExecutor(); // one thread
	Executors.newScheduledThreadPool(n); // delayed & periodic tasks
	üëâ In real systems, we usually use ThreadPoolExecutor directly for control.

4Ô∏è‚É£ Why Should We Avoid Creating Threads Manually?
	‚ùå Manual Thread Creation (Bad Practice): new Thread(() -> doWork()).start();
		Looks simple ‚Äî but causes serious problems at scale.
	
5Ô∏è‚É£ Problems with Creating Threads Manually
	üî¥ 1. High Resource Cost
			Each thread: Allocates stack memory (often 1MB+), Uses OS resources, Causes context switching overhead
			Creating too many threads ‚Üí High memory usage, CPU thrashing, Poor performance
		
		üéØ Interview line: ‚ÄúThreads are expensive; tasks are cheap.‚Äù
	
	üî¥ 2. No Thread Reuse
				for (...) {
					new Thread(task).start();
				}
		‚ùå Threads are created and destroyed repeatedly ‚ùå Wasteful and slow

		Executor: ‚úî Reuses threads, ‚úî Improves throughput								

	üî¥ 3. No Control Over Concurrency
		Manual threads: No limit, Can create thousands of threads, Can crash JVM with OutOfMemoryError
		Executor: newFixedThreadPool(10); // concurrency is bounded
		‚úî Predictable behavior

	üî¥ 4. No Lifecycle Management
		Manual threads=> No way to: Shutdown gracefully, Await completion, Handle failures centrally
		Executor:
			executor.shutdown();
			executor.awaitTermination(...);
		‚úî Clean shutdown, ‚úî Controlled termination

	üî¥ 5. Poor Error Handling
		In manual threads: Exceptions may be swallowed, Hard to track failures
		Executor: Exceptions captured via Future, Centralized handling, Better observability

	üî¥ 6. No Scheduling or Queuing
		Manual threads: Immediate execution only
		Executor: Task queues, Scheduling, Delayed and periodic execution

6Ô∏è‚É£ Benefits of Executor Framework (Why It Exists)
	| Benefit              | Explanation                       |
	| -------------------- | --------------------------------- |
	| Thread reuse         | Fewer threads, better performance |
	| Resource control     | Bounded pools                     |
	| Scalability          | Works well under load             |
	| Task abstraction     | Runnable / Callable               |
	| Error handling       | Futures & policies                |
	| Lifecycle management | start / stop / shutdown           |
	| Monitoring           | Pool size, queue size             |

7Ô∏è‚É£ Real-World Example (Interview-Friendly)
	‚ùå Bad (Manual Threads in Web App): new Thread(() -> processRequest()).start();
		Under load: Thousands of threads, App crashes
	
	
	‚úÖ Good (Executor in Web App): executor.submit(() -> processRequest());
		Limited threads, Requests queued, System remains stable

8Ô∏è‚É£ When Is Manual Thread Creation Acceptable?
	Very rarely: One-off background thread, JVM startup tasks, Low-level framework code
	üéØ Interview line: ‚ÄúApplication code should almost never create threads directly.‚Äù


=============================================================================================================
üîπ Q42. Difference between Executor, ExecutorService, ScheduledExecutorService.
=============================================================================================================
=============================================================================================================
üîπ Q43. Expalin all types of thread pools: Fixed, Cached, Single, Scheduled
=============================================================================================================
üßµ Types of Thread Pools in Java Executor Framework
1Ô∏è‚É£ Fixed Thread Pool: A thread pool with a fixed number of threads.
	ExecutorService executor = Executors.newFixedThreadPool(5);
	üîß How It Works: Creates N threads, Reuses those threads, Tasks beyond N are queued, Threads are never destroyed (until shutdown)
		Internally: Core threads = Max threads = N, Uses unbounded queue (LinkedBlockingQueue)

	‚úÖ When to Use: CPU-bound tasks, Stable, predictable workload, You want to limit concurrency
		Examples: Processing requests, Batch jobs, Background task processing
	
	‚ùå When to Avoid: Very high task submission rate, Risk of unbounded queue ‚Üí OutOfMemoryError
	üéØ Interview Line: ‚ÄúFixed thread pool limits concurrency but can grow the queue indefinitely.‚Äù

2Ô∏è‚É£ Cached Thread Pool: A dynamically growing thread pool.
	ExecutorService executor = Executors.newCachedThreadPool();
	üîß How It Works: Creates new threads as needed, Reuses idle threads, Threads idle for 60 seconds are terminated
		Internally: Core threads = 0, Max threads = Integer.MAX_VALUE, Uses SynchronousQueue (no storage)
	
	‚úÖ When to Use: Short-lived, asynchronous tasks, Bursty workloads, I/O-bound operations
		Examples: Lightweight background tasks, Event handling
	
	‚ùå When to Avoid (Very Important): High load systems, Web applications, Unbounded thread creation ‚Üí CPU & memory exhaustion

	üéØ Interview Line: ‚ÄúCached thread pool can create unlimited threads ‚Äî dangerous under heavy load.‚Äù

3Ô∏è‚É£ Single Thread Executor: A pool with exactly one thread.
	ExecutorService executor = Executors.newSingleThreadExecutor();
	üîß How It Works: One worker thread, Tasks execute sequentially, If thread dies ‚Üí a new one is created, Uses unbounded queue
	
	‚úÖ When to Use: Tasks must run in order, Only one task should execute at a time, You need thread safety via serialization
		Examples: Logging, Event processing, File I/O sequencing
	
	‚ùå When to Avoid: Parallel processing needed, Long-running tasks blocking others

	üéØ Interview Line: ‚ÄúSingleThreadExecutor guarantees task ordering with automatic thread replacement.‚Äù

4Ô∏è‚É£ Scheduled Thread Pool: A pool that can schedule tasks to run: After a delay, Periodically
	ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(2);
	üîß How It Works: Supports: schedule(), scheduleAtFixedRate(), scheduleWithFixedDelay()
	üß™ Examples: 
		Run once after delay: scheduler.schedule(() -> task(), 5, TimeUnit.SECONDS);
		Run periodically (fixed rate): scheduler.scheduleAtFixedRate(task, 0, 10, TimeUnit.SECONDS);
		Run periodically (fixed delay): scheduler.scheduleWithFixedDelay(task, 0, 10, TimeUnit.SECONDS);

	‚úÖ When to Use: Periodic jobs, Heartbeats, Cleanup tasks, Retry mechanisms
	‚ùå When to Avoid: Heavy CPU tasks, Very long-running scheduled tasks
	
	üéØ Interview Line: ‚ÄúScheduledExecutorService replaces Timer with better error handling and concurrency.‚Äù

üî• Comparison Table (Very Important)
| Thread Pool | Threads | Queue      | Best For           | Risk             |
| ----------- | ------- | ---------- | ------------------ | ---------------- |
| Fixed       | Fixed N | Unbounded  | Stable workloads   | OOM via queue    |
| Cached      | Dynamic | None       | Short async tasks  | Thread explosion |
| Single      | 1       | Unbounded  | Ordered execution  | Bottleneck       |
| Scheduled   | Fixed N | DelayQueue | Timers & cron jobs | Task overlap     |

‚ö†Ô∏è Senior-Level Warning (Interview Gold)
	Avoid using Executors factory methods blindly in production.
	Why? Hidden defaults, Unbounded queues, Unbounded threads
	
	‚úÖ Preferred: Use ThreadPoolExecutor directly:
				new ThreadPoolExecutor(core, max, keepAlive, TimeUnit.SECONDS,
					new ArrayBlockingQueue<>(100),
					new ThreadPoolExecutor.AbortPolicy()
				);
				
=============================================================================================================
üîπ Q45. CorePoolSize vs MaximumPoolSize.
=============================================================================================================
1Ô∏è‚É£  CorePoolSize is the minimum number of threads that the thread pool tries to keep alive.
	These threads are: Created eagerly (or on demand), Not destroyed even when idle (by default), They form the baseline capacity of the pool

		new ThreadPoolExecutor(
			corePoolSize,
			maxPoolSize,
			keepAliveTime,
			timeUnit,
			workQueue
		);

	üß† Key Points: Core threads handle normal workload, Tasks are first assigned to core threads, Core threads stay alive indefinitely (unless allowCoreThreadTimeOut(true))

2Ô∏è‚É£  MaximumPoolSize is the maximum number of threads the pool can grow to under heavy load.
	Extra threads beyond corePoolSize are called non-core threads, Created only when needed, Destroyed after being idle for keepAliveTime
	
	üß† Key Points: Used as a burst capacity, Protects the system from creating unlimited threads, Once max is reached, tasks may be rejected

3Ô∏è‚É£ How ThreadPoolExecutor Decides What to Do (Very Important)
	When a new task is submitted, the pool follows this exact order:
	üîÅ Task Submission Algorithm
		1Ô∏è‚É£ If current threads < corePoolSize ‚Üí Create a new thread to run the task
		2Ô∏è‚É£ Else if queue is not full ‚Üí Put the task into the queue
		3Ô∏è‚É£ Else if current threads < maximumPoolSize ‚Üí Create a new non-core thread
		4Ô∏è‚É£ Else ‚Üí Reject the task (RejectedExecutionHandler)

	üéØ Interview Line (Use This!): ‚ÄúThreads grow first, then the queue fills, then threads grow again up to max.‚Äù

4Ô∏è‚É£ Example (This Is Gold for Interviews)
			ThreadPoolExecutor executor =
				new ThreadPoolExecutor(
					2,  // corePoolSize
					5,  // maximumPoolSize
					60, TimeUnit.SECONDS,
					new ArrayBlockingQueue<>(3)
				);

				Now submit tasks üëá
				| Task # | Active Threads | Queue Size | Action     |
				| ------ | -------------- | ---------- | ---------- |
				| 1      | 1              | 0          | New thread |
				| 2      | 2              | 0          | New thread |
				| 3      | 2              | 1          | Queued     |
				| 4      | 2              | 2          | Queued     |
				| 5      | 2              | 3          | Queued     |
				| 6      | 3              | 3          | New thread |
				| 7      | 4              | 3          | New thread |
				| 8      | 5              | 3          | New thread |
				| 9      | 5              | 3          | ‚ùå Rejected |

				This explanation immediately impresses interviewers.

5Ô∏è‚É£ Core vs Maximum ‚Äî Side-by-Side
	| Aspect                  | corePoolSize     | maximumPoolSize |
	| ----------------------- | ---------------- | --------------- |
	| Purpose                 | Baseline threads | Burst limit     |
	| Threads always kept     | ‚úÖ Yes            | ‚ùå No            |
	| Used during normal load | ‚úÖ                | ‚ùå               |
	| Used during peak load   | ‚ùå                | ‚úÖ               |
	| Idle timeout            | ‚ùå (by default)   | ‚úÖ               |
	| Protects from overload  | ‚ùå                | ‚úÖ               |
	
6Ô∏è‚É£ Relationship with Queue (Very Important Insight)
üëâ Queue choice determines whether maximumPoolSize even matters
	‚ö† Unbounded Queue (e.g. LinkedBlockingQueue), Tasks are always queued, Pool never grows beyond corePoolSize, maximumPoolSize is effectively ignored
	
	üéØ Interview line: ‚ÄúWith an unbounded queue, maximumPoolSize becomes meaningless.‚Äù

	‚úÖ Bounded Queue (Recommended): Allows pool to grow beyond core size, Enables back-pressure, Makes maxPoolSize effective

=============================================================================================================
üîπ Q46. What is work queue? What happens when queue is full?
=============================================================================================================
1Ô∏è‚É£  A work queue is an internal task holding queue used by ThreadPoolExecutor to store submitted tasks when threads are busy.
	üëâ It sits between task submission and thread execution.
		ThreadPoolExecutor executor = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, 
															keepAliveTime, TimeUnit.SECONDS, workQueue);

	üß† Why Work Queue Exists
			Threads are limited, Tasks may arrive faster than they can be processed, Queue provides buffering and back-pressure

	üéØ Interview line: ‚ÄúThe work queue buffers tasks when all core threads are busy.‚Äù

2Ô∏è‚É£ How the Work Queue Is Used (Execution Flow)
		When a task is submitted:
			1Ô∏è‚É£ If active threads < corePoolSize ‚Üí Create a new thread
			2Ô∏è‚É£ Else if queue is not full ‚Üí Put task into work queue
			3Ô∏è‚É£ Else if active threads < maximumPoolSize ‚Üí Create a new non-core thread
			4Ô∏è‚É£ Else ‚Üí Reject the task
			This order is extremely important.

3Ô∏è‚É£ Common Types of Work Queues
		üîπ 1. LinkedBlockingQueue (Unbounded): new LinkedBlockingQueue<>();
				Default for newFixedThreadPool, Can grow indefinitely
				‚úî Simple ‚ùå Risk of OutOfMemoryError
				‚ö† With unbounded queue: maximumPoolSize is effectively ignored

				üéØ Interview line: ‚ÄúUnbounded queues protect threads but risk memory.‚Äù

		üîπ 2. ArrayBlockingQueue (Bounded): new ArrayBlockingQueue<>(100);
				Fixed capacity, Provides back-pressure
				‚úî Safer for production ‚úî Enables maxPoolSize usage

		üîπ 3. SynchronousQueue (No Storage): new SynchronousQueue<>();
				No capacity, Direct handoff
				‚úî Used by newCachedThreadPool ‚ùå Can create many threads

		üîπ 4. PriorityBlockingQueue
				Tasks ordered by priority, Unbounded by default
				‚úî Useful for priority-based processing, ‚ùå Still OOM risk

4Ô∏è‚É£ What Happens When the Queue Is Full? This is the core of the question üëá

	üî¥ Case 1: Queue is Full BUT Threads < maxPoolSize: ‚û° ThreadPoolExecutor creates a new non-core thread
				Queue full ‚Üí grow thread pool (up to max)
				‚úî Helps absorb load spikes

	üî¥ Case 2: Queue is Full AND Threads == maxPoolSize; 	‚û° Task is rejected
				This triggers the RejectedExecutionHandler.

‚úî RejectedExecutionHandler is a strategy interface used by ThreadPoolExecutor to decide what to do when a task cannot be accepted for execution.
	A task is rejected when: The work queue is full, The thread pool has reached maximumPoolSize, The executor is shut down
	
5Ô∏è‚É£ Task Rejection Policies (Must Know)
	üîπ 1. AbortPolicy (Default): throw RejectedExecutionException
			‚úî Fail fast, ‚ùå Request lost if not handled

	üîπ 2. CallerRunsPolicy: task.run(); // runs in calling thread
			‚úî Back-pressure, ‚úî Slows down producer, ‚úî Very useful in production

	üîπ 3. DiscardPolicy 	// silently drop task
			‚ùå Dangerous unless acceptable

	üîπ 4. DiscardOldestPolicy: remove oldest task and retry
			‚úî Useful for real-time systems

6Ô∏è‚É£ Real-World Example (Interview Gold)
		ThreadPoolExecutor executor = new ThreadPoolExecutor(2,  // core
																4,  // max
																60, TimeUnit.SECONDS,
																new ArrayBlockingQueue<>(2),
																new ThreadPoolExecutor.CallerRunsPolicy());
			

		Submitting many tasks: 
			First 2 ‚Üí threads
			Next 2 ‚Üí queue
			Next 2 ‚Üí new threads
			Further ‚Üí caller thread executes task
		‚û° System slows gracefully instead of crashing.
		‚úî Correct: Threads ‚Üí Queue ‚Üí More Threads ‚Üí Reject


=============================================================================================================
üîπ Q48. What is keepAliveTime? How to gracefully shutdown executor?
=============================================================================================================
1Ô∏è‚É£  keepAliveTime is the maximum time that excess (non-core) threads are allowed to remain idle before being terminated.
	üîç Which Threads Does keepAliveTime Apply To?
		‚úÖ By default: Applies only to non-core threads, Threads above corePoolSize, When idle longer than keepAliveTime ‚Üí terminated
		‚ùå Does NOT apply to: Core threads (unless explicitly enabled)
	
	üß† Why keepAliveTime Exists: Handle traffic spikes, Shrink pool back to normal size, Free CPU & memory resources
	üéØ Interview line: ‚ÄúkeepAliveTime controls how long extra threads survive after a load spike.‚Äù

2Ô∏è‚É£ Example: 
	Scenario:
		Normal load ‚Üí 2 threads
		Traffic spike ‚Üí grows to 5 threads
		Load drops ‚Üí extra 3 threads stay idle
		After 30 seconds ‚Üí extra threads die
		‚úî Pool returns to stable size

3Ô∏è‚É£ Applying keepAliveTime to Core Threads (Advanced): executor.allowCoreThreadTimeOut(true);
	Now: Even core threads can be terminated
	Useful for: Low-traffic systems, Memory-sensitive environments
	üéØ Interview line: ‚ÄúallowCoreThreadTimeOut makes the pool fully elastic.‚Äù

4Ô∏è‚É£ How to Gracefully Shut Down an Executor
	üîπ What Does Graceful Shutdown Mean? Stop accepting new tasks, allow existing tasks to finish, then release resources.

		‚úÖ Step-by-Step Pattern
			executor.shutdown(); // 1Ô∏è‚É£ Stop accepting new tasks

				try {
					if (!executor.awaitTermination(60, TimeUnit.SECONDS)) {
						executor.shutdownNow(); // 2Ô∏è‚É£ Force shutdown
					}
				} catch (InterruptedException e) {
					executor.shutdownNow();     // 3Ô∏è‚É£ Cancel on interruption
					Thread.currentThread().interrupt();
				}

	üîç What Each Method Does
	üîπ shutdown(): Stops accepting new tasks, Allows queued & running tasks to complete, Does NOT block
		‚úî Preferred for graceful shutdown
	
	üîπ awaitTermination(timeout): Blocks until: All tasks finish, Timeout occurs, Returns true if terminated
	
	üîπ shutdownNow(): Attempts to: Interrupt running tasks, Return queued but unstarted tasks
		‚ö† Not guaranteed to stop tasks immediately

6Ô∏è‚É£ What Happens If You Don‚Äôt Shut Down Executor?
	‚ùå JVM may never exit, ‚ùå Thread leaks, ‚ùå Resource exhaustion, ‚ùå Hanging deployments
	üéØ Interview line: ‚ÄúAn executor is a resource ‚Äî it must be shut down.‚Äù
	

7Ô∏è‚É£ shutdown vs shutdownNow (Quick Comparison)
| Aspect             | shutdown() | shutdownNow() |
| ------------------ | ---------- | ------------- |
| New tasks accepted | ‚ùå No       | ‚ùå No          |
| Running tasks      | ‚úÖ Finish   | ‚ùå Interrupted |
| Queued tasks       | ‚úÖ Executed | ‚ùå Returned    |
| Graceful           | ‚úÖ Yes      | ‚ùå No          |
| Safe default       | ‚úÖ          | ‚ùå             |

=============================================================================================================
üîπ Q50. Difference between Runnable and Callable.
=============================================================================================================
1Ô∏è‚É£ High-Level Difference (Start Like This)
Runnable represents a task that does not return a result and cannot throw checked exceptions, while Callable represents a task that returns a result and can throw checked exceptions.

2Ô∏è‚É£ Runnable vs Callable ‚Äî Core Differences
	| Aspect             | Runnable         | Callable               |
	| ------------------ | ---------------- | ---------------------- |
	| Package            | `java.lang`      | `java.util.concurrent` |
	| Method             | `void run()`     | `V call()`             |
	| Return value       | ‚ùå No    	    | ‚úÖ Yes      	         |
	| Checked exceptions | ‚ùå No   	        | ‚úÖ Yes       	         |
	| Introduced in      | Java 1.0         | Java 5                 |
	| Used with          | Thread, Executor | ExecutorService        |
	| Result handling    | Not supported    | Via `Future`           |

3Ô∏è‚É£ Runnable Explained: Runnable represents a unit of work that can be executed by a thread.
				Runnable task = () -> {
					System.out.println("Task running");
				};
	üîπ Execution: new Thread(task).start(); or executor.execute(task);
		‚ùå Limitations of Runnable: Cannot return a value, Cannot throw checked exceptions, Error handling is awkward
			No direct feedback mechanism

		üß† When Runnable Is Enough: Fire-and-forget tasks, Logging, Background monitoring, Simple asynchronous work

4Ô∏è‚É£ Callable Explained: Callable<V> represents a task that: Returns a result, Can throw checked exceptions

				Callable<Integer> task = () -> {
					return 42;
				};

	üîπ Execution:
				Future<Integer> future = executor.submit(task);
				Integer result = future.get();

	üî• Key Advantage: Clean error handling, Strongly typed return value, Integration with Future
	üß† When Callable Is Preferred: Business computations, Remote calls, Database queries, Any task where result matters

5Ô∏è‚É£ Exception Handling Difference (Important)
	Runnable
			public void run() {
				throw new RuntimeException(); // allowed
				// checked exception ‚ùå not allowed
			}

		Checked exceptions must be: Caught inside run(), Wrapped in RuntimeException

	Callable
			public Integer call() throws Exception {
				return fetchFromDB();
			}
		‚úî Clean exception propagation, ‚úî Exceptions returned via Future.get() as ExecutionException

6Ô∏è‚É£ How Executor Handles Them
		executor.execute(Runnable);  // no result
		executor.submit(Runnable);   // returns Future<?> (result = null)
		executor.submit(Callable);   // returns Future<V>


üéØ Interview line: ‚Äúsubmit works for both, execute works only for Runnable.‚Äù

7Ô∏è‚É£ Real-World Example (Interview Gold)
	‚ùå Using Runnable for computation (Bad)
		int[] result = new int[1];
		executor.execute(() -> result[0] = compute());

	‚úÖ Using Callable (Correct)
		Future<Integer> future = executor.submit(() -> compute());
		int result = future.get();

=============================================================================================================
üîπ Q51. What is Future? Limitations of Future. How CompletableFuture solves Future problems?
=============================================================================================================
1Ô∏è‚É£  A Future<V> represents the result of an asynchronous computation that may complete in the future.
	You submit a task, get a Future, and later ask: Is it done? Give me the result, Cancel it
			ExecutorService executor = Executors.newFixedThreadPool(2);
			Future<Integer> future = executor.submit(() -> { Thread.sleep(1000); return 42; });
		// later
			Integer result = future.get(); // blocks

2Ô∏è‚É£ What Can You Do with Future?
	Key methods:
				future.get();          // blocking
				future.get(timeout);  // blocking with timeout
				future.isDone();      // polling
				future.cancel(true);  // cancellation

	üéØ Interview line: ‚ÄúFuture represents a handle to an async computation.‚Äù

3Ô∏è‚É£ Limitations of Future (This Is the Main Part)
	üî¥ 1. Blocking Nature: future.get(); // blocks current thread
			No non-blocking callback, Wastes threads, Bad for scalable systems
			üéØ Interview line: ‚ÄúFuture forces you to block to get the result.‚Äù

	üî¥ 2. No Callbacks: You cannot say: ‚ÄúWhen the result is ready, do something.‚Äù
			Instead, you must: Poll using isDone(), or Block using get()
			‚ùå Inefficient, ‚ùå Ugly code

	üî¥ 3. No Chaining / Composition: 
			You cannot easily combine futures: Run task B after task A, Combine results of A and B, Handle pipelines
			‚ùå Manual orchestration, ‚ùå Nested blocking

	üî¥ 4. Poor Exception Handling
				try {
					future.get();
				} catch (ExecutionException e) {
					Throwable cause = e.getCause();
				}
			Wrapped exceptions, Hard to compose error handling, No centralized error flow

	üî¥ 5. Cannot Complete Manually
			With Future: Only executor controls completion, You cannot push results yourself
			‚ùå Not flexible for event-driven systems

4Ô∏è‚É£ Summary of Future Limitations
	| Limitation           | Impact                  |
	| -------------------- | ----------------------- |
	| Blocking get()       | Poor scalability        |
	| No callbacks         | Inefficient async       |
	| No chaining          | Hard to build pipelines |
	| Weak error handling  | Messy code              |
	| No manual completion | Limited flexibility     |

5Ô∏è‚É£ How CompletableFuture Solves These Problems
	CompletableFuture (Java 8+) is: A Future PLUS a CompletionStage, Supports non-blocking, functional, async programming
		CompletableFuture<Integer> cf = CompletableFuture.supplyAsync(() -> 42);

6Ô∏è‚É£ How CompletableFuture Fixes Each Problem
		‚úÖ 1. Non-Blocking Callbacks
				cf.thenAccept(result -> {
					System.out.println(result);
				});
			‚úî No blocking, ‚úî Event-driven, ‚úî Scales well
			üéØ Interview line: ‚ÄúCompletableFuture enables non-blocking callbacks.‚Äù

		‚úÖ 2. Easy Chaining
				CompletableFuture
					.supplyAsync(() -> 10)
					.thenApply(x -> x * 2)
					.thenApply(x -> x + 5)
					.thenAccept(System.out::println);
			‚úî Clean pipeline, ‚úî No nested futures

		‚úÖ 3. Combine Multiple Async Tasks
				CompletableFuture<Integer> f1 = CompletableFuture.supplyAsync(() -> 10);
				CompletableFuture<Integer> f2 = CompletableFuture.supplyAsync(() -> 20);
				f1.thenCombine(f2, Integer::sum).thenAccept(System.out::println);
			‚úî Impossible with plain Future, ‚úî Very powerful

		‚úÖ 4. Better Exception Handling
					cf.exceptionally(ex -> {
						log.error("Error", ex);
						return fallbackValue;
					});
				or
					cf.handle((result, ex) -> {
						if (ex != null) return fallback;
						return result;
					});
				‚úî Centralized, ‚úî Declarative

		‚úÖ 5. Manual Completion
					CompletableFuture<Integer> cf = new CompletableFuture<>();
				// later
					cf.complete(42);
					cf.completeExceptionally(new RuntimeException());
				‚úî Perfect for: Event-driven systems, Callbacks, Integrations
				
7Ô∏è‚É£ Future vs CompletableFuture (Quick Comparison)
| Aspect             | Future     | CompletableFuture |
| ------------------ | ---------- | ----------------- |
| Blocking           | ‚úÖ Yes      | ‚ùå Optional        |
| Callbacks          | ‚ùå No       | ‚úÖ Yes             |
| Chaining           | ‚ùå No       | ‚úÖ Yes             |
| Combine tasks      | ‚ùå No       | ‚úÖ Yes             |
| Exception handling | Poor       | Strong            |
| Manual completion  | ‚ùå No       | ‚úÖ Yes             |
| Programming style  | Imperative | Functional        |

8Ô∏è‚É£ When Should You Use What?
	‚úÖ Use Future when: Simple async task, Blocking is acceptable, Legacy code
	‚úÖ Use CompletableFuture when: Non-blocking systems, Async pipelines, Microservices, High-throughput apps, Reactive-style workflows
	
	üéØ Interview line: ‚ÄúFuture is about waiting; CompletableFuture is about reacting.‚Äù

=============================================================================================================
üîπ Q52. Difference between thenApply vs thenCompose and thenAccept vs thenRun.
=============================================================================================================
1Ô∏è‚É£ thenApply vs thenCompose: üëâ This is about what your function returns.
	üîπ thenApply ‚Äî transform the result
		üìå What it does: Takes the result of a stage, Transforms it, Returns a new value

			CompletableFuture<Integer> cf =
				CompletableFuture.supplyAsync(() -> 10)
					.thenApply(x -> x * 2); // transforms value
			System.out.println(cf.join()); // 20

	üî• Important detail: If your function returns a CompletableFuture, you‚Äôll get nested futures ‚ùå
			CompletableFuture<CompletableFuture<Integer>> cf =
				CompletableFuture.supplyAsync(() -> 10)
					.thenApply(x -> CompletableFuture.supplyAsync(() -> x * 2));

		üëâ Usually not what you want.

	üîπ thenCompose ‚Äî flatten async operations
		üìå What it does: Takes the result of a stage, Calls another async operation, Flattens the result

				CompletableFuture<Integer> cf =
					CompletableFuture.supplyAsync(() -> 10)
						.thenCompose(x ->
							CompletableFuture.supplyAsync(() -> x * 2)
						);
				System.out.println(cf.join()); // 20

		
üéØ Interview Line: ‚ÄúUse thenApply when you return a value, and thenCompose when you return a CompletableFuture.‚Äù

üß™ Real-World Example (Very Interview-Friendly)
			CompletableFuture<User> user =
				getUserId()
					.thenCompose(id -> fetchUserFromDB(id))
					.thenApply(u -> enrichUser(u));
		‚úî Async DB call ‚Üí thenCompose
		‚úî Simple transformation ‚Üí thenApply

üî• thenApply vs thenCompose Summary
| Aspect           | thenApply                | thenCompose              |
| ---------------- | ------------------------ | ------------------------ |
| Input            | Value                    | Value                    |
| Function returns | Value                    | CompletableFuture        |
| Output           | CompletableFuture<Value> | CompletableFuture<Value> |
| Nesting          | ‚ùå Possible               | ‚ùå No                  |
| Use case         | Data transformation      | Async chaining           |


2Ô∏è‚É£ thenAccept vs thenRun: üëâ This is about whether you need the result or not.
	üîπ thenAccept ‚Äî consume the result
		üìå What it does: Takes the result, Performs a side-effect, Returns CompletableFuture<Void>
			CompletableFuture<Void> cf =
				CompletableFuture.supplyAsync(() -> "Hello")
					.thenAccept(msg -> System.out.println(msg));

	üîπ thenRun ‚Äî run without result
		üìå What it does: Does not receive the result, Just runs some logic, Returns CompletableFuture<Void>
			CompletableFuture<Void> cf =
				CompletableFuture.supplyAsync(() -> "Hello")
					.thenRun(() -> System.out.println("Done"));

	üéØ Interview Line: ‚ÄúthenAccept uses the result; thenRun ignores it.‚Äù

üî• thenAccept vs thenRun Summary
| Aspect           | thenAccept               | thenRun                 |
| ---------------- | ------------------------ | ----------------------- |
| Access to result | ‚úÖ Yes                    | ‚ùå No                    |
| Return type      | CompletableFuture<Void>  | CompletableFuture<Void> |
| Use case         | Logging, saving, sending | Notifications, cleanup  |


=============================================================================================================
üîπ Q54. What thread executes CompletableFuture by default?
=============================================================================================================
1Ô∏è‚É£ What Thread Executes CompletableFuture by Default?
	By default, CompletableFuture async tasks run on the ForkJoinPool.commonPool().
	This applies to: supplyAsync(), runAsync(), thenApplyAsync(), thenComposeAsync(), thenAcceptAsync(), etc.

		CompletableFuture.supplyAsync(() -> {
			System.out.println(Thread.currentThread().getName());
			return 42;
		});


	üëâ Output will look like: ForkJoinPool.commonPool-worker-1

2Ô∏è‚É£ What Is ForkJoinPool.commonPool()?
	A shared, global thread pool, Introduced in Java 8
	Used by: CompletableFuture, Parallel Streams, Other async APIs

	üîπ Size of commonPool: Runtime.getRuntime().availableProcessors() - 1 : So on an 8-core machine ‚Üí 7 threads.
	
	üéØ Interview line: ‚ÄúCompletableFuture uses the common ForkJoinPool unless an executor is explicitly provided.‚Äù

3Ô∏è‚É£ Important Exception: Non-Async Methods :‚ùó This is where many people get confused
	Methods WITHOUT Async run on the same thread that completes the previous stage.

		CompletableFuture
			.supplyAsync(() -> 10)
			.thenApply(x -> {
				System.out.println(Thread.currentThread().getName());
				return x * 2;
			});

		supplyAsync ‚Üí commonPool thread, thenApply ‚Üí same thread, not a new one

	üéØ Interview line: ‚ÄúNon-async stages run in the thread that completes the previous stage.‚Äù

4Ô∏è‚É£ Async vs Non-Async (Very Important)
| Method           | Thread Used                   |
| ---------------- | ----------------------------- |
| supplyAsync      | ForkJoinPool.commonPool       |
| runAsync         | ForkJoinPool.commonPool       |
| thenApply        | Same thread as previous stage |
| thenCompose      | Same thread as previous stage |
| thenAccept       | Same thread as previous stage |
| thenApplyAsync   | ForkJoinPool.commonPool       |
| thenComposeAsync | ForkJoinPool.commonPool       |

5Ô∏è‚É£ Providing Your Own Executor (Best Practice)
	You can override the default behavior: 	
		ExecutorService executor = Executors.newFixedThreadPool(5);
		CompletableFuture.supplyAsync(() -> task(), executor)
                 .thenApplyAsync(result -> process(result), executor);
		
		‚úî Better control, ‚úî Avoids starving commonPool, ‚úî Recommended for production systems

	üéØ Interview line: ‚ÄúIn production, always prefer a custom executor over the common pool.‚Äù

6Ô∏è‚É£ Why Using commonPool Blindly Is Dangerous
	‚ùå Blocking I/O tasks can: Starve the common pool
		Affect: Parallel streams, Other CompletableFutures, Cause system-wide slowdown
		Especially dangerous in: Web applications, Microservices, Blocking DB / REST calls

=============================================================================================================
üîπ Q55. Difference between async and non-async methods.
=============================================================================================================
1Ô∏è‚É£ What Does Async vs Non-Async Mean?
	In CompletableFuture, the difference between async and non-async methods is NOT about parallelism ‚Äî it‚Äôs about which thread executes the next stage.

2Ô∏è‚É£ Non-Async Methods (thenApply, thenCompose, thenAccept, thenRun)
	üîπ How They Execute: They run in the same thread that completes the previous stage.
		
				CompletableFuture
					.supplyAsync(() -> {
						System.out.println(Thread.currentThread().getName());
						return 10;
					})
					.thenApply(x -> {
						System.out.println(Thread.currentThread().getName());
						return x * 2;
					});

				Output (example)
				ForkJoinPool.commonPool-worker-1
				ForkJoinPool.commonPool-worker-1

		‚úî No thread switch, ‚úî Lower overhead, ‚ùå Can block important threads if logic is heavy

	üß† Key Insight
		If previous stage completes on:
			main thread ‚Üí next stage runs on main thread
			ForkJoinPool thread ‚Üí next stage runs there
			custom executor thread ‚Üí next stage runs there

	üéØ Interview line: ‚ÄúNon-async stages inherit the thread of the completing stage.‚Äù

3Ô∏è‚É£ Async Methods (thenApplyAsync, thenComposeAsync, thenAcceptAsync, thenRunAsync)
	üîπ How They Execute: They always run in a different thread, obtained from an executor.

				CompletableFuture
					.supplyAsync(() -> {
						System.out.println(Thread.currentThread().getName());
						return 10;
					})
					.thenApplyAsync(x -> {
						System.out.println(Thread.currentThread().getName());
						return x * 2;
					});

				Output (example)
				ForkJoinPool.commonPool-worker-1
				ForkJoinPool.commonPool-worker-3
				
			‚úî Forces thread switch, ‚úî Prevents blocking previous thread, ‚ùå Slight scheduling overhead

üîë Default Executor: If no executor is supplied: Uses ForkJoinPool.commonPool()
		You can override it: .thenApplyAsync(fn, customExecutor);
		
5Ô∏è‚É£ When Should You Use Which?
	‚úÖ Use Non-Async When: Logic is small and fast, No blocking I/O, You want minimal overhead, Continuation is lightweight
			.thenApply(x -> x + 1)

	‚úÖ Use Async When: Logic is CPU-heavy or blocking, You don‚Äôt want to block completion thread, You need explicit thread control
			.thenApplyAsync(x -> heavyComputation(), executor)	

			
=============================================================================================================
üîπ Q56. Why HashMap not thread safe? Difference between HashMap and ConcurrentHashMap. 
=============================================================================================================
1Ô∏è‚É£ HashMap is not thread-safe because it does not provide any synchronization or memory-visibility guarantees when accessed by multiple threads. But that alone is not enough for interviews. Here‚Äôs the real reason üëá

2Ô∏è‚É£ What Goes Wrong Internally in HashMap?
	‚ùå 1. Race Conditions During Put/Get
			HashMap operations like put() and get() are not atomic.
				map.put(key, value);
			Internally involves: Calculating hash, Finding bucket, Traversing list/tree, Updating pointers
			If two threads modify the same bucket: Data corruption can occur, Entries can be lost
	
	‚ùå 2. Infinite Loop During Rehashing (Classic Bug)
			Before Java 8 especially:
				When HashMap resizes (rehashing): Buckets are re-linked, Multiple threads rehashing simultaneously can create a cycle in the linked list
				Result: map.get(key); // CPU goes to 100%, infinite loop
		
		üéØ Interview line: ‚ÄúConcurrent resize in HashMap can cause infinite loops.‚Äù

	‚ùå 3. Memory Visibility Issues
			Without synchronization: One thread‚Äôs updates may not be visible to another, Due to CPU cache + instruction reordering
				map.put("a", 1);   // Thread A
				map.get("a");      // Thread B ‚Üí may return null

	‚ùå 4. Structural Corruption: 
			Multiple threads can: Overwrite internal pointers, Corrupt bucket chains, Break tree structure (Java 8+)
			üß† Key Insight: HashMap is designed for single-threaded performance, not concurrency safety.

3Ô∏è‚É£ Why ConcurrentHashMap Is Thread-Safe?
	üîπ Core Idea:
		ConcurrentHashMap allows safe concurrent access with high performance by minimizing locking.
		It does NOT lock the entire map.
	
4Ô∏è‚É£ How ConcurrentHashMap Achieves Thread Safety
	üîπ Java 7 and Earlier: Segment-based locking, Map divided into segments, Each segment had its own lock
	üîπ Java 8+ (Important for Interviews): Java 8 redesigned ConcurrentHashMap: ‚úî No segments
			‚úî Uses: CAS (Compare-And-Swap), Fine-grained locking, Volatile reads, Synchronized only on bucket level
	
		üß™ Put Operation (Simplified): 
			First try CAS to insert
			If contention: Lock only the bucket, Other buckets remain accessible
			‚úî High concurrency, ‚úî Low contention

		üß™ Read Operation: map.get(key);
			‚úî Lock-free, ‚úî Uses volatile reads, ‚úî Extremely fast

5Ô∏è‚É£ Key Differences: HashMap vs ConcurrentHashMap
| Aspect                      | HashMap   | ConcurrentHashMap   |
| --------------------------- | --------- | ------------------- |
| Thread-safe                 | ‚ùå No      | ‚úÖ Yes               |
| Synchronization             | ‚ùå None    | ‚úÖ Fine-grained      |
| Null keys                   | ‚úÖ One     | ‚ùå Not allowed       |
| Null values                 | ‚úÖ Allowed | ‚ùå Not allowed       |
| Performance (single thread) | Faster    | Slightly slower     |
| Performance (multi-thread)  | ‚ùå Unsafe  | ‚úÖ Scales well       |
| Fail-fast iterator          | ‚úÖ Yes     | ‚ùå Weakly consistent |
| Infinite loop risk          | ‚úÖ Yes     | ‚ùå No                |

6Ô∏è‚É£ Why ConcurrentHashMap Does NOT Allow Nulls?
	üéØ Interview favorite question.
			Reason: 
				null is used to indicate absence of mapping
				In concurrent access: get() returning null is ambiguous: key absent? value is null?
				Disallowing null removes ambiguity

7Ô∏è‚É£ Iterators: Fail-Fast vs Weakly Consistent
	HashMap: Fail-fast, Throws ConcurrentModificationException
	ConcurrentHashMap: Weakly consistent, Does not throw exception
	Reflects some (not necessarily all) updates
	üéØ Interview line: ‚ÄúConcurrentHashMap iterators trade consistency for availability.‚Äù

8Ô∏è‚É£ When Should You Use What?
	‚úÖ Use HashMap When: Single-threaded, Local method usage, No concurrent access
	‚úÖ Use ConcurrentHashMap When: Shared mutable state, High read/write concurrency, Performance matters
	‚ùå Avoid Collections.synchronizedMap() for high concurrency ‚Äî it uses global locking.

=============================================================================================================
üîπ Q58. Difference between CopyOnWriteArrayList and synchronizedList. When should CopyOnWrite be used?
=============================================================================================================
1Ô∏è‚É£ What Is synchronizedList? List<Integer> list = Collections.synchronizedList(new ArrayList<>());
	üîπ How It Works: 
			Wraps a normal ArrayList, Uses one intrinsic lock for every operation
			All methods are synchronized on the same monitor
			üëâ Only one thread at a time can read or write.
		üîß Characteristics: 
				Thread-safe via coarse-grained locking
				Reads and writes block each other
				Iteration must be manually synchronized
					synchronized (list) {
						for (int i : list) {
							// safe iteration
						}
					}

2Ô∏è‚É£ What Is CopyOnWriteArrayList? List<Integer> list = new CopyOnWriteArrayList<>();
	üîπ How It Works
			On every write (add/remove/update): Creates a new copy of the underlying array
			Readers: Read from a stable snapshot, No locking at all
			üëâ Reads are lock-free, writes are expensive.
		üîß Characteristics:
				Thread-safe via copy-on-write
				No synchronization needed for iteration
				Iterators are snapshot-based
					for (int i : list) {
						// always safe
					}

3Ô∏è‚É£ Core Difference (High-Level)
	synchronizedList uses locking; CopyOnWriteArrayList uses immutability + copying.

4Ô∏è‚É£ CopyOnWriteArrayList vs synchronizedList (Side-by-Side)
| Aspect            | synchronizedList    | CopyOnWriteArrayList |
| ----------------- | ------------------- | -------------------- |
| Thread safety     | Via locking         | Via copying          |
| Read concurrency  | ‚ùå One at a time     | ‚úÖ Fully concurrent   |
| Write cost        | Low                 | ‚ùå High (array copy)  |
| Read cost         | Medium (lock)       | ‚úÖ Very fast          |
| Iteration         | Must synchronize    | Safe without sync    |
| Iterator behavior | Fail-fast           | Snapshot (no CME)    |
| Memory usage      | Low                 | ‚ùå Higher             |
| Best for          | Balanced read/write | Read-heavy workloads |

5Ô∏è‚É£ Iteration Difference (Interview Favorite)
	üî¥ synchronizedList (Fail-Fast)
			for (int i : list) { } // ‚ùå CME if not synchronized

			Requires:
				synchronized (list) {
					for (int i : list) { }
				}

	üü¢ CopyOnWriteArrayList (Snapshot Iterator)
			for (int i : list) { } // ‚úÖ always safe
			Iterator sees a snapshot, Does not reflect later writes, Never throws ConcurrentModificationException
		üéØ Interview line: ‚ÄúCopyOnWriteArrayList iterates over a snapshot, not live data.‚Äù

6Ô∏è‚É£ When Should You Use CopyOnWriteArrayList?
	‚úÖ Use CopyOnWriteArrayList When: 
		Reads vastly outnumber writes, Iteration is frequent
		You want lock-free reads, Slightly stale data is acceptable

	üß™ Real-World Examples: Event listeners list, Observer pattern, Configuration readers, Caches of reference data

7Ô∏è‚É£ When NOT to Use CopyOnWriteArrayList ‚ùå
		Write-heavy workloads, Large lists with frequent updates, Real-time systems where updates must be immediately visible, Memory-sensitive systems
		üéØ Interview line: ‚ÄúCopyOnWriteArrayList is terrible for write-heavy workloads.‚Äù

8Ô∏è‚É£ Why CopyOnWrite Exists (Design Insight)
	The idea: ‚ÄúMake reads extremely cheap by making writes expensive.‚Äù
	This flips the traditional locking model.

=============================================================================================================
üîπ Q59. What is fail-fast vs fail-safe iterator? Which collections are thread safe by default?
=============================================================================================================
1Ô∏è‚É£ What Is a Fail-Fast Iterator?
	A fail-fast iterator throws a ConcurrentModificationException if the collection is structurally modified while it is being iterated (except through the iterator‚Äôs own remove()).
	üëâ It fails immediately to prevent unpredictable behavior.

	üîç How It Works Internally: Collection maintains a modCount, Iterator captures expectedModCount
		If they differ ‚Üí exception thrown
		üß™ Example (Fail-Fast)
					List<Integer> list = new ArrayList<>();
					list.add(1);
					list.add(2);

					for (Integer i : list) {
						list.add(3); // ‚ùå ConcurrentModificationException
					}
			üîß Characteristics: Detects concurrent modification, Not thread-safe, Used mainly in non-concurrent collections
				Best-effort (not guaranteed in all cases)

2Ô∏è‚É£ What Is a Fail-Safe Iterator?
	A fail-safe iterator does not throw ConcurrentModificationException if the collection is modified during iteration.
	üëâ It works on a snapshot or separate structure.
	üß™ Example (Fail-Safe)
				List<Integer> list = new CopyOnWriteArrayList<>();
				list.add(1);
				list.add(2);

				for (Integer i : list) {
					list.add(3); // ‚úÖ No exception
				}
		üîß Characteristics: Iterator works on a copy/snapshot, No exception on modification, Thread-safe, May not reflect latest updates

	üéØ Interview line: ‚ÄúFail-safe iterators trade real-time consistency for safety.‚Äù

3Ô∏è‚É£ Fail-Fast vs Fail-Safe (Side-by-Side)
| Aspect                   | Fail-Fast             | Fail-Safe                               |
| ------------------------ | --------------------- | --------------------------------------- |
| Behavior on modification | Throws exception      | Continues safely                        |
| Uses snapshot            | ‚ùå No                  | ‚úÖ Yes                                   |
| Thread-safe              | ‚ùå No                  | ‚úÖ Yes                                   |
| Consistency              | Strong (detects bugs) | Weak (eventual)                         |
| Performance              | Fast                  | Slower (copy overhead)                  |
| Example collections      | ArrayList, HashMap    | CopyOnWriteArrayList, ConcurrentHashMap |

4Ô∏è‚É£ Which Collections Are Thread-Safe by Default?
	‚úÖ Thread-Safe Collections (By Default)
		üîπ Legacy (Synchronized by design): Vector, Hashtable, Stack
			‚ö† Thread-safe but poor performance due to global locking.

		üîπ Concurrent Collections (Preferred): From java.util.concurrent:
			ConcurrentHashMap, CopyOnWriteArrayList, CopyOnWriteArraySet, ConcurrentLinkedQueue, ConcurrentLinkedDeque
			BlockingQueue implementations: ArrayBlockingQueue, LinkedBlockingQueue, PriorityBlockingQueue, DelayQueue

		‚úî Thread-safe, ‚úî High concurrency, ‚úî Designed for multi-core systems

	
	‚ùå NOT Thread-Safe by Default: Require external synchronization:
		ArrayList, LinkedList, HashMap, HashSet, TreeMap, TreeSet

	üü° Conditionally Thread-Safe (Wrappers)
			List list = Collections.synchronizedList(new ArrayList<>());
			Map map = Collections.synchronizedMap(new HashMap<>());
		‚úî Thread-safe via global locking, ‚ùå Iteration still needs external synchronization

5Ô∏è‚É£ Iterator Behavior in Concurrent Collections (Interview Favorite)
	üîπ ConcurrentHashMap: Iterator is weakly consistent, Does not throw CME, May or may not reflect recent updates
	üîπ CopyOnWriteArrayList: Iterator is snapshot-based, Never reflects modifications after iterator creation

=============================================================================================================
üîπ Q60. What are atomic classes? Difference between atomic and volatile.
=============================================================================================================
1Ô∏è‚É£  Atomic classes are a set of classes in java.util.concurrent.atomic that provide thread-safe, lock-free operations on   single variables.
They guarantee: Atomicity, Visibility, Ordering
üëâ All without using synchronized.

	üîπ Common Atomic Classes (Must Know)
		AtomicInteger, AtomicLong, AtomicBoolean, AtomicReference<T>, AtomicIntegerArray, AtomicLongArray
		AtomicStampedReference (ABA problem)
		üß™ Example
					AtomicInteger counter = new AtomicInteger(0);
					counter.incrementAndGet(); // atomic
					counter.compareAndSet(1, 5);
			‚úî Thread-safe, No locking, High performance

2Ô∏è‚É£ How Atomic Classes Work Internally
	üî• Core Mechanism: CAS (Compare-And-Swap): CAS is a CPU-level atomic instruction.

			if (currentValue == expectedValue)
				currentValue = newValue;
			else
				fail
		
		The JVM: Uses CPU instructions (like CMPXCHG), Retries automatically if CAS fails, Ensures correctness without blocking

		üéØ Interview line: ‚ÄúAtomic classes are implemented using CAS and memory barriers.‚Äù

3Ô∏è‚É£ What Is volatile? 
	volatile is a keyword that guarantees: Visibility, Ordering, But NOT atomicity (for compound operations).
		volatile int count = 0;
	
	‚úî Other threads see latest value, ‚ùå count++ is NOT thread-safe

4Ô∏è‚É£ Difference Between Atomic and volatile (Core Question)
| Aspect         | Atomic Classes        | volatile             |
| -------------- | --------------------- | -------------------- |
| Atomicity      | ‚úÖ Yes                 | ‚ùå No                 |
| Visibility     | ‚úÖ Yes                 | ‚úÖ Yes                |
| Lock-free      | ‚úÖ Yes                 | ‚úÖ Yes                |
| Compound ops   | ‚úÖ Safe                | ‚ùå Unsafe             |
| Implementation | CAS + memory barriers | Memory barriers only |
| Performance    | High                  | Very high            |
| Use case       | Counters, sequences   | Flags, state         |
| Blocking       | ‚ùå No                  | ‚ùå No                 |

5Ô∏è‚É£ Why volatile int i++ Is NOT Enough
		volatile int i = 0;
		i++; // NOT atomic
	Why? 
		i++ = read ‚Üí increment ‚Üí write
		Multiple threads can interleave ‚Üí lost updates

	‚úÖ Correct Using AtomicInteger
		AtomicInteger i = new AtomicInteger(0);
		i.incrementAndGet(); // atomic

6Ô∏è‚É£ Atomic Classes vs synchronized (Senior Insight)
	| Aspect              | Atomic | synchronized |
	| ------------------- | ------ | ------------ |
	| Blocking            | ‚ùå No   | ‚úÖ Yes        |
	| Context switching   | ‚ùå No   | ‚úÖ Yes        |
	| Scalability         | High   | Lower        |
	| Complexity          | Low    | Medium       |
	| Protects invariants | ‚ùå No   | ‚úÖ Yes        |

	üéØ Interview line: ‚ÄúAtomic classes are great for single variables, not invariants.‚Äù

7Ô∏è‚É£ When Should You Use Atomic Classes?
	‚úÖ Use Atomic When: Single variable, Counters, Statistics, Sequence generators, Non-blocking algorithms
	‚ùå Avoid Atomic When:
		Multiple variables must be updated together
		Complex invariants, Business logic consistency is required
		In those cases ‚Üí use locks.

8Ô∏è‚É£ AtomicReference (Advanced Interview Topic)
	AtomicReference<User> ref = new AtomicReference<>();
	ref.compareAndSet(oldUser, newUser);
	
	‚úî Atomic object replacement, ‚úî Lock-free state transitions
	
	Used in: State machines, Caches, Immutable object swaps

=============================================================================================================
üîπ Q61. What is CAS (Compare And Swap)? How AtomicInteger works internally?
=============================================================================================================
1Ô∏è‚É£  CAS (Compare-And-Swap) is a hardware-level atomic instruction that updates a value only if it matches an expected value.
	Conceptually: 
		CAS(memoryLocation, expectedValue, newValue)
		if (memoryLocation == expectedValue)
			memoryLocation = newValue
			return true
		else
			return false

		üëâ This entire operation is atomic ‚Äî no other thread can interleave.

	üß† Why CAS Exists: 
		Avoid blocking (synchronized), Avoid context switches
		Enable lock-free concurrency, Scale better on multi-core CPUs

	üéØ Interview line: ‚ÄúCAS allows threads to coordinate without blocking.‚Äù

2Ô∏è‚É£ CAS at the CPU Level
	Modern CPUs provide CAS instructions: x86 ‚Üí CMPXCHG, ARM ‚Üí LDREX / STREX
	Guarantees: Atomicity, Memory visibility, Happens-before semantics
	üëâ JVM directly maps CAS to these instructions.

3Ô∏è‚É£ How AtomicInteger Uses CAS Internally
	üîπ Key Internal Field
			public class AtomicInteger extends Number {
				private volatile int value;
			}
		
		Important points: value is volatile ‚Üí visibility guaranteed, Updates are done using CAS

4Ô∏è‚É£ Internal Working of incrementAndGet()
	üî• Code You Write
			AtomicInteger counter = new AtomicInteger(0);
			counter.incrementAndGet();

		üîç What Actually Happens Internally (Simplified)
				public final int incrementAndGet() {
					for (;;) {
						int current = value;          // volatile read
						int next = current + 1;
						if (compareAndSet(current, next)) {
							return next;
						}
						// else retry
					}
				}
			This is a spin loop.

		üß† Step-by-Step Execution
			1Ô∏è‚É£ Thread reads current value
			2Ô∏è‚É£ Calculates new value
			3Ô∏è‚É£ Calls CAS
			4Ô∏è‚É£ If CAS succeeds ‚Üí done
			5Ô∏è‚É£ If CAS fails ‚Üí retry

			CAS fails if: Another thread updated the value in between

	üéØ Interview Line: ‚ÄúAtomicInteger uses a CAS-based retry loop to ensure atomic updates.‚Äù

5Ô∏è‚É£ Why CAS Is Lock-Free but Not Wait-Free
	Lock-free: At least one thread always makes progress
	Not wait-free: A thread may retry indefinitely under heavy contention
		Thread A ‚Üí CAS fails ‚Üí retry
		Thread B ‚Üí CAS fails ‚Üí retry
	üëâ But no thread is blocked.

6Ô∏è‚É£ Why AtomicInteger Is Better Than synchronized Counter
	‚ùå synchronized Counter
			synchronized void increment() {
				count++;
			}
		Thread blocking, Context switching, Lower scalability

	‚úÖ AtomicInteger
			counter.incrementAndGet();
		No blocking, Uses CPU instructions, Scales better under contention
	
	üéØ Interview line: ‚ÄúCAS avoids blocking but may spin under contention.‚Äù

7Ô∏è‚É£ CAS vs volatile (Important Clarification)
	| Feature    | volatile | CAS      |
	| ---------- | -------- | -------- |
	| Visibility | ‚úÖ        | ‚úÖ        |
	| Atomicity  | ‚ùå        | ‚úÖ        |
	| Blocking   | ‚ùå        | ‚ùå        |
	| Use case   | Flags    | Counters |

	üëâ AtomicInteger = volatile + CAS

8Ô∏è‚É£ The ABA Problem (Advanced Topic)
	üîπ What Is ABA?
			Value = A
			Thread 1 reads A
			Thread 2 changes A ‚Üí B ‚Üí A
			Thread 1 CAS succeeds (but state changed!)
			CAS cannot detect this.

	üîπ How Java Solves It: AtomicStampedReference, AtomicMarkableReference, AtomicStampedReference<Integer> ref;
	üéØ Interview bonus point: ‚ÄúCAS alone can‚Äôt detect ABA; stamped references fix it.‚Äù

9Ô∏è‚É£ Why AtomicInteger Cannot Protect Invariants
		x.incrementAndGet();
		y.incrementAndGet();
	‚ùå Not atomic together, ‚ùå Invariant can break
üëâ CAS works per variable, not across variables.


=============================================================================================================
üîπ Q63. Difference between AtomicLong and LongAdder. When to use LongAdder?
=============================================================================================================
1Ô∏è‚É£ AtomicLong vs LongAdder ‚Äî High-Level Difference
	AtomicLong uses a single CAS-based value, while LongAdder spreads updates across multiple counters (cells) to reduce contention under high concurrency.

2Ô∏è‚É£ What Is AtomicLong?
	üîπ How It Works:
		Maintains: one volatile long value, Updates via CAS (Compare-And-Swap), All threads compete on one memory location
				AtomicLong counter = new AtomicLong(0);
				counter.incrementAndGet();
		
		üîß Characteristics: Lock-free, Strong consistency, Exact value always available, CAS retries under contention
		‚ö† Problem Under High Contention:
			Many threads update the same value, CAS failures increase, CPU spins ‚Üí performance degrades

üéØ Interview line: ‚ÄúAtomicLong doesn‚Äôt scale well under high contention.‚Äù

3Ô∏è‚É£ What Is LongAdder?
	üîπ How It Works
		Maintains: One base value, Multiple cells (striped counters), Threads update different cells instead of one value
				LongAdder adder = new LongAdder();
				adder.increment();
				long sum = adder.sum();

		üîß Internal Idea (Simplified)								
				Thread 1 ‚Üí Cell[0]
				Thread 2 ‚Üí Cell[3]
				Thread 3 ‚Üí Cell[1]
		üëâ Updates are distributed ‚Üí less contention
		üß† Key Insight: ‚ÄúLongAdder trades exact immediacy for scalability.‚Äù

4Ô∏è‚É£ How LongAdder Improves Performance
	Reduces CAS contention, Minimizes false sharing, Scales linearly with threads, Uses CAS internally but on different cells

5Ô∏è‚É£ Key Difference: Consistency vs Throughput
	| Aspect              | AtomicLong     | LongAdder             |
	| ------------------- | -------------- | --------------------- |
	| Update mechanism    | Single CAS     | Striped CAS           |
	| Contention handling | Poor           | Excellent             |
	| Read cost           | O(1)           | O(n) (sum cells)      |
	| Value accuracy      | Always exact   | Eventually consistent |
	| Best under          | Low contention | High contention       |
	| Memory usage        | Low            | Higher                |

6Ô∏è‚É£ Why LongAdder.sum() Is Not Constant-Time
			long value = adder.sum();
		Reads base, Iterates all cells, Aggregates values
	
	‚úî Accurate at moment of call, ‚ùå More expensive than AtomicLong.get()

7Ô∏è‚É£ When Should You Use LongAdder?
	‚úÖ Use LongAdder When: High write contention, Frequent increments, Occasional reads, Metrics, counters, statistics
	üß™ Real-World Examples: Request counters, QPS metrics, Hit/miss counters, Monitoring systems
	üéØ Interview line: ‚ÄúLongAdder is ideal for hot counters.‚Äù

8Ô∏è‚É£ When NOT to Use LongAdder ‚ùå
	When you need: Precise value after every update, Strong consistency guarantees, When updates are rare
	When value must be read very frequently
	üëâ Use AtomicLong instead.

9Ô∏è‚É£ Why LongAdder Is NOT a Drop-In Replacement
		if (adder.sum() == 100) {
			// ‚ùå unsafe logic
		}
		Because: Value may change immediately after sum(), No CAS-style conditional update
	
	üéØ Interview line: ‚ÄúLongAdder is not suitable for CAS-style logic.‚Äù


=============================================================================================================
üîπ Q64. What is ForkJoinPool? Difference between ForkJoinPool and ExecutorService.
=============================================================================================================
1Ô∏è‚É£  ForkJoinPool is a specialized thread pool designed for parallel execution of recursive, divide-and-conquer tasks.
	It works by:
		Forking a large task into smaller subtasks, Joining their results
		Using work-stealing to balance load across CPU cores, Introduced in Java 7.

2Ô∏è‚É£ Core Idea: Divide and Conquer
	Instead of one big task:
					Task
					 ‚îú‚îÄ‚îÄ Subtask A
					 ‚îú‚îÄ‚îÄ Subtask B
					 ‚îî‚îÄ‚îÄ Subtask C
		Each subtask: Is small, Can run independently, Can be stolen by idle threads

3Ô∏è‚É£ What Is Work-Stealing?
	üîπ How It Works: 
		Each worker thread has its own deque, Threads push new subtasks to the bottom
		Idle threads steal from the top of other workers‚Äô deques

	‚úî Minimizes contention, Maximizes CPU utilization

	üéØ Interview line: ‚ÄúForkJoinPool balances load using work-stealing.‚Äù

4Ô∏è‚É£ ForkJoinPool Components
	üîπ Task Types: 
			RecursiveTask<V> ‚Üí returns result
			RecursiveAction ‚Üí no result
	
		üß™ Example:
				class SumTask extends RecursiveTask<Integer> {
					private final int[] arr;
					private final int start, end;

					protected Integer compute() {
						if (end - start <= 10) {
							int sum = 0;
							for (int i = start; i < end; i++) sum += arr[i];
							return sum;
						}
						int mid = (start + end) / 2;
						SumTask left = new SumTask(arr, start, mid);
						SumTask right = new SumTask(arr, mid, end);

						left.fork();
						return right.compute() + left.join();
					}
				}

5Ô∏è‚É£ ForkJoinPool vs ExecutorService (High-Level)
| Aspect           | ForkJoinPool                | ExecutorService              |
| ---------------- | --------------------------- | ---------------------------- |
| Purpose          | Parallel divide-and-conquer | General async task execution |
| Task type        | Recursive                   | Independent                  |
| Load balancing   | Work-stealing               | Shared queue                 |
| Thread usage     | CPU-bound optimized         | General-purpose              |
| Blocking tasks   | ‚ùå Bad                       | ‚úÖ Better                     |
| Parallel streams | ‚úÖ Uses                      | ‚ùå No                         |

6Ô∏è‚É£ Key Differences Explained (Interview-Critical)
	üîπ 1. Task Granularity 
			ForkJoinPool: Many small tasks, Designed for splitting
			ExecutorService: Fewer, independent tasks, No splitting awareness

	üîπ 2. Queue Model
			ForkJoinPool: Per-thread deque, Work-stealing
			ExecutorService: Central blocking queue
	
	üîπ 3. Thread Utilization
			ForkJoinPool: Keeps CPUs busy, Idle threads steal work
			ExecutorService: Threads wait on queue
	
	üîπ 4. Blocking Behavior (Very Important)
			ForkJoinPool: Blocking is dangerous, Can starve worker threads
			ExecutorService: Handles blocking tasks better
	üéØ Interview line: ‚ÄúForkJoinPool is for CPU-bound tasks, not blocking I/O.‚Äù

7Ô∏è‚É£ Common Uses of ForkJoinPool
	Parallel streams, Large array processing, Recursive algorithms, Sorting, Searching, Map-reduce style operations

8Ô∏è‚É£ ForkJoinPool.commonPool(): 	Global shared pool
	Used by: CompletableFuture, Parallel streams, Size ‚âà CPU cores ‚àí 1
	‚ö† Blocking here affects entire JVM.

9Ô∏è‚É£ When to Use What?
	‚úÖ Use ForkJoinPool When: CPU-bound work, Recursive tasks, Large datasets, Parallel computation
	‚ùå Avoid ForkJoinPool When: Blocking I/O, Long-running tasks, External calls

	‚úÖ Use ExecutorService When: I/O-bound tasks, Independent tasks, You need bounded queues, Predictable throughput

=============================================================================================================
üîπ Q65. What is work-stealing algorithm? When should ForkJoin be used?
=============================================================================================================
1Ô∏è‚É£ What Is the Work-Stealing Algorithm?
	Work-stealing is a scheduling algorithm where:
		Each worker thread maintains its own task deque (double-ended queue)
		Busy threads push new subtasks to their own deque
		Idle threads steal tasks from other threads
		üëâ This keeps all CPU cores busy without centralized coordination.

2Ô∏è‚É£ How Work-Stealing Works (Step-by-Step)
	üîπ Core Rules: Each worker thread has its own deque, New tasks are pushed to the bottom
		The owning thread: Pops tasks from the bottom (LIFO)
		Idle threads: Steal tasks from the top (FIFO) of another thread‚Äôs deque
	
	üß† Why Two Ends? Reduces contention, Improves cache locality, Minimizes synchronization
	üéØ Interview line: ‚ÄúWork-stealing uses per-thread deques to minimize contention.‚Äù

3Ô∏è‚É£ Visual Example (Mental Model)
		Worker-1 Deque: [ TaskA | TaskB | TaskC ]
							  ‚Üë pop (owner)
		Worker-2 (idle) ‚Üí steals TaskA from top
	
	‚úî Worker-2 becomes busy, ‚úî Worker-1 continues work

4Ô∏è‚É£ Why Work-Stealing Is Better Than a Shared Queue?
	üî¥ Shared Queue (ExecutorService): All threads compete for one queue, High contention, Poor scalability
	üü¢ Work-Stealing (ForkJoinPool): Mostly thread-local queues, Stealing only when needed, Scales well on multi-core CPUs

5Ô∏è‚É£ When Should ForkJoinPool Be Used?
	‚úÖ Use ForkJoinPool When:
		Tasks can be recursively split, Workload is CPU-bound, Subtasks are independent, Large dataset processing
		Parallel algorithms
	
	üß™ Real-World Examples: Parallel array processing, Sorting (merge sort, quicksort), Searching, Map-reduce style tasks
		Parallel streams, CompletableFuture CPU tasks

	üéØ Interview line: ‚ÄúForkJoin is ideal for fine-grained CPU-bound parallelism.‚Äù

6Ô∏è‚É£ When Should ForkJoinPool NOT Be Used?
	‚ùå Avoid ForkJoinPool When: Tasks are blocking (I/O, DB, REST calls), Tasks are long-running, Work cannot be divided
		Strict ordering is required
		Why? Blocking steals worker threads, Starves the pool, Reduces parallelism

7Ô∏è‚É£ How ForkJoinPool Handles Blocking (Advanced)
	Java provides: ForkJoinPool.managedBlock(...)
		Allows the pool to: Temporarily add threads, Avoid starvation
		‚ö† Still not ideal for heavy I/O.

8Ô∏è‚É£ ForkJoin vs ExecutorService (Quick Summary)
| Aspect      | ForkJoinPool  | ExecutorService |
| ----------- | ------------- | --------------- |
| Scheduling  | Work-stealing | Shared queue    |
| Task nature | Recursive     | Independent     |
| Best for    | CPU-bound     | I/O-bound       |
| Blocking    | ‚ùå Bad         | ‚úÖ OK            |
| Scalability | Very high     | Moderate        |

=============================================================================================================
üîπ Q66. How parallel streams work internally? Why parallel streams can be dangerous? How to control parallel stream thread pool?
=============================================================================================================
1Ô∏è‚É£  A parallel stream splits a stream pipeline into multiple subtasks and executes them in parallel using ForkJoinPool.
		list.parallelStream()
			.map(this::process)
			.forEach(System.out::println);
	
	üîß Internal Mechanics (Step by Step)
			1Ô∏è‚É£ Source Splitting (Spliterator): Each stream source has a Spliterator
				It knows how to: Estimate size, Split itself into smaller chunks
					Data ‚Üí Spliterator ‚Üí split ‚Üí split ‚Üí split
			
			2Ô∏è‚É£ Task Creation: 
				Each split becomes a ForkJoinTask, Tasks are recursively subdivided until they are ‚Äúsmall enough‚Äù

			3Ô∏è‚É£ Execution via ForkJoinPool
				Tasks are submitted to ForkJoinPool.commonPool(), Threads use work-stealing
				Idle threads steal subtasks from busy threads

			4Ô∏è‚É£ Result Combination: Partial results are merged, Final result is produced

		üéØ Interview line: ‚ÄúParallel streams are implemented using ForkJoinPool and work-stealing.‚Äù

2Ô∏è‚É£ Which Thread Pool Do Parallel Streams Use?
	üîπ Default Behavior: Parallel streams always use ForkJoinPool.commonPool()
		Same pool used by: CompletableFuture (by default), Other parallel streams, Size ‚âà CPU cores - 1
		‚ö† This is global and shared across the JVM.

3Ô∏è‚É£ Why Parallel Streams Can Be Dangerous. This is the most important part.
	üî¥ 1. Blocking Operations Can Starve the JVM
			list.parallelStream()
				.map(this::callRemoteService) // blocking I/O
				.forEach(System.out::println);
			‚ùå Each blocking call occupies a ForkJoin worker, ‚ùå Other tasks (CF, streams) wait, ‚ùå System-wide slowdown

			üéØ Interview line: ‚ÄúBlocking inside parallel streams can starve the common pool.‚Äù

	üî¥ 2. No Control Over Thread Pool (By Default)
			You cannot: Set queue size, Set rejection policy,Isolate workloads, All parallel streams share one pool.

	üî¥ 3. Side Effects & Race Conditions
			int sum = 0;
			list.parallelStream().forEach(i -> sum += i); // ‚ùå broken
			
			Parallel streams require stateless, non-mutating operations, Shared mutable state ‚Üí race conditions

	üî¥ 4. Performance Can Be Worse Than Sequential
			Parallel streams add overhead: Task splitting, Context switching, Result merging
			For: Small collections, Cheap operations ‚û° Sequential streams are faster

		üéØ Interview line: ‚ÄúParallel streams are not automatically faster.‚Äù

	üî¥ 5. Debugging & Observability Issues
			Non-deterministic execution order, Harder stack traces, Harder to reason about failures

4Ô∏è‚É£ When Are Parallel Streams a Good Fit?
	‚úÖ Use Parallel Streams When: Data size is large, Operations are CPU-bound, Tasks are independent
		No shared mutable state, Order doesn‚Äôt matter
		
		üß™ Good Examples: Large array transformations, Numeric computations, CPU-heavy map/filter/reduce

5Ô∏è‚É£ How to Control Parallel Stream Thread Pool
	‚ö† Important Truth (Interview Gold): You cannot directly configure the thread pool used by parallel streams.
	But you have workarounds.

		‚úÖ Option 1: Use Custom ForkJoinPool (Recommended)
					ForkJoinPool pool = new ForkJoinPool(4);

					pool.submit(() ->
						list.parallelStream()
							.map(this::process)
							.forEach(System.out::println)
					).join();

			‚úî Isolates workload, ‚úî Controls parallelism, ‚úî Prevents commonPool starvation

		üéØ Interview line: ‚ÄúTo control parallel streams, run them inside a custom ForkJoinPool.‚Äù

		‚ùå Option 2: JVM Property (Global, Not Recommended)
			-Djava.util.concurrent.ForkJoinPool.common.parallelism=4
			Problems: Affects entire JVM, Risky in shared environments, Hard to tune per workload

6Ô∏è‚É£ Parallel Streams vs ExecutorService (Important Comparison)
| Aspect         | Parallel Stream | ExecutorService |
| -------------- | --------------- | --------------- |
| Thread pool    | commonPool      | Custom          |
| Control        | ‚ùå Low           | ‚úÖ High          |
| Best for       | CPU-bound       | I/O or mixed    |
| Error handling | Poor            | Better          |
| Debugging      | Hard            | Easier          |

=============================================================================================================
üîπ Q67. How does JVM handle thread stack? What is stack overflow in multithreading?
=============================================================================================================
1Ô∏è‚É£ How Does JVM Handle Thread Stack?
	üîπ Per-Thread Stack (Very Important): üëâ Each Java thread has its own private stack.
		Stack is not shared, Created when thread is created, Destroyed when thread terminates
			Thread-1 ‚Üí Stack-1
			Thread-2 ‚Üí Stack-2
			Thread-3 ‚Üí Stack-3
		üéØ Interview line: ‚ÄúStack memory is thread-confined.‚Äù

2Ô∏è‚É£ What Is Stored in the Thread Stack? 	Each thread stack consists of stack frames, one per method call.
	üîπ Each Stack Frame Contains: 
		Local variables, Method parameters, Operand stack, Return address, Reference to constant pool
			void a() {
				int x = 10;
				b();
			}

			void b() {
				int y = 20;
			}
		Call sequence:
					Stack:
					| b() frame |
					| a() frame |

3Ô∏è‚É£ Stack Size in JVM: Stack size is fixed per thread
	Configurable using JVM option: -Xss512k, -Xss1m
	Default: Depends on OS + JVM, Typically 512 KB ‚Äì 1 MB per thread

	üéØ Interview line: ‚ÄúStack size limits recursion depth and local variable usage.‚Äù

4Ô∏è‚É£ Why Stack Is Important in Multithreading
	Because: Every thread consumes its own stack memory, Too many threads ‚Üí huge memory consumption
	Example: 1000 threads √ó 1MB stack = 1GB memory
	‚ùå JVM crash, ‚ùå OutOfMemoryError

5Ô∏è‚É£ What Is StackOverflowError? StackOverflowError occurs when a thread exhausts its stack space.
	Common causes: Infinite recursion, Deep recursion, Large local variables, Excessive method calls
	üß™ Simple Example
			void recursive() {
				recursive();
			}
		‚û° Each call adds a new stack frame, ‚û° Stack fills up, ‚û° StackOverflowError

6Ô∏è‚É£ StackOverflow in Multithreading (Key Question)
	üî• Important Clarification: 
		StackOverflowError is per-thread, not global.
		One thread overflowing stack does NOT affect others
		Each thread crashes independently
		
		üß† Example Scenario
					new Thread(() -> recursive()).start();
					new Thread(() -> doWork()).start();

			Thread-1 ‚Üí StackOverflowError
			Thread-2 ‚Üí Continues normally

		üéØ Interview line: ‚ÄúStack overflow affects only the failing thread.‚Äù

7Ô∏è‚É£ Multithreading + Stack Overflow = Hidden Danger
	üî¥ Problem 1: Too Many Threads
				Even without recursion: Creating thousands of threads, Each thread needs stack memory
				‚û° JVM may throw: java.lang.OutOfMemoryError: unable to create new native thread
	
	üî¥ Problem 2: Recursion + Thread Pools
				Recursive logic inside: Executor threads, ForkJoinPool tasks
				‚û° Can kill worker threads, ‚û° Reduce pool capacity, ‚û° Throughput degradation

8Ô∏è‚É£ Stack vs Heap in Multithreading (Interview Favorite)
| Aspect        | Stack              | Heap                  |
| ------------- | ------------------ | --------------------- |
| Shared?       | ‚ùå No               | ‚úÖ Yes                 |
| Thread safety | Inherent           | Needs synchronization |
| Stores        | Local variables    | Objects               |
| Lifetime      | Method scope       | Until GC              |
| Error         | StackOverflowError | OutOfMemoryError      |

9Ô∏è‚É£ How JVM Protects Stack Memory
	Stack is: Fixed size, Bounds-checked
	JVM throws: StackOverflowError, No silent corruption

1Ô∏è‚É£ How Many Threads Can a JVM Create?
There is no fixed number. The maximum number of threads depends on OS limits, available memory, and JVM configuration (especially stack size).

=============================================================================================================
üîπ Q69. Difference between concurrency and parallelism. What is safepoint?
=============================================================================================================
1Ô∏è‚É£ Difference Between Concurrency and Parallelism
	üîπ Concurrency is about structuring a program to handle multiple tasks that overlap in time, even if they are not 	executing at the same instant.
		üëâ It‚Äôs about correctness, coordination, and responsiveness.
		
		üß† Key Idea: Tasks make progress together, Execution may be interleaved, Can exist on single-core CPU
			Task A ‚Üí pause ‚Üí Task B ‚Üí pause ‚Üí Task A
		
		üß™ Example: Web server handling thousands of requests, Each request waits for I/O, Threads/tasks are interleaved
		üéØ Interview line: ‚ÄúConcurrency is about managing multiple tasks, not executing them simultaneously.‚Äù

	üîπ Parallelism is about executing multiple tasks simultaneously using multiple CPU cores.
		üëâ It‚Äôs about performance and throughput.

		üß† Key Idea: Tasks run at the same time, Requires multi-core hardware
					Core 1 ‚Üí Task A
					Core 2 ‚Üí Task B
					Core 3 ‚Üí Task C
		
		üß™ Example: Parallel streams, ForkJoinPool,Matrix multiplication
		
		üéØ Interview line: ‚ÄúParallelism is about doing more work faster.‚Äù

üî• Concurrency vs Parallelism ‚Äî Side by Side
| Aspect              | Concurrency              | Parallelism                |
| ------------------- | ------------------------ | -------------------------- |
| Focus               | Structure & coordination | Execution speed            |
| Execution           | Interleaved              | Simultaneous               |
| Requires multi-core | ‚ùå No                     | ‚úÖ Yes                      |
| Goal                | Responsiveness           | Throughput                 |
| Example             | Web server               | Data processing            |
| Java tools          | Locks, executors         | ForkJoin, parallel streams |

üß† Important Insight (Interview Gold): Concurrency does not imply parallelism, but parallelism implies concurrency.

2Ô∏è‚É£ What Is a Safepoint? Now the JVM internals part.
	A safepoint is a state where all Java threads are paused (or at a known safe state) so that the JVM can perform global operations safely.
	üëâ At a safepoint, the JVM knows: Where every thread is, What objects they reference
		That no thread is modifying critical structures

3Ô∏è‚É£ Why Does JVM Need Safepoints?
	Some JVM operations cannot run concurrently with application threads:
	üî• Common Safepoint Operations:
			Garbage Collection (especially Stop-The-World GC)
			Deoptimization, Class redefinition (HotSwap), Biased lock revocation, Thread stack inspection
	
	üéØ Interview line: ‚ÄúSafepoints allow the JVM to reach a globally consistent state.‚Äù

4Ô∏è‚É£ How Safepoints Work Internally
	üîπ Key Mechanism: 
		JVM inserts safepoint checks at specific locations:
			Method calls, Loop back edges, Return points
				while (true) {
					work(); // safepoint check here
				}
			When a safepoint is requested: Threads run until they reach a safepoint, Then they pause

	üß† Important Detail
		Threads are not stopped at arbitrary instructions.
		They stop only at well-defined safe locations.

5Ô∏è‚É£ What Happens at a Safepoint?
	All application threads: Are paused, OR Are in a known safe state, JVM performs the operation, Threads are resumed
	This is often called: Stop-The-World (STW)

6Ô∏è‚É£ Safepoint vs Blocking (Important Distinction)
	| Safepoint           | Blocking                 |
	| ------------------- | ------------------------ |
	| JVM-initiated       | Application-initiated    |
	| Affects all threads | Affects specific threads |
	| For JVM internals   | For synchronization      |
	| Example: GC         | Example: synchronized    |

	üéØ Interview line: ‚ÄúSafepoints are JVM pauses, not application locks.‚Äù

7Ô∏è‚É£ Performance Impact of Safepoints
	Frequent safepoints ‚Üí latency spikes
	Long GC safepoints ‚Üí application pauses
	Tight loops without safepoint checks ‚Üí safepoint bias problems
	Modern JVMs optimize this heavily.

=============================================================================================================
üîπ Q70. How does GC interact with running threads? Can GC stop application threads?
=============================================================================================================
1Ô∏è‚É£ How Does GC Interact with Running Threads?
	Yes, the JVM can stop application threads for garbage collection ‚Äî but how and when depends on the GC algorithm.
	GC and application threads coexist, but sometimes GC needs exclusive access to the heap.

2Ô∏è‚É£ Key Concept: Safepoint
	GC never stops threads randomly.
	üëâ JVM brings all application threads to a safepoint.
		At a safepoint: Thread state is well-defined, Object references are known, No heap mutation happens

	üéØ Interview line: ‚ÄúGC pauses threads only at safepoints, not at arbitrary instructions.‚Äù

3Ô∏è‚É£ Can GC Stop Application Threads?
	‚úÖ Yes ‚Äî This Is Called Stop-The-World (STW)
	During STW: All application (mutator) threads are paused, GC threads run, Application resumes afterward
	This is required for: Heap consistency, Correct object reachability, Compacting memory

4Ô∏è‚É£ When Does GC Stop Threads?
	üî¥ Always STW Phases (All GCs)
		Some GC phases must stop all threads: 
			Initial root marking, Final remark, Heap compaction, Class unloading, Deoptimization
			Even ‚Äúconcurrent‚Äù collectors have short STW pauses.
	
		üéØ Interview line: ‚ÄúNo GC is 100% pause-free.‚Äù

5Ô∏è‚É£ Types of GC vs Thread Interaction
	üîπ 1. Serial GC: Single GC thread, Long STW pauses, All app threads stopped
		‚ùå Not for production servers
	
	üîπ 2. Parallel GC: Multiple GC threads, Still fully STW, Faster than Serial, but pauses scale with heap size
	
	üîπ 3. CMS (Concurrent Mark Sweep) (legacy): Most work done concurrently
			Still has: Initial Mark ‚Üí STW, Remark ‚Üí STW, Shorter pauses, but more complex.
	
	üîπ 4. G1 GC: Region-based, Mostly concurrent, Predictable, short STW pauses, Designed for low latency
	
	üîπ 5. ZGC / Shenandoah: 
		Ultra-low latency GCs, Do almost everything concurrently
		STW pauses are very short (milliseconds), Heap size independent pauses

	üéØ Interview line: ‚ÄúModern GCs minimize pause time, not eliminate it.‚Äù

6Ô∏è‚É£ What Happens to Threads During STW?
	When GC is triggered:
		1Ô∏è‚É£ JVM requests safepoint
		2Ô∏è‚É£ All application threads: Finish current instruction, Reach a safepoint, Pause execution
		3Ô∏è‚É£ GC threads run
		4Ô∏è‚É£ Threads are resumed
		
	Important: Threads do not lose stack data, Execution continues normally afterward

7Ô∏è‚É£ Do All Threads Stop?
	üîπ What Stops: Application (mutator) threads
	üîπ What Keeps Running: GC threads, JVM internal threads (limited)

8Ô∏è‚É£ Why GC Must Stop Threads (Sometimes)
	Because without stopping: Objects could move while being accessed, References could become invalid, Heap could become inconsistent
	
	üéØ Interview line: ‚ÄúStopping threads ensures a consistent view of the heap.‚Äù

9Ô∏è‚É£ Performance Impact of GC Pauses
	Effects of long STW: Latency spikes, Request timeouts, ‚ÄúApplication freeze‚Äù, SLA violations
	That‚Äôs why: Low-latency systems prefer G1 / ZGC, Heap tuning matters
	
=============================================================================================================
üîπ Q71. ThreadLocal ‚Äì how it works internally? Memory leak with ThreadLocal?
=============================================================================================================
1Ô∏è‚É£ What is ThreadLocal?
	ThreadLocal<T> provides thread-confined variables ‚Äî each thread has its own independent copy of the variable.
		ThreadLocal<Integer> tl = new ThreadLocal<>();
		tl.set(10);
		Integer value = tl.get();
		
	üëâ Same ThreadLocal object, different value per thread.
	üéØ Interview line: ‚ÄúThreadLocal gives per-thread state without synchronization.‚Äù

2Ô∏è‚É£ How ThreadLocal Works Internally (Very Important)
	This is where most people fail interviews.
	üîë Key Insight: ThreadLocal values are NOT stored inside ThreadLocal. They are stored inside the Thread object.

3Ô∏è‚É£ Internal Structure
	Each Thread has a field:
		ThreadLocal.ThreadLocalMap threadLocals;
		Structure:
					Thread
					 ‚îî‚îÄ‚îÄ ThreadLocalMap
						  ‚îú‚îÄ‚îÄ Entry (key = ThreadLocal, value = Object)
						  ‚îú‚îÄ‚îÄ Entry (key = ThreadLocal, value = Object)
						  ‚îî‚îÄ‚îÄ ...

4Ô∏è‚É£ ThreadLocalMap Entry (Critical Detail)
		static class Entry extends WeakReference<ThreadLocal<?>> {
			Object value;
		}

	‚ö† Important: Key ‚Üí ThreadLocal (WeakReference), Value ‚Üí Actual object (Strong reference)
	üéØ Interview line (very important): ‚ÄúThreadLocalMap uses weak keys but strong values.‚Äù

5Ô∏è‚É£ Step-by-Step: set() / get()
	üîπ set(value): Get current thread, Get thread.threadLocals, Put (ThreadLocal ‚Üí value) into map
	
	üîπ get(): Get current thread, Look up current ThreadLocal key, Return associated value

6Ô∏è‚É£ Why ThreadLocal Is Fast & Thread-Safe
	No locking, No contention, Data is thread-confined, Map is accessed by only one thread
	
	üéØ Interview line: ‚ÄúThreadLocal achieves thread safety through isolation, not synchronization.‚Äù

7Ô∏è‚É£ Memory Leak with ThreadLocal (Most Important Part)
	‚ùó Yes ‚Äî ThreadLocal can cause memory leaks, especially with thread pools.
	üî• How the Leak Happens
		Scenario:
				ThreadLocal<User> tl = new ThreadLocal<>();
				tl.set(new User()); // large object
		Later:
				tl = null; // ThreadLocal reference lost

		What happens internally?
			Key (ThreadLocal) ‚Üí GCed (weak reference)
			Value (User) ‚Üí ‚ùå NOT GCed
			Entry remains in ThreadLocalMap with:
				key = null
				value = User (strong reference)
			‚û° Memory leak

8Ô∏è‚É£ Why This Is Worse with Thread Pools
	Thread pools: Threads live for a long time, ThreadLocalMap lives as long as thread lives
	So: Request finishes, Thread reused, Old ThreadLocal values still there
	
	üéØ Interview line: ‚ÄúThreadLocal leaks are dangerous in thread pools because threads don‚Äôt die.‚Äù

9Ô∏è‚É£ When Does Cleanup Happen?
	ThreadLocalMap cleans entries: During get(), set(), remove() NOT automatically on GC
	If you never touch ThreadLocal again ‚Üí leak persists.

	üî¥ Real Production Example: Web application
		ThreadLocal stores: User context, Security context, DB connection, Thread pool reused, Missing remove()
		Heap grows slowly ‚Üí OutOfMemoryError

üîü How to Prevent ThreadLocal Memory Leaks
	‚úÖ Always call remove()
						try {
							threadLocal.set(value);
							// use it
						} finally {
							threadLocal.remove();
						}

	‚úÖ Use frameworks carefully: Spring Security cleans context, MDC logging cleans ThreadLocal
	
	‚ùå Do NOT store: Large objects, DB connections, Request-scoped data without cleanup

11Ô∏è‚É£ ThreadLocal vs Shared Variable (Quick Contrast)
| Aspect            | ThreadLocal  | Shared Variable |
| ----------------- | ------------ | --------------- |
| Synchronization   | ‚ùå Not needed | ‚úÖ Required      |
| Visibility issues | ‚ùå No         | ‚úÖ Yes           |
| Memory risk       | ‚ö† Leak risk  | ‚ùå No            |
| Best for          | Context data | Shared state    |


=============================================================================================================
üîπ Q72. Design a producer-consumer system.
=============================================================================================================
3Ô∏è‚É£ Correct Java Design (Recommended Solution)
‚úÖ Using BlockingQueue (Best Practice)
üîπ Data Model
			class Task {
				private final int id;

				Task(int id) {
					this.id = id;
				}

				int getId() {
					return id;
				}
			}
üîπ Producer
			class Producer implements Runnable {

				private final BlockingQueue<Task> queue;
				private static int counter = 0;

				Producer(BlockingQueue<Task> queue) {
					this.queue = queue;
				}

				@Override
				public void run() {
					try {
						while (!Thread.currentThread().isInterrupted()) {
							Task task = new Task(counter++);
							queue.put(task); // blocks if queue is full
							System.out.println("Produced: " + task.getId());
							Thread.sleep(500);
						}
					} catch (InterruptedException e) {
						Thread.currentThread().interrupt();
					}
				}
			}
üîπ Consumer
			class Consumer implements Runnable {
				private final BlockingQueue<Task> queue;

				Consumer(BlockingQueue<Task> queue) {
					this.queue = queue;
				}

				@Override
				public void run() {
					try {
						while (!Thread.currentThread().isInterrupted()) {
							Task task = queue.take(); // blocks if queue empty
							System.out.println("Consumed: " + task.getId());
							Thread.sleep(1000);
						}
					} catch (InterruptedException e) {
						Thread.currentThread().interrupt();
					}
				}
			}
üîπ Main Driver
		public class ProducerConsumerApp {
			public static void main(String[] args) throws InterruptedException {

				BlockingQueue<Task> queue = new ArrayBlockingQueue<>(5);

				ExecutorService executor = Executors.newFixedThreadPool(4);

				executor.submit(new Producer(queue));
				executor.submit(new Producer(queue));

				executor.submit(new Consumer(queue));
				executor.submit(new Consumer(queue));

				Thread.sleep(10000); // run system

				executor.shutdownNow(); // graceful shutdown
			}
		}

4Ô∏è‚É£ Why This Design Is Correct (Explain This in Interview)
	‚úÖ Thread Safety: BlockingQueue handles synchronization internally, No explicit locks needed
	‚úÖ Back-Pressure: Queue is bounded, Producers block when queue is full
	‚úÖ No Busy Waiting: put() and take() block efficiently
	‚úÖ Scalable: Add producers/consumers easily, Tune queue size independently

5Ô∏è‚É£ Execution Flow (Mental Model)
Producer ---> put() ---> [ BlockingQueue ] ---> take() ---> Consumer
              (blocks if full)               (blocks if empty)


üéØ Interview line: ‚ÄúBlockingQueue gives natural flow control between producers and consumers.‚Äù

=============================================================================================================
üîπ Q73. Implement rate limiter using Java concurrency.
=============================================================================================================
1Ô∏è‚É£  A rate limiter controls how many requests/actions are allowed per unit of time, protecting systems from overload.
	Examples: 
		100 requests / second per user
		1000 API calls / minute
		10 DB writes / second

2Ô∏è‚É£ Which Algorithm to Use?
	Token Bucket is the best choice because:Simple, Smooth traffic, Allows bursts, Easy to implement using Java concurrency

ü™£ Token Bucket Algorithm (Quick)
	Bucket has max capacity, Tokens are added at a fixed rate, Each request consumes 1 token
	If no token ‚Üí request rejected

3Ô∏è‚É£ Token Bucket ‚Äì Java Implementation (Recommended)
	‚úÖ Thread-safe, ‚úÖ Non-blocking, ‚úÖ Production-ready
	üîπ Core Design Choices
			AtomicLong ‚Üí thread-safe token count
			ScheduledExecutorService ‚Üí refill tokens
			No synchronized blocks
			Fast under high concurrency

üîπ Implementation
					import java.util.concurrent.*;
					import java.util.concurrent.atomic.AtomicLong;

					public class RateLimiter {

						private final long capacity;
						private final long refillRate; // tokens per second
						private final AtomicLong tokens;

						private final ScheduledExecutorService scheduler =
								Executors.newSingleThreadScheduledExecutor();

						public RateLimiter(long capacity, long refillRate) {
							this.capacity = capacity;
							this.refillRate = refillRate;
							this.tokens = new AtomicLong(capacity);

							startRefill();
						}

						private void startRefill() {
							scheduler.scheduleAtFixedRate(() -> {
								long current;
								long updated;
								do {
									current = tokens.get();
									updated = Math.min(capacity, current + refillRate);
								} while (!tokens.compareAndSet(current, updated));
							}, 1, 1, TimeUnit.SECONDS);
						}

						public boolean allowRequest() {
							long current;
							do {
								current = tokens.get();
								if (current == 0) {
									return false;
								}
							} while (!tokens.compareAndSet(current, current - 1));
							return true;
						}

						public void shutdown() {
							scheduler.shutdown();
						}
					}

üîπ Usage Example
				public class RateLimiterTest {

					public static void main(String[] args) throws InterruptedException {

						RateLimiter limiter = new RateLimiter(5, 2); // 5 max, 2/sec

						ExecutorService executor = Executors.newFixedThreadPool(10);

						for (int i = 0; i < 20; i++) {
							executor.submit(() -> {
								if (limiter.allowRequest()) {
									System.out.println("Request allowed");
								} else {
									System.out.println("Request rejected");
								}
							});
							Thread.sleep(200);
						}

						executor.shutdown();
						limiter.shutdown();
					}
				}

5Ô∏è‚É£ Alternative Implementations (Interview Bonus)
	üîπ Using Semaphore (Simpler but Less Precise)
				Semaphore semaphore = new Semaphore(10);

				boolean allow() {
					return semaphore.tryAcquire();
				}
		‚ùå No smooth refill, Less control
	
	üîπ Fixed Window Counter (Not Recommended)
			Counter resets every second, Causes burst at window boundary, Easy but inaccurate
	
	üîπ Sliding Window (More Complex): More accurate, Requires timestamps, Higher memory & CPU

=============================================================================================================
üîπ Q74. Design a thread-safe cache. How to ensure ordering of tasks?
=============================================================================================================
PART 1Ô∏è‚É£ Design a Thread-Safe Cache
	1Ô∏è‚É£ Core Requirements (Say This First)
		A good cache should provide: Thread safety, Fast reads, Controlled writes, Optional eviction (LRU / TTL)
		No corruption under concurrency
	
	2Ô∏è‚É£ Best Default Choice in Java: ConcurrentHashMap + proper coordination
		Why? Lock-free reads, Fine-grained locking for writes, Scales under high concurrency

	3Ô∏è‚É£ Simple Thread-Safe Cache (No Eviction)
		‚úÖ Basic Design
						class Cache<K, V> {

							private final ConcurrentHashMap<K, V> map = new ConcurrentHashMap<>();

							public V get(K key) {
								return map.get(key);
							}

							public void put(K key, V value) {
								map.put(key, value);
							}

							public V computeIfAbsent(K key, Function<K, V> loader) {
								return map.computeIfAbsent(key, loader);
							}
						}

		üî• Interview line: ‚ÄúConcurrentHashMap provides thread safety without global locking.‚Äù

	4Ô∏è‚É£ Prevent Cache Stampede (Very Important)
		‚ùå Problem: Multiple threads miss the cache and load the same value repeatedly.
		‚úÖ Solution: computeIfAbsent
					V value = cache.computeIfAbsent(key, k -> loadFromDB(k));
		
		‚úî Loader called only once, ‚úî Other threads wait for result
		üéØ Interview line: ‚ÄúcomputeIfAbsent prevents cache stampede.‚Äù

	5Ô∏è‚É£ Thread-Safe Cache with LRU Eviction
		‚ùå Why plain ConcurrentHashMap is not enough: No ordering, No eviction
		‚úÖ Option 1: Synchronized LRU (Simple, but limited)
					class LRUCache<K, V> {

						private final Map<K, V> cache =
							Collections.synchronizedMap(
								new LinkedHashMap<>(16, 0.75f, true) {
									protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
										return size() > 100;
									}
								}
							);
					}

				‚úî Maintains access order, ‚ùå Global lock, ‚ùå Not great for high concurrency

		‚úÖ Option 2: Production-Grade (Recommended)
			Use Caffeine / Guava Cache
			Why? Lock-free reads, Window-TinyLFU eviction, Time-based expiration, Async loading
			üéØ Interview line: ‚ÄúIn production, use Caffeine instead of reinventing cache.‚Äù

		6Ô∏è‚É£ Summary: Cache Design Choices
			| Use Case         | Recommended             |
			| ---------------- | ----------------------- |
			| Simple cache     | ConcurrentHashMap       |
			| Avoid stampede   | computeIfAbsent         |
			| LRU / TTL        | Caffeine                |
			| High concurrency | ConcurrentHashMap-based |
			| Strong ordering  | Separate mechanism      |

PART 2Ô∏è‚É£ How to Ensure Ordering of Tasks
 This is often the hidden second part of the question.
	7Ô∏è‚É£ What Does ‚ÄúOrdering‚Äù Mean? 
		Tasks must execute: In submission order, Or per key (same key ‚Üí ordered), Or globally ordered

	8Ô∏è‚É£ Global Ordering of Tasks
		‚úÖ Best Solution: Single-Thread Executor
				ExecutorService executor =
						Executors.newSingleThreadExecutor();

				executor.submit(task1);
				executor.submit(task2);


		‚úî Tasks execute one by one, ‚úî Strict FIFO ordering
		üéØ Interview line: ‚ÄúSingleThreadExecutor guarantees ordering.‚Äù
	
	9Ô∏è‚É£ Ordering Per Key (Very Important Real-World Case)
		üîπ Example: Cache updates per user, Same user ‚Üí ordered, Different users ‚Üí parallel

			‚úÖ Design: Key-based executors
						class OrderedExecutor {

							private final int stripes = 16;
							private final ExecutorService[] executors;

							OrderedExecutor() {
								executors = new ExecutorService[stripes];
								for (int i = 0; i < stripes; i++) {
									executors[i] = Executors.newSingleThreadExecutor();
								}
							}

							public void submit(Object key, Runnable task) {
								int index = Math.abs(key.hashCode() % stripes);
								executors[index].submit(task);
							}
						}

			‚úî Same key ‚Üí same executor ‚Üí ordered, ‚úî Different keys ‚Üí parallel

			üéØ Interview line: ‚ÄúStriped executors preserve per-key ordering with concurrency.‚Äù

	üîü Ordering Inside Cache Updates
			Example: Ordered cache writes
						executor.submit(() -> cache.put(key, value));
			Guarantees: No race condition, Deterministic ordering

	11Ô∏è‚É£ When NOT to Use Locks for Ordering ‚ùå
		‚ùå Using synchronized for ordering, ‚ùå Busy waiting, ‚ùå Sleeping threads
		Ordering is an execution concern, not a locking concern.
	
=============================================================================================================
üîπ Q75. How to limit concurrent API calls?
=============================================================================================================
1Ô∏è‚É£ What Does ‚ÄúLimit Concurrent API Calls‚Äù Mean?
	It means: Only N API calls should be executing at the same time, (extra calls must wait, queue, or fail fast)
	‚ö† This is NOT the same as rate limiting (X requests per second).
	üî• Key distinction (say this in interview)
		Concurrency limit ‚Üí how many calls run at once
		Rate limit ‚Üí how many calls per time window

2Ô∏è‚É£ Best & Simplest Solution: Semaphore
	‚úÖ Semaphore is MADE for this problem
	üîπ How Semaphore Works: Maintains a fixed number of permits
		Each API call: acquires a permit, executes, releases permit
	
	‚úÖ Implementation (Interview-Ready)
				import java.util.concurrent.Semaphore;

				public class ApiLimiter {

					private final Semaphore semaphore;

					public ApiLimiter(int maxConcurrentCalls) {
						this.semaphore = new Semaphore(maxConcurrentCalls);
					}

					public void callApi(Runnable apiCall) {
						try {
							semaphore.acquire();   // blocks if limit reached
							apiCall.run();
						} catch (InterruptedException e) {
							Thread.currentThread().interrupt();
						} finally {
							semaphore.release();
						}
					}
				}
	
	üîπ Usage
				ApiLimiter limiter = new ApiLimiter(5); // max 5 concurrent calls

				limiter.callApi(() -> {
					System.out.println("Calling API");
				});

	üéØ Interview line: ‚ÄúSemaphore is the cleanest way to cap concurrent API calls.‚Äù

3Ô∏è‚É£ Fail-Fast Instead of Blocking (Very Important Variant)
	Sometimes you don‚Äôt want to block threads.
	‚úÖ Use tryAcquire()
				public boolean callApiIfAllowed(Runnable apiCall) {
					if (!semaphore.tryAcquire()) {
						return false; // reject immediately
					}
					try {
						apiCall.run();
						return true;
					} finally {
						semaphore.release();
					}
				}

		‚úî Fast failure, ‚úî Protects system under overload
	
	üéØ Interview line: ‚ÄúFail-fast is better than blocking in high-load systems.‚Äù

4Ô∏è‚É£ Using Thread Pool to Limit Concurrency
	Another very common and valid approach.
	‚úÖ Fixed Thread Pool
				ExecutorService executor =
						Executors.newFixedThreadPool(5);

				executor.submit(() -> callApi());

		‚úî At most 5 API calls at once, ‚úî Others are queued
		‚ö† Drawback: Queue can grow unbounded, No easy rejection unless customized

	‚úÖ Better: Bounded Queue + Rejection
				ThreadPoolExecutor executor =
					new ThreadPoolExecutor(
						5, 5,
						0, TimeUnit.MILLISECONDS,
						new ArrayBlockingQueue<>(50),
						new ThreadPoolExecutor.AbortPolicy()
					);

		‚úî Limits concurrency, ‚úî Limits queue, ‚úî Rejects overload
		üéØ Interview line: ‚ÄúThread pool + bounded queue gives concurrency + back-pressure.‚Äù

5Ô∏è‚É£ Semaphore vs Thread Pool (Important Comparison)
	| Aspect    | Semaphore          | Thread Pool    |
	| --------- | ------------------ | -------------- |
	| Controls  | Concurrency        | Execution      |
	| Blocking  | Optional           | Usually        |
	| Fail fast | Easy               | Harder         |
	| Queue     | ‚ùå No               | ‚úÖ Yes          |
	| Use case  | External API calls | Task execution |

	üéØ Interview insight: ‚ÄúUse Semaphore when threads already exist; use thread pool to manage threads.‚Äù

6Ô∏è‚É£ Combining Rate Limiting + Concurrency Limiting (Senior Answer)
	Real systems usually need both.
	Example: Max 5 concurrent calls, Max 100 calls per minute
	Solution: Semaphore ‚Üí concurrency, Token Bucket ‚Üí rate
	üéØ Interview line: ‚ÄúConcurrency protects resources; rate limiting protects fairness.‚Äù

7Ô∏è‚É£ Reactive / Async Systems (Bonus)
	If API calls are async / non-blocking: Semaphore still works
	Or use: Reactive back-pressure, Bounded schedulers
But for Java interviews ‚Üí Semaphore wins.

=============================================================================================================
üîπ Q76. How to handle back-pressure?
=============================================================================================================
1Ô∏è‚É£  Back-pressure is a mechanism by which a system slows down producers when consumers cannot keep up.
	In simple words:
		Producer is too fast, Consumer is too slow, System must say: ‚ÄúSlow down or stop‚Äù
	
	üéØ Interview line: ‚ÄúBack-pressure prevents overload by propagating pressure upstream.‚Äù

2Ô∏è‚É£ Why Back-Pressure Is Critical
	Without back-pressure: Memory grows unbounded, Queues explode, Latency spikes, System crashes (OOM)
	Real-world analogy: A restaurant stops accepting orders when the kitchen is full.

3Ô∏è‚É£ Common Back-Pressure Strategies (Very Important)
	1Ô∏è‚É£ Blocking (Slow Down Producer)
		üîπ Using BlockingQueue (Classic)
				BlockingQueue<Task> queue = new ArrayBlockingQueue<>(100);
				queue.put(task); // blocks if queue is full
		
			‚úî Simple, ‚úî Natural throttling ‚ùå Producer threads get blocked

		üéØ Interview line: ‚ÄúBlockingQueue provides built-in back-pressure.‚Äù

	2Ô∏è‚É£ Fail Fast (Reject Requests)
		Instead of blocking, reject immediately.
		üîπ Semaphore example
				if (!semaphore.tryAcquire()) {
					throw new RuntimeException("System overloaded");
				}

			‚úî Protects system, ‚úî Predictable latency, ‚ùå Caller must retry

		üéØ Interview line: ‚ÄúFail-fast is preferred in high-load systems.‚Äù

	3Ô∏è‚É£ Bounded Queues (Most Important)
		Never allow unbounded buffering.
				ThreadPoolExecutor executor =
						new ThreadPoolExecutor(
							10, 10,
							0, TimeUnit.MILLISECONDS,
							new ArrayBlockingQueue<>(100),
							new ThreadPoolExecutor.AbortPolicy()
						);

			‚úî Controls memory, ‚úî Applies back-pressure via rejection
		üéØ Interview line: ‚ÄúUnbounded queues hide problems; bounded queues expose them.‚Äù

	4Ô∏è‚É£ Rate Limiting (Slow Down Input)
		Limit how fast producers can generate work. Token Bucket, Leaky Bucket

					if (!rateLimiter.allow()) {
						return HTTP_429;
					}
			‚úî Prevents bursts, ‚úî Smooth traffic
		üéØ Interview line: ‚ÄúRate limiting is a form of back-pressure.‚Äù
	
	5Ô∏è‚É£ Load Shedding (Drop Work Intentionally)
		When overloaded: Drop low-priority requests, Sample traffic
		Examples: Drop logs, Skip metrics, Reject background tasks
		üéØ Interview line: ‚ÄúDropping work is better than crashing.‚Äù
	
	6Ô∏è‚É£ Caller Runs Policy (Executor Trick)
				new ThreadPoolExecutor(
					5, 5,
					0, TimeUnit.MILLISECONDS,
					new ArrayBlockingQueue<>(50),
					new ThreadPoolExecutor.CallerRunsPolicy()
				);
		What happens: When queue is full, Producer thread executes task itself, Producer slows down naturally
		‚úî Elegant back-pressure ‚úî No data loss
	
		üéØ Interview line: ‚ÄúCallerRunsPolicy pushes back pressure to the caller.‚Äù
	
	4Ô∏è‚É£ Back-Pressure in Different Architectures
		üîπ Synchronous APIs: Block, Reject (HTTP 429)
		üîπ Asynchronous / Event Systems: Bounded queues, Message broker credits, Consumer acknowledgments
		üîπ Reactive Systems: Demand-based flow, Subscriber controls rate
		üéØ Interview line: ‚ÄúReactive systems make back-pressure explicit.‚Äù

6Ô∏è‚É£ Choosing the Right Strategy (Interview Gold)
| Scenario        | Best Strategy    |
| --------------- | ---------------- |
| CPU overload    | Fail fast        |
| I/O overload    | Bounded queue    |
| External API    | Rate limit       |
| Background jobs | Drop / delay     |
| Thread pools    | CallerRunsPolicy |

=============================================================================================================
üîπ Q77. How would you debug high CPU due to threads?
=============================================================================================================
1Ô∏è‚É£ First Clarify the Problem (Always Start Here)
	Before jumping to tools, answer these mentally:
		Is CPU high constantly or spiking?
		Is it one core or all cores?
		Is the app responsive or hung?
		JVM process or system-wide?
	üéØ Interview line: ‚ÄúHigh CPU is a symptom; first I identify the pattern.‚Äù

2Ô∏è‚É£ OS Level: Identify the Hot Thread
	üîπ Step 1: Find the JVM Process: 
				top
				# or
				ps -ef | grep java

				Note the PID.

	üîπ Step 2: Find CPU-Hungry Threads
				top -H -p <PID>
				This shows: Per-thread CPU usage, Thread IDs (TID)
				üëâ Identify threads using high %CPU.

		üéØ Interview line: ‚ÄúI locate the exact threads consuming CPU, not just the process.‚Äù

3Ô∏è‚É£ JVM Level: Take Thread Dumps
	üîπ Step 3: Convert Thread ID
			Thread dump shows thread IDs in hex, OS tools show decimal.
			printf "%x\n" <TID>
	
	üîπ Step 4: Capture Thread Dump
			jstack <PID> > threaddump.txt
			‚ö† Take multiple dumps (2‚Äì3) with a gap of a few seconds.

		üéØ Interview line: ‚ÄúMultiple thread dumps reveal stuck or spinning threads.‚Äù

4Ô∏è‚É£ Analyze Thread Dumps (Most Important)
	Look for threads that: Appear in RUNNABLE state repeatedly, Same stack trace across dumps
	üî• Common CPU-Killer Patterns
		‚ùå 1. Busy Loop (Classic)
				while (true) {
					// no blocking
				}
				Thread dump: java.lang.Thread.State: RUNNABLE
				‚û° Consumes 100% CPU

		‚ùå 2. Spin Waiting / CAS Storm
				while (!flag.compareAndSet(false, true)) {}
				Symptoms: RUNNABLE, Atomic operations, High contention

			üéØ Interview line: ‚ÄúCAS can cause CPU spin under contention.‚Äù

		‚ùå 3. Excessive Synchronization / Lock Contention
			Threads stuck trying to acquire lock: JVM spends CPU managing contention
			Look for: - waiting to lock <0x...>
		
		‚ùå 4. ForkJoinPool / Parallel Streams Abuse
			Parallel streams, Blocking inside them, Common pool starvation
			
			üéØ Interview line: ‚ÄúBlocking ForkJoinPool threads leads to CPU spikes.‚Äù
		
		‚ùå 5. GC Thrashing
			Symptoms: CPU high, Throughput low
			Check GC logs: -XX:+PrintGCDetails
			Look for: Too frequent GC, High GC CPU time

5Ô∏è‚É£ Correlate with JVM Metrics
	üîπ Use JConsole / VisualVM / JMC
		Check: Thread count, Runnable threads, GC activity, Heap pressure
		üéØ Interview line: ‚ÄúCPU spikes often correlate with GC or runnable thread explosion.‚Äù

6Ô∏è‚É£ Production-Safe Commands (Very Important)
	| Tool                | Safe in Prod?         |
	| ------------------- | --------------------- |
	| `top -H`            | ‚úÖ Yes                 |
	| `jstack`            | ‚úÖ Yes                 |
	| `jcmd GC.heap_info` | ‚úÖ Yes                 |
	| `jmap -histo`       | ‚ö† Carefully           |
	| `jmap -dump`        | ‚ùå Avoid unless needed |

7Ô∏è‚É£ Typical Root Causes Summary
	| Cause                   | Symptom                |
	| ----------------------- | ---------------------- |
	| Infinite loop           | Single thread 100% CPU |
	| Busy wait               | Many RUNNABLE threads  |
	| Lock contention         | CPU + low throughput   |
	| Too many threads        | Context switching      |
	| GC pressure             | CPU + latency          |
	| Parallel streams misuse | ForkJoinPool hot       |

8Ô∏è‚É£ How to Fix (After Finding Root Cause)
	Replace busy loops with blocking calls, Add back-pressure, Reduce thread count, Fix synchronization, Avoid blocking ForkJoinPool, Tune GC / heap, Use async APIs
	üéØ Interview line: ‚ÄúFix is always simpler than diagnosis ‚Äî diagnosis is the hard part.‚Äù

=============================================================================================================
üîπ Q78. Real production issue you faced with concurrency.
=============================================================================================================
‚úÖ Sample Answer: Real Production Concurrency Issue
	üî• Problem: High CPU + request timeouts under load
	üìå Context (Set the Scene)
	‚ÄúWe had a Spring Boot microservice handling around 3‚Äì4k requests per minute. During peak hours, the service suddenly started timing out and CPU usage went close to 90‚Äì100%, even though traffic hadn‚Äôt increased significantly.‚Äù

üîç Symptoms Observed
	High CPU usage, Increased latency, Thread pool exhaustion, Requests getting stuck
	No increase in heap usage (important clue)
	
	üéØ Interview line: ‚ÄúThe system was alive but not making progress.‚Äù
	üõ†Ô∏è Investigation Steps
		1Ô∏è‚É£ OS-Level Check
			Used top -H -p <pid>
			Found many threads consuming CPU, Most were ForkJoinPool.commonPool-worker-*
			üö® Red flag.

		2Ô∏è‚É£ Thread Dump Analysis
			Took multiple thread dumps, Found threads stuck in RUNNABLE
			Stack traces showed:
					CompletableFuture.supplyAsync(() -> {
						return restTemplate.callExternalApi(); // blocking call
					});

		‚ùå Root Cause: Blocking I/O was executed inside CompletableFuture without a custom executor, so it ran on the ForkJoinPool.commonPool.
		This caused: 
			ForkJoinPool threads to block, Other async tasks and parallel streams to starve
			CPU spike due to thread contention and retries
		
		üéØ Interview gold line: ‚ÄúWe accidentally blocked the common ForkJoinPool.‚Äù

‚úÖ Fix Implemented
	üîß Step 1: Isolate Blocking Calls
		Created a dedicated I/O thread pool:
			ExecutorService ioExecutor =
					Executors.newFixedThreadPool(20);

	üîß Step 2: Use Custom Executor
			CompletableFuture.supplyAsync(
				() -> restTemplate.callExternalApi(),
				ioExecutor
			);

	üîß Step 3: Add Timeouts & Back-Pressure
		Added timeouts on external calls, Limited concurrency using Semaphore, Monitored queue sizes
	üìà Result: CPU dropped from ~95% to ~40%, Latency stabilized, No more timeouts, System became predictable under load

üß† Lesson Learned (VERY IMPORTANT)
‚ÄúNever run blocking I/O on ForkJoinPool or parallel streams. Always isolate blocking work using dedicated executors and apply back-pressure.‚Äù

=============================================================================================================
üîπ Q79. How do you test multithreaded code?
=============================================================================================================
1Ô∏è‚É£ Why Multithreaded Code Is Hard to Test
	Non-deterministic execution, Race conditions appear rarely, Tests pass locally, fail in CI, Timing-dependent bugs
	
	üéØ Interview line: ‚ÄúConcurrency bugs are probabilistic, not deterministic.‚Äù

2Ô∏è‚É£ Core Principles for Testing Multithreaded Code
	Say these early: Reduce timing dependency, Control thread scheduling, Test invariants, not order, Repeat tests aggressively
	
	üéØ Interview line: ‚ÄúI test behavior and invariants, not execution order.‚Äù

3Ô∏è‚É£ Unit Testing with Controlled Concurrency
	üîπ Use ExecutorService Instead of Raw Threads
			ExecutorService executor = Executors.newFixedThreadPool(10);
		This gives: Controlled concurrency, Deterministic shutdown
	
	üîπ Use CountDownLatch to Coordinate Threads
		Example: Testing race condition
						@Test
						void testConcurrentIncrement() throws InterruptedException {

							AtomicInteger counter = new AtomicInteger();
							int threads = 10;

							CountDownLatch start = new CountDownLatch(1);
							CountDownLatch done = new CountDownLatch(threads);

							for (int i = 0; i < threads; i++) {
								new Thread(() -> {
									try {
										start.await();
										counter.incrementAndGet();
									} catch (InterruptedException ignored) {
									} finally {
										done.countDown();
									}
								}).start();
							}

							start.countDown(); // start all threads
							done.await();

							assertEquals(threads, counter.get());
						}


			üéØ Interview line: ‚ÄúCountDownLatch helps start threads at the same time.‚Äù

4Ô∏è‚É£ Testing Blocking & Waiting Behavior
	üîπ Use CyclicBarrier for Repeated Synchronization
			CyclicBarrier barrier = new CyclicBarrier(5);
		Useful for: Simulating contention, Reusing barrier in loops

5Ô∏è‚É£ Testing Time-Sensitive Code
	‚ùå Avoid Thread.sleep()
	‚úÖ Use timeouts
		assertTrue(latch.await(2, TimeUnit.SECONDS));
	
	üéØ Interview line: ‚ÄúSleeping makes tests flaky; timeouts make them stable.‚Äù

6Ô∏è‚É£ Stress & Repetition Testing (Very Important)
	üîπ Repeat Tests
					@RepeatedTest(1000)
					void concurrentTest() {
						// test logic
					}
		Why? Race conditions surface statistically
		
		üéØ Interview line: ‚ÄúConcurrency bugs appear under repetition, not single runs.‚Äù

7Ô∏è‚É£ Testing Thread Safety Invariants
	Test what must always be true, not timing: No data corruption, Correct final state, No lost updates, No deadlock
	Example:
		assertTrue(balance >= 0);

8Ô∏è‚É£ Testing Deadlocks
	üîπ Use Timeouts
				assertTimeout(Duration.ofSeconds(2), () -> {
					runConcurrentCode();
				});
		If it hangs ‚Üí test fails.
	
9Ô∏è‚É£ Integration & Load Testing
	Unit tests are not enough.
	üîπ Use: Load tests, Stress tests, Chaos testing, Production-like environments
	
	üéØ Interview line: ‚ÄúMost concurrency bugs escape unit tests.‚Äù

üîü Specialized Tools (Bonus Points)
	Mention these if interviewer is senior:
		JCStress (official JVM concurrency stress tool)
		ThreadSanitizer (native code)
		JMH (performance + concurrency)
		Java Flight Recorder (JFR)
	
	üéØ Interview line: ‚ÄúJCStress tests correctness under all possible interleavings.‚Äù
	
=============================================================================================================
üîπ Q80. How to make singleton thread safe?
=============================================================================================================
1Ô∏è‚É£ Option 3: Double-Checked Locking (Correct Only in Java 5+)
	‚úÖ This is VERY IMPORTANT
			class Singleton {

				private static volatile Singleton instance;

				public static Singleton getInstance() {
					if (instance == null) {               // first check (no lock)
						synchronized (Singleton.class) {
							if (instance == null) {       // second check (with lock)
								instance = new Singleton();
							}
						}
					}
					return instance;
				}
			}

			üî• Why volatile is required
				Without volatile, JVM may reorder: Allocate memory, Assign reference, Initialize object
				Another thread may see a half-constructed object.
			
			üéØ Interview line: ‚ÄúDouble-checked locking is broken without volatile.‚Äù
			‚úÖ Pros: Lazy initialization, High performance, Thread-safe
			‚ùå Cons: Complex, Easy to get wrong

2Ô∏è‚É£ Option 4: Eager Initialization (Simple & Safe)
		class Singleton {
			private static final Singleton INSTANCE = new Singleton();

			public static Singleton getInstance() {
				return INSTANCE;
			}
		}

		‚úÖ Pros: Thread-safe (class loading guarantees),Simple, No synchronization
		‚ùå Cons: Instance created even if never used
		üéØ Interview line: ‚ÄúClass loading guarantees thread safety.‚Äù

3Ô∏è‚É£ ‚≠ê BEST OPTION: Initialization-on-Demand Holder Idiom: üëâ Most recommended for interviews
		
		class Singleton {

			private Singleton() {}

			private static class Holder {
				private static final Singleton INSTANCE = new Singleton();
			}

			public static Singleton getInstance() {
				return Holder.INSTANCE;
			}
		}
		
		üî• Why this is best: Lazy initialization, No synchronization, Thread-safe, Clean & elegant
		How? JVM loads Holder only when accessed, Class loading is thread-safe
		üéØ Interview gold line: ‚ÄúThis uses JVM class-loading guarantees.‚Äù

4‚É£ ‚≠ê BEST & SIMPLEST: Enum Singleton (Effective Java)
		enum Singleton {
			INSTANCE;
		}

		‚úÖ Pros: Thread-safe, Prevents reflection attack, Prevents serialization issues, Simplest
		‚ùå Cons: Not flexible (no lazy params), Some teams avoid enums
		üéØ Interview line: ‚ÄúEnum singleton is the safest singleton.‚Äù

5Ô∏è‚É£ Comparison Table (Interview Cheat Sheet)
| Approach               | Thread Safe | Lazy | Performance | Recommended |
| ---------------------- | ----------- | ---- | ----------- | ----------- |
| Synchronized method    | ‚úÖ           | ‚úÖ    | ‚ùå           | ‚ùå           |
| Double-checked locking | ‚úÖ           | ‚úÖ    | ‚úÖ           | ‚ö†           |
| Eager initialization   | ‚úÖ           | ‚ùå    | ‚úÖ           | ‚úÖ           |
| Holder idiom           | ‚úÖ           | ‚úÖ    | ‚úÖ           | ‚≠ê‚≠ê‚≠ê         |
| Enum                   | ‚úÖ           | ‚ùå    | ‚úÖ           | ‚≠ê‚≠ê‚≠ê         |

6‚É£ Serialization & Reflection (Senior-Level Follow-up)
	üî¥ Serialization breaks singleton
			Fix:
				protected Object readResolve() {
					return INSTANCE;
				}

	üî¥ Reflection breaks private constructor
		Enum avoids this completely.




