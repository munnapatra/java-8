=============================================================================================================
ğŸ”¹ Q1. What is the Java Collection Framework? Why was it introduced?
=============================================================================================================
âœ… The Java Collection Framework is a unified architecture in Java that provides interfaces, implementations, and algorithms to store and manipulate groups of objects.
It is part of the java.util package and includes:
	Interfaces â†’ List, Set, Queue, Map
	Implementations â†’ ArrayList, HashSet, HashMap, TreeMap, etc.
	Utility Algorithms â†’ sorting, searching, synchronization (via Collections class)

âœ… Why Was Java Collection Framework Introduced?
	Before JCF (pre-Java 1.2), Java had: Vector, Stack, Hashtable, Enumeration
	These were: Inconsistent, Not part of a unified hierarchy, Mostly synchronized (performance overhead), Lacked common interfaces
	
	ğŸ¯ Key Reasons for Introducing JCF
		1ï¸âƒ£ Standardization: 
			Provide a common set of interfaces so developers can code against interfaces instead of implementations.
			Example:
					List<String> list = new ArrayList<>();
			This allows flexibility to switch to:
					List<String> list = new LinkedList<>();
		
		2ï¸âƒ£ Reusability & Interoperability
			All collections follow common interfaces â†’ algorithms work across implementations.
			Example:
					Collections.sort(list);
			Works for any List implementation.

		3ï¸âƒ£ Performance Improvements
			Non-synchronized alternatives (e.g., ArrayList instead of Vector)
			Better data structures (e.g., HashMap, TreeMap)
	
		4ï¸âƒ£ Algorithm Abstraction
			The Collections utility class provides: Sorting, Searching, Reversing, Synchronization wrappers
			Example:
					Collections.reverse(list);
		
		5ï¸âƒ£ Cleaner Design (OOP Principles): Interface-based design, Polymorphism, Extensibility, Separation of concerns

=============================================================================================================
ğŸ”¹ Q2. Difference between Collection and Collections?
=============================================================================================================
| Feature            | `Collection`                  | `Collections`                            |
| ------------------ | ----------------------------- | ---------------------------------------- |
| Type               | Interface                     | Utility Class                            |
| Package            | `java.util`                   | `java.util`                              |
| Purpose            | Represents a group of objects | Provides utility methods for collections |
| Part of hierarchy  | Yes (root interface)          | No                                       |
| Can create object? | No (it's an interface)        | No (private constructor)                 |
| Examples           | List, Set, Queue              | sort(), reverse(), synchronizedList()    |

=============================================================================================================
ğŸ”¹ Q3. Explain the Collection hierarchy in detail.
=============================================================================================================
1ï¸âƒ£ Root: Iterable
			Iterable
			   â””â”€â”€ Collection

		ğŸ”¹ Why Iterable? Introduced to support for-each loop
			Defines: Iterator<T> iterator(); 
					Without Iterable, this would not work: for(String s : list) { }

2ï¸âƒ£ Collection Interface: This is the root interface for all collections except Map.
	Core methods: add(), remove(), size(), contains(), iterator()
	It represents a group of objects.

3ï¸âƒ£ Subinterfaces of Collection
	Collection
	 â”œâ”€â”€ List
	 â”œâ”€â”€ Set
	 â””â”€â”€ Queue

	ğŸ”¹ A) List Interface: 
			Characteristics: Ordered, Allows duplicates, Allows null, Index-based access
			Implementations:
				| Class      | Internal Structure | Use Case               |
				| ---------- | ------------------ | ---------------------- |
				| ArrayList  | Dynamic array      | Fast read              |
				| LinkedList | Doubly linked list | Frequent insert/delete |
				| Vector     | Synchronized array | Legacy                 |
				| Stack      | Extends Vector     | LIFO (legacy)          |

	ğŸ”¹ B) Set Interface
			Characteristics: No duplicates, No index-based access
			Implementations:
				| Class         | Internal Structure | Order           |
				| ------------- | ------------------ | --------------- |
				| HashSet       | Hash table         | No order        |
				| LinkedHashSet | Hash + Linked List | Insertion order |
				| TreeSet       | Red-Black Tree     | Sorted          |

	ğŸ”¹ C) Queue Interface
			Characteristics: FIFO structure, Used for processing order
			Implementations:
				| Class         | Internal Structure | Use Case          |
				| ------------- | ------------------ | ----------------- |
				| PriorityQueue | Heap               | Priority-based    |
				| ArrayDeque    | Resizable array    | Faster than Stack |
				| LinkedList    | Doubly linked list | Queue/Deque       |

4ï¸âƒ£ Map Hierarchy (Separate from Collection)
	âš  Map does NOT extend Collection. Why? Because Map stores key-value pairs, not individual elements.
			Map
			 â”œâ”€â”€ HashMap
			 â”œâ”€â”€ LinkedHashMap
			 â”œâ”€â”€ TreeMap
			 â”œâ”€â”€ Hashtable (Legacy)
			 â”œâ”€â”€ ConcurrentHashMap
			 â”œâ”€â”€ WeakHashMap
			 â””â”€â”€ EnumMap

	Key Differences:
		| Map Type          | Ordering         | Thread-safe | Internal Structure |
		| ----------------- | ---------------- | ----------- | ------------------ |
		| HashMap           | No               | No          | Hash table         |
		| LinkedHashMap     | Insertion/Access | No          | Hash + Linked      |
		| TreeMap           | Sorted           | No          | Red-Black Tree     |
		| ConcurrentHashMap | No               | Yes         | CAS + Buckets      |
		| Hashtable         | No               | Yes         | Legacy             |

5ï¸âƒ£ Abstract Classes (Important for Senior Devs)
	Between interfaces and implementations, Java provides skeletal implementations:
		AbstractCollection
		   â”œâ”€â”€ AbstractList
		   â”œâ”€â”€ AbstractSet
		   â”œâ”€â”€ AbstractQueue
		   â””â”€â”€ AbstractMap
		   
		Why? To reduce boilerplate when creating custom collections.
		Example: If you implement List, better extend AbstractList.

6ï¸âƒ£ Complete Structure in Tree Form
		Iterable
		 â””â”€â”€ Collection
			  â”œâ”€â”€ List
			  â”‚     â”œâ”€â”€ ArrayList
			  â”‚     â”œâ”€â”€ LinkedList
			  â”‚     â”œâ”€â”€ Vector
			  â”‚     â””â”€â”€ Stack
			  â”‚
			  â”œâ”€â”€ Set
			  â”‚     â”œâ”€â”€ HashSet
			  â”‚     â”œâ”€â”€ LinkedHashSet
			  â”‚     â””â”€â”€ TreeSet
			  â”‚
			  â””â”€â”€ Queue
					â”œâ”€â”€ PriorityQueue
					â”œâ”€â”€ ArrayDeque
					â””â”€â”€ LinkedList

		Map (Separate)
		 â”œâ”€â”€ HashMap
		 â”œâ”€â”€ LinkedHashMap
		 â”œâ”€â”€ TreeMap
		 â”œâ”€â”€ Hashtable
		 â”œâ”€â”€ ConcurrentHashMap
		 â”œâ”€â”€ WeakHashMap
		 â””â”€â”€ EnumMap

=============================================================================================================
ğŸ”¹ Q5. Difference between Iterable and Iterator?
=============================================================================================================
ğŸ”¥ Difference Between Iterable and Iterator
	| Feature          | `Iterable`                                   | `Iterator`                        |
	| ---------------- | -------------------------------------------- | --------------------------------- |
	| Type             | Interface                                    | Interface                         |
	| Package          | `java.lang`                                  | `java.util`                       |
	| Purpose          | Represents a collection that can be iterated | Used to iterate over elements     |
	| Key Method       | `iterator()`                                 | `hasNext()`, `next()`, `remove()` |
	| For-each support | Yes                                          | No                                |
	| Role             | Provides iterator                            | Traverses elements                |

1ï¸âƒ£ Iterable â€“ The Source of Iteration
	Iterable represents something that can be iterated.
	It has only one abstract method:
			Iterator<T> iterator();
		Why was it introduced? To support enhanced for-each loop (Java 5).
		Example:
			List<String> list = new ArrayList<>();
			for (String s : list) {
				System.out.println(s);
			}
		Internally, this works because List implements Iterable.
		How For-Each Works Internally
				Iterator<String> it = list.iterator();
				while(it.hasNext()) {
					String s = it.next();
				}

2ï¸âƒ£ Iterator â€“ The Traversal Mechanism
	Iterator is used to actually traverse the collection.
	Key methods:
			boolean hasNext();
			E next();
			void remove();

		Example:
			Iterator<String> it = list.iterator();
			while (it.hasNext()) {
				System.out.println(it.next());
			}

ğŸ¯ Core Concept Difference
	Iterable = Collection side
	Iterator = Traversal side
	Think of it like:
		Iterable â†’ "I can provide something to iterate"
		Iterator â†’ "I am doing the iteration"

=============================================================================================================
ğŸ”¹ Q6. Why does Collection extend Iterable?
=============================================================================================================
ğŸ”¥ Collection extends Iterable so that all collection types can be used in enhanced for-each loops and provide a standard way to traverse elements.
	ğŸ“Œ Design Reason
		Iterable defines only one method:Iterator<T> iterator();
		By extending Iterable, every collection automatically guarantees: 
			It can provide an Iterator, It supports for-each loop, It follows a consistent traversal mechanism

	ğŸ” Without Iterable, This Would Not Work
			List<String> list = new ArrayList<>();
				for (String s : list) {
					System.out.println(s);
				}
			For-each works only because List â†’ Collection â†’ Iterable.

ğŸ”„ Internal Working of For-Each
	This: for(String s : list)
	Is converted by compiler into:
		Iterator<String> it = list.iterator();
		while(it.hasNext()) {
			String s = it.next();
		}
		So without Iterable, the compiler would not know how to get an iterator.

ğŸ“Œ Hierarchy Relationship
Iterable
   â””â”€â”€ Collection
          â”œâ”€â”€ List
          â”œâ”€â”€ Set
          â””â”€â”€ Queue

ğŸ¯ Why Not Put iterator() Directly in Collection?
	Because: Iterable is a more general contract, Any class (even non-collection) can be iterable
	Example:
			class Range implements Iterable<Integer> {
			   ...
			}
	So Iterable separates: Traversal capability, From collection behavior, This follows Single Responsibility Principle.


=============================================================================================================
ğŸ”¹ Q7. What are the benefits of using generics with collections?
=============================================================================================================
ğŸ”¥ Generics were introduced in Java 5 to provide type safety at compile time.
	Before generics, collections stored Object type only.
		1ï¸âƒ£ Type Safety (Compile-Time Checking)
			âŒ Before Generics
				List list = new ArrayList();
				list.add("Java");
				list.add(10);   // Allowed
				
				String s = (String) list.get(1);  // Runtime ClassCastException
				Error happens at runtime.

			âœ… With Generics
				List<String> list = new ArrayList<>();
				list.add("Java");
				list.add(10);  // Compile-time error
				Error caught at compile time. ğŸ‘‰ This is the biggest benefit.

		2ï¸âƒ£ No Explicit Type Casting
			Without generics: String s = (String) list.get(0);
			With generics: String s = list.get(0);
			Cleaner, safer code.
		
		3ï¸âƒ£ Improved Code Readability
				Map<String, Integer> map = new HashMap<>();
			
				Immediately clear: Key â†’ String, Value â†’ Integer
				without generics, intent is unclear.

		4ï¸âƒ£ Eliminates ClassCastException (Mostly)
			Generics shift errors from:	âŒ Runtime
			âœ… To Compile-time This makes systems more reliable.

		5ï¸âƒ£ Enables Generic Algorithms
			Example:
					public <T> void printList(List<T> list) {
						for (T item : list) {
							System.out.println(item);
						}
					}
			Reusable for any type.

		6ï¸âƒ£ Better API Design
			Collections API heavily uses generics: List<E>, Set<E>, Map<K, V>, Comparator<T>
			This makes framework flexible and type-safe.

		7ï¸âƒ£ Supports Bounded Types
			Example:
				public <T extends Number> void process(List<T> list)
			Ensures only numeric types are allowed.

ğŸ“Œ Internal Note (Very Important for 8+ Years)
	Generics in Java are implemented using Type Erasure.
		Meaning: Type information is removed at runtime, Generics exist only at compile-time
		Example:
			List<String>
			List<Integer>
		At runtime both become: List

=============================================================================================================
ğŸ”¹ Q8. Can you explain fail-fast vs fail-safe iterators?
=============================================================================================================
1ï¸âƒ£ Fail-Fast Iterator
A fail-fast iterator throws ConcurrentModificationException if the collection is structurally modified after the iterator is created (except through iteratorâ€™s own remove()).
	Example:
		List<String> list = new ArrayList<>();
		list.add("A");
		list.add("B");
		
		Iterator<String> it = list.iterator();
		while (it.hasNext()) {
			list.add("C");  // Structural modification
			System.out.println(it.next()); // Throws exception
		}

	ğŸ” How Fail-Fast Works Internally
		Collections like: ArrayList, HashMap, HashSet maintain an internal variable called: modCount
		When: Element is added, Element is removed, Structure changes, modCount increments.
		When iterator is created: It stores current modCount as expectedModCount.
		During iteration:
			if (modCount != expectedModCount)
				throw ConcurrentModificationException;

		ğŸ“Œ Key Points About Fail-Fast: 
		Works on best-effort basis, Not guaranteed (not a synchronization mechanism), Detects bugs early, Not thread-safe

2ï¸âƒ£ Fail-Safe Iterator
Fail-safe iterators do NOT throw exception when collection is modified.
They operate on a clone (snapshot) of the collection.
	Example:
		CopyOnWriteArrayList<String> list = new CopyOnWriteArrayList<>();
		list.add("A");
		list.add("B");
		
		Iterator<String> it = list.iterator();
		list.add("C");  // No exception
		No ConcurrentModificationException.

	ğŸ” How Fail-Safe Works
		Collections like: CopyOnWriteArrayList, ConcurrentHashMap create a separate copy for iteration.
		Changes go to: Original collection, Not the snapshot

âš  Important Clarification
	Fail-safe is not an official term in Java documentation. Itâ€™s commonly used in interviews.
	Official docs say: â€œThe iterators are fail-fast.â€

=============================================================================================================
ğŸ”¹ Q9. What is UnsupportedOperationException in collections?
=============================================================================================================
ğŸ”¥ UnsupportedOperationException is a runtime exception thrown when a collection does not support a particular operation.
	It belongs to: java.lang.UnsupportedOperationException
	It usually occurs when you try to: Add, Remove, Modify
	Elements in a collection that is fixed-size or unmodifiable.

	ğŸ“Œ Common Scenarios
		1ï¸âƒ£ Using Arrays.asList()
			List<String> list = Arrays.asList("A", "B", "C");
			list.add("D");   // âŒ UnsupportedOperationException

			Why? Arrays.asList() returns a fixed-size list backed by the array.
			You can: list.set(0, "X");   // âœ… Allowed
			But cannot:
				list.add("D");      // âŒ Not allowed
				list.remove(0);     // âŒ Not allowed

		2ï¸âƒ£ Using List.of() (Java 9+)
			List<String> list = List.of("A", "B");
			list.add("C");   // âŒ UnsupportedOperationException

			This is a completely immutable list.

		3ï¸âƒ£ Using Collections.unmodifiableList()
			List<String> original = new ArrayList<>();
			original.add("A");
			List<String> unmodifiable = Collections.unmodifiableList(original);
			unmodifiable.add("B");  // âŒ UnsupportedOperationException
			
			But: original.add("B");  // âœ… Modifies underlying list
			This is unmodifiable, not immutable.

ğŸ“Œ Why Does This Exception Exist?
	To enforce: Immutability, Fixed-size behavior, Read-only views, API safety
	It prevents structural modification when the collection design does not allow it.
	
ğŸ“Š Types of Collections That Throw It
| Collection Type      | Supports add/remove? |
| -------------------- | -------------------- |
| ArrayList            | Yes                  |
| Arrays.asList()      | No (fixed size)      |
| List.of()            | No (immutable)       |
| Unmodifiable wrapper | No                   |
| CopyOnWriteArrayList | Yes                  |

ğŸ“Œ Internal Design Reason
	Java follows interface-based design.
	Example: 
		List interface defines:
			boolean add(E e);
		But implementation is free to decide: Support it Or throw UnsupportedOperationException
		This allows: Read-only collections, Partial implementations, Optimized immutable collections

ğŸ“Œ Important Interview Clarification
	UnsupportedOperationException is:
		âŒ NOT checked exception, âŒ NOT compile-time error
		âœ… Runtime exception

=============================================================================================================
ğŸ”¹ Q10. How does equals() and hashCode() affect collections?
=============================================================================================================
ğŸ”¥ They directly impact hash-based collections, mainly:
	HashMap, HashSet, LinkedHashMap, ConcurrentHashMap
	
	ğŸ“Œ Why Are They Important?
		Hash-based collections use:
			1ï¸âƒ£ hashCode() â†’ To decide bucket
			2ï¸âƒ£ equals() â†’ To compare objects within bucket

	ğŸ” Internal Working (HashMap Example)
		When you do: map.put(key, value);
			Step 1 â†’ hashCode() is called, It determines bucket index.
			Step 2 â†’ If collision occurs equals() is used to check if key already exists.

	ğŸ“Œ The Contract (Very Important)
		Java defines strict contract:
			1ï¸âƒ£ If two objects are equal using equals(): ğŸ‘‰ They MUST have same hashCode()
			2ï¸âƒ£ If two objects have same hashCode(): ğŸ‘‰ They MAY or MAY NOT be equal

	ğŸ”¥ What Happens If You Violate Contract?
		âŒ Case 1: Override equals() but NOT hashCode()
				class Person {
				   String name;

				   public boolean equals(Object o) {
					   return true;
				   }
				}
			Result: HashSet may store duplicates, HashMap lookup fails, Behavior becomes inconsistent

		âŒ Case 2: Mutable Keys in HashMap
				class Person {
				   String name;
				}

			If used as key:
				map.put(person, "data");
				person.name = "changed";
				map.get(person);  // âŒ May return null
				
				Because: hashCode changed, Bucket changed, Object becomes unreachable
		
		ğŸ“Œ HashSet Behavior
			HashSet internally uses: HashMap<E, Object>
				So: Duplicate prevention depends on equals() + hashCode()
				If incorrectly implemented â†’ duplicates allowed.
		
		ğŸ“Œ Performance Impact
			Poor hashCode() implementation like:
				public int hashCode() {
				   return 1;
				}
			All elements go into same bucket â†’
			HashMap becomes linked list â†’
			Performance drops from: O(1) â†’ O(n)
		
		ğŸ“Œ Where equals() Alone Is Used?
			In: List.contains(), List.remove(), TreeSet (if comparator not used)
			These do not use hashCode().

=============================================================================================================
ğŸ”¹ Q11. what is ArrayList,it default capacity,How does resizing happen, how does ArrayList internally work and
What happens internally when ArrayList grows?
=============================================================================================================
ğŸ”¥ ArrayList is a resizable array implementation of the List interface.
	It provides: Ordered collection, Allows duplicates, Allows null, Fast random access (O(1))
	It belongs to java.util package and extends AbstractList.

ğŸ”¥ 2ï¸âƒ£ Default Capacity of ArrayList
ğŸ”¹ Before Java 8: Default capacity = 10
ğŸ”¹ Java 8+ Important change:
	When you do: new ArrayList<>();
	It does NOT immediately create array of size 10.
	Instead: It creates an empty array (DEFAULTCAPACITY_EMPTY_ELEMENTDATA)
	Capacity becomes 10 only when first element is added

ğŸ”¥ 3ï¸âƒ£ Internal Structure of ArrayList
	Internally it maintains: 
			transient Object[] elementData;
			private int size;
		elementData â†’ Actual array storing elements
		size â†’ Number of elements currently stored

ğŸ”¥ 4ï¸âƒ£ How Does ArrayList Internally Work?
		When you do: list.add("A");
			Step 1 â†’ Check Capacity
					If size < elementData.length â†’ Insert directly.
			Step 2 â†’ If Full â†’ Resize
			
ğŸ”¥ 5ï¸âƒ£ What Happens When ArrayList Grows?
		When capacity is full:
			1ï¸âƒ£ New larger array is created
			2ï¸âƒ£ Old elements copied to new array
			3ï¸âƒ£ Reference updated
			ğŸ”¹ Growth Formula (Java 8+)
				New capacity = oldCapacity + (oldCapacity >> 1)
				Which means: oldCapacity * 1.5
				
			ğŸ”¹ Internal Method Used
				Arrays.copyOf(elementData, newCapacity);
				This internally uses: System.arraycopy()
				Which is very fast (native method).

ğŸ”¥ 6ï¸âƒ£ Time Complexity
| Operation         | Complexity     |
| ----------------- | -------------- |
| get(index)        | O(1)           |
| add()             | O(1) amortized |
| add() when resize | O(n)           |
| remove(index)     | O(n)           |
Why amortized O(1)? Because resize doesnâ€™t happen every time.
	
ğŸ”¥ 7ï¸âƒ£ What Happens Internally During Resize (Step-by-Step)
		Suppose capacity = 10 and size = 10
		When adding 11th element:
			1ï¸âƒ£ ensureCapacityInternal() called
			2ï¸âƒ£ grow() method triggered
			3ï¸âƒ£ New array created (capacity 15)
			4ï¸âƒ£ System.arraycopy() copies 10 elements
			5ï¸âƒ£ elementData updated
			6ï¸âƒ£ New element inserted

ğŸ”¥ 8ï¸âƒ£ Memory Consideration
		ArrayList may waste unused memory: If capacity = 100 But size = 10, Remaining 90 slots unused.
		You can reduce using: list.trimToSize();

ğŸ”¥ 9ï¸âƒ£ Important Interview Edge Cases
	â“ What if we know size in advance?
		Use: new ArrayList<>(1000); Improves performance by avoiding resizing.
	â“ Why ArrayList is not thread-safe?
		Because multiple threads modifying elementData without synchronization can corrupt state.
	â“ What if initial capacity is negative? Throws IllegalArgumentException.

=============================================================================================================
ğŸ”¹ Q12. Why ArrayList allows duplicate and null values? Difference between ensureCapacity() and trimToSize()?
=============================================================================================================
ğŸ“Œ A) Why Duplicates Are Allowed?
	Because ArrayList implements the List interface.
		List contract says: Ordered collection, Allows duplicate elements, Unlike Set, which enforces uniqueness
		List is designed to preserve: Insertion order, Positional access (index-based), Repeated values
		Internally, ArrayList is just a dynamic array: transient Object[] elementData;
		It does NOT check: hashCode(), equals(), uniqueness
		So:
			list.add("A");
			list.add("A");  // Allowed
			No validation is performed.

ğŸ“Œ B) Why Null Is Allowed?
	Because: ArrayList stores elements in Object[], null is a valid reference in Java
	There is no restriction in List contract against null.
		list.add(null);  // Allowed
	Internally it just stores: elementData[index] = null;

ğŸ“Œ Why Some Collections Do NOT Allow Null?
	| Collection        | Null Allowed? | Reason                         |
	| ----------------- | ------------- | ------------------------------ |
	| HashMap           | One null key  | Design decision                |
	| TreeMap           | No null key   | Uses compareTo()               |
	| ConcurrentHashMap | No null       | Avoid ambiguity in concurrency |
	| ArrayList         | Yes           | No restriction                 |

2ï¸âƒ£ Difference Between ensureCapacity() and trimToSize()
	These are capacity management methods in ArrayList.
		ğŸ“Œ A) ensureCapacity(int minCapacity) 
			Purpose: Increases internal array capacity if needed.
			Use Case: When you know in advance how many elements will be added.
			Example:
				ArrayList<String> list = new ArrayList<>();
				list.ensureCapacity(1000);

			This: Avoids multiple resizes, Improves performance
			Internally:
				If: minCapacity > currentCapacity, Then grow() is triggered. Otherwise, nothing happens.

		ğŸ“Œ B) trimToSize()
			Purpose: Reduces internal capacity to match current size.
			Example: list.trimToSize();
				If: Capacity = 100 Size = 10
				After trim: Capacity becomes 10

â“ When would you use ensureCapacity()?
	When: Loading large dataset from DB, Parsing big file, Bulk insert known size
	Example:
			List<String> data = new ArrayList<>(100000);
		Or:
			list.ensureCapacity(100000);

â“ When would you use trimToSize()?
	When: After removing many elements, Before caching list, Before serializing list, To reduce memory footprint

=============================================================================================================
ğŸ”¹ Q14. Why ArrayList is not thread-safe? When should you NOT use ArrayList?
=============================================================================================================
ğŸ“Œ Because ArrayList does not use synchronization, so multiple threads modifying it simultaneously can cause:
	Data corruption, Lost updates, Inconsistent state, ArrayIndexOutOfBoundsException

	ğŸ” Internal Reason
		Internally, ArrayList has:
			transient Object[] elementData;
			private int size;
		
		When you call: list.add("A");
		Internally:
			1ï¸âƒ£ Check capacity
			2ï¸âƒ£ Insert element
			3ï¸âƒ£ Increment size
			None of these operations are synchronized.

	ğŸ“Œ Race Condition Example
		Suppose two threads run:
			list.add("A");
			list.add("B");
		Possible issue: Both read same size, Both write to same index, One value gets overwritten, size becomes inconsistent

ğŸ”¥ What Happens During Resize in Multithreading?
	If two threads trigger resize simultaneously:
		Both create new arrays, Copy happens inconsistently, elementData reference may be corrupted
		You may get: ArrayIndexOutOfBoundsException or NullPointerException

ğŸ”¥ Why Vector Is Thread-Safe?
	Because Vector methods are: public synchronized boolean add(E e)
	But synchronization causes: Performance overhead, Lock contention
	Thatâ€™s why Vector is considered legacy.

ğŸ”¥ How to Make ArrayList Thread-Safe?
	Option 1: Collections.synchronizedList(new ArrayList<>());
	Option 2 (Better for read-heavy): CopyOnWriteArrayList<>();

ğŸ”¥ When Should You NOT Use ArrayList?
	ğŸš« 1ï¸âƒ£ In Multi-Threaded Environment (Without Synchronization)
		Use: CopyOnWriteArrayList, Concurrent collections
	
	ğŸš« 2ï¸âƒ£ Frequent Insert/Delete at Beginning or Middle
		ArrayList shifts elements: O(n)
		Use: LinkedList

	ğŸš« 3ï¸âƒ£ When You Need Sorted Structure
		Use: TreeSet, TreeMap
	
	ğŸš« 4ï¸âƒ£ When You Need Constant-Time Insert/Delete
		Use: LinkedList (for frequent structural modification)

	ğŸš« 5ï¸âƒ£ When Memory Is Highly Constrained
		ArrayList: Keeps extra unused capacity, Can waste memory

	ğŸš« 6ï¸âƒ£ Real-Time Systems (Where Resize Is Risky)
		Resize operation: O(n), Can cause latency spikes


=============================================================================================================
ğŸ”¹ Q15. What is LinkedList and Internal structure of LinkedList?
=============================================================================================================
ğŸ”¥ LinkedList is a doubly linked list implementation of: List, Deque, Queue, It belongs to java.util package.
	It allows: Duplicate elements, Null values, Insertion at beginning/middle/end
	Unlike ArrayList, it does NOT use a dynamic array.

	ğŸ”¥ Internal Structure of LinkedList
		Internally, LinkedList is implemented as a doubly linked list. Each element is stored in a Node object.

	ğŸ“Œ Internal Node Class (Simplified)
				private static class Node<E> {
					E item;
					Node<E> next;
					Node<E> prev;
				}
				Each node contains:
					item â†’ actual data
					next â†’ pointer to next node
					prev â†’ pointer to previous node

	ğŸ“Œ LinkedList Internal Fields
			transient Node<E> first;
			transient Node<E> last;
			private int size;
			
			first â†’ head pointer
			last â†’ tail pointer
			size â†’ number of elements
			
ğŸ”¥ How LinkedList Works Internally
	ğŸ”¹ Adding Element at End : list.add("A");
		Internally:
			1ï¸âƒ£ Create new node
			2ï¸âƒ£ Set newNode.prev = last
			3ï¸âƒ£ last.next = newNode
			4ï¸âƒ£ Update last pointer
			5ï¸âƒ£ Increment size
			Time Complexity â†’ O(1)

	ğŸ”¹ Adding at Beginning: list.addFirst("A");
			1ï¸âƒ£ Create new node
			2ï¸âƒ£ newNode.next = first
			3ï¸âƒ£ first.prev = newNode
			4ï¸âƒ£ Update first pointer
			Time Complexity â†’ O(1)

	ğŸ”¹ Adding in Middle
			1ï¸âƒ£ Traverse to index â†’ O(n)
			2ï¸âƒ£ Update previous & next links
			Time Complexity â†’ O(n)

	ğŸ”¹ Removing Element
		Removal does: 
			prev.next = next;
			next.prev = prev;
			No shifting required.

ğŸ”¥ Why Random Access Is Slow?
	If you do: list.get(500);
	LinkedList: Traverses node by node, From head or tail (whichever is closer)	Time Complexity â†’ O(n)
	While ArrayList â†’ O(1)

ğŸ”¥ Time Complexity Comparison
	| Operation       | LinkedList | ArrayList      |
	| --------------- | ---------- | -------------- |
	| get(index)      | O(n)       | O(1)           |
	| add() end       | O(1)       | O(1) amortized |
	| add() middle    | O(n)       | O(n)           |
	| remove() middle | O(n)       | O(n)           |
	| insert at start | O(1)       | O(n)           |

ğŸ”¥ Memory Overhead: 
	LinkedList consumes more memory because:
		Each node stores: Data, Next pointer, Prev pointer, So memory overhead is higher than ArrayList.

ğŸ”¥ When to Use LinkedList?
	âœ… Frequent insert/delete at beginning
	âœ… Implementing stack/queue
	âœ… Deque operations
	ğŸš« Not good for: Random access, Heavy read scenarios

=============================================================================================================
ğŸ”¹ Q16. Difference between ArrayList and LinkedList (beyond basics)?
=============================================================================================================
ğŸ“Œ 1ï¸âƒ£ Internal Data Structure
	ğŸ”¹ ArrayList: Backed by dynamic array, Continuous memory allocation
			Uses Object[] elementData
	
	ğŸ”¹ LinkedList: Doubly linked list, Each element stored in a separate Node
		Node contains: Data, next reference, prev reference

ğŸ”¥ 2ï¸âƒ£ CPU Cache Friendliness (Very Important)
	ArrayList: Elements stored sequentially, CPU cache-friendly, Faster iteration in real-world
	LinkedList: Nodes scattered in heap, Cache misses occur frequently, Slower iteration despite same O(n)
	
	ğŸ‘‰ In practice, ArrayList often outperforms LinkedList even for inserts.

ğŸ”¥ 3ï¸âƒ£ Time Complexity (Realistic View)
| Operation     | ArrayList      | LinkedList     |
| ------------- | -------------- | -------------- |
| get(index)    | O(1)           | O(n)           |
| add at end    | O(1) amortized | O(1)           |
| insert middle | O(n) shift     | O(n) traversal |
| remove middle | O(n) shift     | O(n) traversal |
| iteration     | Faster         | Slower         |

Important insight: ğŸ‘‰ LinkedList still requires traversal O(n) before insertion.
So benefit is only when: You already have node reference Or inserting at head/tail

ğŸ”¥ 4ï¸âƒ£ Memory Overhead
	ArrayList: Only object references stored, Some unused capacity
	LinkedList: Each node stores: Object reference, prev pointer, next pointer, Node object overhead
	ğŸ‘‰ LinkedList consumes significantly more memory.

ğŸ”¥ 5ï¸âƒ£ Resize Cost
	ArrayList: Resizes using 1.5x growth, O(n) copy during resize
	LinkedList: No resizing, Always allocates new node, But resize happens rarely (amortized O(1)).

ğŸ”¥ 6ï¸âƒ£ GC Impact
	LinkedList: Creates many Node objects, More pressure on GC, Higher object allocation overhead
	ArrayList: Single array object, Lower GC pressure

ğŸ”¥ 7ï¸âƒ£ Real Production Insight
	In real-world systems:
		ğŸ‘‰ ArrayList is used 90% of the time
		ğŸ‘‰ LinkedList is rarely used
	Because: Modern CPUs favor contiguous memory, LinkedList overhead outweighs theoretical benefit

ğŸ”¥ 8ï¸âƒ£ When LinkedList Makes Sense
	Use LinkedList when: Implementing queue/deque, Frequent add/remove at head
	You need constant-time insert with known node reference
	Better alternative often: ArrayDeque

ğŸ”¥ 9ï¸âƒ£ Iteration Performance (Surprising Fact)
	Even though both are O(n): ArrayList iteration is usually 2â€“5x faster
	Because of: Cache locality, No pointer chasing

=============================================================================================================
ğŸ”¹ Q17. Why LinkedList is preferred for frequent insert/delete?
=============================================================================================================
ğŸ”¥ Why LinkedList Is Preferred for Frequent Insert/Delete?
	ğŸ“Œ Core Reason: Because LinkedList does not require shifting elements.
		Unlike ArrayList, which shifts elements during insert/delete, LinkedList just updates node references.

	ğŸ”¹ In ArrayList (Insert in Middle)
		Example: list.add(2, "X");
		Internally:
			1ï¸âƒ£ All elements from index 2 onward are shifted right
			2ï¸âƒ£ System.arraycopy() used
			3ï¸âƒ£ Time complexity â†’ O(n)

	ğŸ”¹ In LinkedList (Insert in Middle)
		Internally:
			1ï¸âƒ£ Traverse to index (O(n))
			2ï¸âƒ£ Update pointers:
				prev.next = newNode;
				newNode.prev = prev;
				newNode.next = next;
				next.prev = newNode;
				No shifting.

ğŸ”¥ But Here Is the Senior-Level Truth
	Most people stop here. But interviewer expects more ğŸ‘‡
	âš  Important Clarification; LinkedList insertion is O(1) only if you already have node reference.
	If you insert by index: list.add(5000, "X");
	LinkedList must: Traverse to index â†’ O(n), Then update links â†’ O(1), Total â†’ O(n)
	Same complexity as ArrayList (but without shifting cost).

ğŸ”¥ When LinkedList Truly Shines
	âœ… Frequent insert/remove at head: addFirst(), removeFirst(), Time â†’ O(1)
	âœ… Implementing Queue / Deque
		Because: Head/tail operations are constant time.
	
ğŸ”¥ Real-World Insight
	In practice: ArrayList often outperforms LinkedList, Because CPU cache favors contiguous memory
	LinkedList has pointer chasing overhead So LinkedList is preferred only when:
		Heavy structural modification at ends, Not random access, Not heavy iteration

ğŸ“Š Performance Reality Table
| Operation        | ArrayList | LinkedList |
| ---------------- | --------- | ---------- |
| Insert at start  | O(n)      | O(1)       |
| Insert at end    | O(1)*     | O(1)       |
| Insert in middle | O(n)      | O(n)       |
| Remove start     | O(n)      | O(1)       |
| Random access    | O(1)      | O(n)       |

=============================================================================================================
ğŸ”¹ Q20. LinkedList vs ArrayDeque?
=============================================================================================================
ğŸ”¥ LinkedList vs ArrayDeque
	Both implement the Deque interface and can be used as:
		Queue (FIFO)
		Stack (LIFO)
	
		But internally they are completely different.

ğŸ“Œ 1ï¸âƒ£ Internal Data Structure
	ğŸ”¹ LinkedList: Doubly linked list, Each element stored in a separate Node
		Node contains: Data, next pointer, prev pointer

	ğŸ”¹ ArrayDeque: Resizable circular array, Uses single Object[], Maintains head and tail indexes, No shifting during add/remove at ends

ğŸ”¥ 2ï¸âƒ£ Performance Comparison
| Operation       | LinkedList | ArrayDeque |
| --------------- | ---------- | ---------- |
| addFirst()      | O(1)       | O(1)       |
| addLast()       | O(1)       | O(1)       |
| removeFirst()   | O(1)       | O(1)       |
| removeLast()    | O(1)       | O(1)       |
| Memory usage    | High       | Lower      |
| Cache locality  | Poor       | Excellent  |
| Iteration speed | Slower     | Faster     |

ğŸ”¥ 3ï¸âƒ£ Why ArrayDeque Is Usually Faster
	ğŸ”¹ Cache Locality: 
		ArrayDeque: Stores elements in contiguous memory, CPU cache-friendly
		LinkedList: Nodes scattered in heap, Pointer chasing â†’ cache misses
	
	ğŸ”¹ Less Memory Overhead
		LinkedList: Each node has 2 references, Extra object allocation, More GC pressure
		ArrayDeque: Single array, Minimal object overhead

ğŸ”¥ 4ï¸âƒ£ Resizing Behavior
	ArrayDeque: Resizes when full, Doubles capacity, O(n) copy but rare
	LinkedList: No resizing, But allocates node per element

ğŸ”¥ 5ï¸âƒ£ Null Handling
	| Collection | Null Allowed? |
	| ---------- | ------------- |
	| LinkedList | Yes           |
	| ArrayDeque | âŒ No    	 |
ArrayDeque disallows null to distinguish: null return vs empty queue

ğŸ”¥ 6ï¸âƒ£ When to Use What?
	âœ… Use ArrayDeque when: Implementing stack, Implementing queue, Performance matters, High-throughput system
	âœ… Use LinkedList when: You need List + Deque together, You need null elements, You require frequent insertion in middle (rare case)

ğŸ”¥ 7ï¸âƒ£ Real-World Insight
	In production systems:
		ğŸ‘‰ ArrayDeque is preferred over LinkedList for stack/queue
		ğŸ‘‰ LinkedList is rarely used for pure deque operations

Even Java documentation says: ArrayDeque is likely to be faster than LinkedList when used as a queue.

=============================================================================================================
ğŸ”¹ Q21. Why Vector is considered legacy? Difference between Vector and ArrayList? Why Enumeration is obsolete?
Should Vector ever be used today?
=============================================================================================================
ğŸ”¥ Vector was introduced in Java 1.0 (before Collection Framework).
	When Java 1.2 introduced the modern Collection Framework:
		ArrayList replaced Vector, Iterator replaced Enumeration, More flexible concurrency models were introduced
	
	ğŸ‘‰ Vector is considered legacy because:
			1ï¸âƒ£ All its methods are synchronized (performance overhead)
			2ï¸âƒ£ Poor scalability in multi-threaded systems
			3ï¸âƒ£ Coarse-grained locking (whole object lock)
			4ï¸âƒ£ Better alternatives exist today

ğŸ“Œ Internal Structure of Vector
	Like ArrayList, Vector is backed by:
			Object[] elementData;
			int elementCount;
		But its methods are: public synchronized boolean add(E e), Every method acquires a lock.

ğŸ”¥ Difference Between Vector and ArrayList
| Feature             | Vector             | ArrayList    |
| ------------------- | ------------------ | ------------ |
| Introduced          | Java 1.0           | Java 1.2     |
| Thread-safe         | Yes (synchronized) | No           |
| Performance         | Slower             | Faster       |
| Growth policy       | Doubles capacity   | Grows by 50% |
| Enumeration support | Yes                | No           |
| Modern usage        | Rare               | Very common  |

ğŸ”¥ Growth Policy Difference
	Vector (Default)
		When full: newCapacity = oldCapacity * 2, OR uses capacityIncrement if specified.
		ArrayList: newCapacity = oldCapacity + (oldCapacity >> 1), (1.5x growth)
		ArrayList growth is more memory efficient.

ğŸ”¥ Why Enumeration Is Obsolete?
	Enumeration was introduced before Iterator.
		Enumeration Methods:
				hasMoreElements()
				nextElement()
			Limitations: âŒ No remove() support, âŒ Poor naming, âŒ Limited functionality
		
		Iterator Replaced Enumeration
			Iterator provides: hasNext(), next(), remove()
			Better design and extensibility.

ğŸ”¥ Why Vectorâ€™s Synchronization Is a Problem
	Vector synchronizes every method, even reads: public synchronized E get(int index)
		This causes: Lock contention, Reduced throughput, Poor scalability
		Modern alternative: Collections.synchronizedList(new ArrayList<>()) Or better: CopyOnWriteArrayList

ğŸ”¥ Should Vector Ever Be Used Today?
	âŒ Generally No
	Better alternatives exist:
	| Scenario         | Better Choice          |
	| ---------------- | ---------------------- |
	| Single-threaded  | ArrayList              |
	| Multi-threaded   | CopyOnWriteArrayList   |
	| High concurrency | Concurrent collections |

âš  Rare Case Use Vector only if: Maintaining legacy system, Working with very old APIs expecting Vector
Otherwise â†’ avoid.

=============================================================================================================
ğŸ”¹ Q22. What is Hashset and How does HashSet internally work? How HashSet ensures uniqueness? Why HashSet allows only one null?
=============================================================================================================
ğŸ”¥ HashSet is an implementation of the Set interface that Does NOT allow duplicates, Does NOT maintain insertion order
Allows one null, Provides average O(1) time complexity and It belongs to java.util package.

ğŸ”¥ Internal Implementation (Very Important)
	ğŸ‘‰ HashSet is internally backed by a HashMap.
		Simplified source code idea: 
				private transient HashMap<E, Object> map;
				private static final Object PRESENT = new Object();
			
			When you do: set.add("A");
			Internally:
				map.put("A", PRESENT);
			So HashSet stores:
				Element â†’ as key
				Dummy object â†’ as value

ğŸ”¥ How HashSet Internally Works
	When you call: set.add(element);
		Step 1ï¸âƒ£ â†’ Call hashCode(), Determines bucket index.
		Step 2ï¸âƒ£ â†’ Go to bucket, Index calculated as: hash & (n - 1), Where n = table size.
		Step 3ï¸âƒ£ â†’ Check Collision, If bucket empty â†’ insert, If not â†’ compare using equals()
		Step 4ï¸âƒ£ â†’ If equal found Do NOT insert (duplicate prevented)
		Step 5ï¸âƒ£ â†’ If not equal, Add to bucket (linked list or tree)

ğŸ”¥ Java 8 Improvement
	If bucket size > 8 and table size â‰¥ 64:
		Linked list converts into: ğŸ‘‰ Red-Black Tree
		Improves worst-case performance from: O(n) â†’ O(log n)

ğŸ”¥ How HashSet Ensures Uniqueness?
	Uniqueness depends on: 	1ï¸âƒ£ hashCode(), 2ï¸âƒ£ equals()
		Two elements are considered duplicates if:
			existing.equals(newElement) == true, they land in same bucket (same hashCode).
			If equals() returns true: ğŸ‘‰ Insertion rejected.

âš  Important Interview Trap
	If you override equals() but NOT hashCode(): ğŸ‘‰ HashSet may allow duplicates.
	Because: Different hashCode â†’ different bucket, equals() never compared

ğŸ”¥ Why HashSet Allows Only One Null?
	Because: HashSet is backed by HashMap, HashMap allows only one null key
	Internally:
		hash(null) = 0
		All null values go to bucket 0.
		When second null is added: equals(null, null) â†’ true
		So duplicate â†’ rejected.

ğŸ“Œ What Happens Internally When Adding null?
	set.add(null);
		Internally: map.put(null, PRESENT);
		HashMap allows one null key â†’ so HashSet allows one null element.

ğŸ“Š Time Complexity
| Operation  | Average | Worst Case |
| ---------- | ------- | ---------- |
| add()      | O(1)    | O(log n)   |
| remove()   | O(1)    | O(log n)   |
| contains() | O(1)    | O(log n)   |

=============================================================================================================
ğŸ”¹ Q23. How equals() and hashCode() work together? What happens if hashCode changes after insertion?
=============================================================================================================
ğŸ”¥ They work together in hash-based collections like: HashMap, HashSet, LinkedHashMap, ConcurrentHashMap
	ğŸ“Œ Step-by-Step Flow (HashMap Example)
		When you do: map.put(key, value);
		1ï¸âƒ£ hashCode() is called: Used to determine the bucket index.
			int hash = key.hashCode();
			int index = hash & (table.length - 1);
		
		2ï¸âƒ£ Go to that bucket
			If bucket empty â†’ insert directly.
			If bucket contains elements â†’ collision handling begins.

		3ï¸âƒ£ equals() is used
			Inside the bucket: if (existingKey.equals(newKey))
				If true â†’ value replaced.
				If false â†’ add to bucket (linked list or tree).

ğŸ“Œ The Contract (Very Important)
	Java defines:
		Rule 1ï¸âƒ£ 	If two objects are equal (equals() returns true), ğŸ‘‰ they MUST have same hashCode().
		Rule 2ï¸âƒ£  If two objects have same hashCode(), ğŸ‘‰ they MAY or MAY NOT be equal.

ğŸ”¥ Why Both Are Needed?
	hashCode() â†’ Fast bucket lookup
	equals() â†’ Exact object comparison
	Without hashCode: All elements go to same bucket â†’ O(n)
	Without equals: Cannot verify actual equality inside bucket.

ğŸ”¥ What Happens If hashCode() Changes After Insertion?
		Consider:
				class Person {
				   String name;

				   public int hashCode() {
					   return name.hashCode();
				   }

				   public boolean equals(Object o) {
					   return this.name.equals(((Person)o).name);
				   }
				}

			Now:
				Person p = new Person("John");
				map.put(p, "Data");
			
			Then: p.name = "David";   // hashCode changes
			Now: map.get(p);   // âŒ Returns null

		ğŸ“Œ Why Does This Happen?
			Because:
				1ï¸âƒ£ When inserted, bucket calculated using old hashCode.
				2ï¸âƒ£ After mutation, new hashCode points to different bucket.
				3ï¸âƒ£ HashMap searches wrong bucket.
				4ï¸âƒ£ Object becomes unreachable.

ğŸ”¥ Consequences of Changing hashCode After Insert
	Key becomes unreachable: Memory leak, containsKey() returns false, remove() fails, Data inconsistency
	Very dangerous in production systems.
	ğŸ”¥ Golden Rule
		ğŸ‘‰ Keys in HashMap should be immutable.
		Thatâ€™s why: String is immutable, Integer is immutable
		Most recommended keys are immutable

=============================================================================================================
ğŸ”¹ Q24. What is  LinkedHashSet.How LinkedHashSet maintains insertion order? 
=============================================================================================================
ğŸ”¥ LinkedHashSet is a Set implementation that:
		Does NOT allow duplicates, Maintains insertion order, Allows one null, Provides near O(1) performance
		It belongs to java.util package.

ğŸ“Œ Key Difference from HashSet
| Feature            | HashSet         | LinkedHashSet             |
| ------------------ | --------------- | ------------------------- |
| Order              | No order        | Maintains insertion order |
| Internal Structure | HashMap         | LinkedHashMap             |
| Performance        | Slightly faster | Slightly slower           |

ğŸ”¥ Internal Working of LinkedHashSet
	ğŸ‘‰ LinkedHashSet is backed internally by a LinkedHashMap, not just HashMap.
		Simplified internal code idea:
				public class LinkedHashSet<E> extends HashSet<E> {

					public LinkedHashSet() {
						super(16, .75f, true);
					}
				}

		It uses: LinkedHashMap<E, Object>
		Instead of: HashMap<E, Object>

ğŸ”¥ How LinkedHashSet Maintains Insertion Order?
	The magic happens in LinkedHashMap.
	LinkedHashMap maintains:
		1ï¸âƒ£ A hash table (like HashMap)
		2ï¸âƒ£ A doubly linked list running through all entries
	Each entry stores: before, after references.

ğŸ”¥ Internal Entry Structure (Conceptually)
		In LinkedHashMap, each entry has:
			static class Entry<K,V> extends HashMap.Node<K,V> {
					Entry<K,V> before, after;
			}

		So besides bucket links, it maintains a linked list connecting all elements in insertion order.
	
ğŸ”¥ Insertion Flow: When you do:
		set.add("A");
		set.add("B");
		set.add("C");
		
		Internally:
			1ï¸âƒ£ Insert into hash table (like HashSet)
			2ï¸âƒ£ Append entry at end of doubly linked list

	Iteration follows: head â†’ A â†’ B â†’ C â†’ tail

ğŸ”¥ Performance Characteristics
| Operation  | Complexity |
| ---------- | ---------- |
| add()      | O(1)       |
| remove()   | O(1)       |
| contains() | O(1)       |
| iteration  | O(n)       |
Slightly slower than HashSet due to maintaining linked list.

ğŸ”¥ Memory Overhead
	LinkedHashSet uses more memory because:
		Each entry stores: hash, key, next (bucket link), before, after
		So extra 2 references compared to HashSet.

ğŸ”¥ Important Interview Insight
	LinkedHashSet maintains: ğŸ‘‰ Insertion order, NOT sorting order.
	If you need sorted order â†’ use TreeSet.

ğŸ”¥ Does LinkedHashSet Allow Access Order? No.
		But LinkedHashMap supports: new LinkedHashMap<>(16, 0.75f, true);
		Where: true = access-order (used for LRU cache)
		LinkedHashSet does NOT expose this directly.

=============================================================================================================
ğŸ”¹ Q27. What is TreeSet, How TreeSet maintains sorting?
=============================================================================================================
ğŸ”¥ TreeSet is a Set implementation that: Does NOT allow duplicates, Maintains elements in sorted order, Does NOT allow null (in natural ordering case), Provides O(log n) time complexity, It belongs to java.util package.

ğŸ“Œ Internal Implementation
	ğŸ‘‰ TreeSet is internally backed by a TreeMap.
		Simplified idea: private transient NavigableMap<E,Object> m;
		When you do: set.add("A");
		Internally: map.put("A", PRESENT);
		Exactly like HashSet uses HashMap, TreeSet uses TreeMap.

ğŸ”¥ What Data Structure Does TreeMap Use? ğŸ‘‰ Red-Black Tree (self-balancing binary search tree)
ğŸ”¥ How TreeSet Maintains Sorting?
	Sorting is maintained using:
			1ï¸âƒ£ Natural Ordering (Comparable)
				If elements implement Comparable:
					class Person implements Comparable<Person> {
					   public int compareTo(Person p) {
						   return this.age - p.age;
					   }
					}
				TreeSet automatically uses compareTo().
			
			2ï¸âƒ£ Custom Comparator: 
					TreeSet<Person> set = new TreeSet<>(new Comparator<Person>() {
					   public int compare(Person p1, Person p2) {
						   return p1.age - p2.age;
					   }
					});
				TreeSet uses compare() method.

ğŸ”¥ Internal Working During Insertion
		When you add an element:
			set.add(50);
			set.add(20);
			set.add(70);
		TreeSet:
			1ï¸âƒ£ Compares with root
			2ï¸âƒ£ Goes left or right
			3ï¸âƒ£ Inserts at correct position
			4ï¸âƒ£ Rebalances tree to maintain red-black properties
			So order is automatically maintained.

ğŸ”¥ Important Interview Insight
		In TreeSet:
			ğŸ‘‰ Duplicate detection is based on: compareTo() == 0, NOT equals()
				If compareTo() returns 0: Element is considered duplicate, Not inserted, Even if equals() returns false.

ğŸ”¥ Why TreeSet Does NOT Allow Null?
	Because: During insertion, it calls: compareTo()
		If element is null â†’ NullPointerException.
	There is no meaningful way to compare null values in sorting.

ğŸ”¥ Time Complexity
| Operation  | Complexity |
| ---------- | ---------- |
| add()      | O(log n)   |
| remove()   | O(log n)   |
| contains() | O(log n)   |
Slower than HashSet (O(1)), but maintains sorted order.

ğŸ”¥ When to Use TreeSet?
		âœ… When you need sorted data
		âœ… When you need range queries
		âœ… When you need first(), last(), higher(), lower()
	Not ideal for: âŒ Fast random lookup, âŒ High-performance hashing scenarios
	
=============================================================================================================
ğŸ”¹ Q28. Difference between Comparable and Comparator? What happens if compareTo returns 0?
=============================================================================================================
ğŸ“Œ 1ï¸âƒ£ Comparable: Interface in java.lang, Defines natural ordering, Implemented inside the class itself
		Method: int compareTo(T o);
		Example:
			class Person implements Comparable<Person> {
				int age;

				public int compareTo(Person p) {
					return this.age - p.age;
				}
			}
		Now objects are sorted by default using age.

ğŸ“Œ 2ï¸âƒ£ Comparator: Interface in java.util, Defines custom ordering, Implemented separately from the class
		Method: int compare(T o1, T o2);
		Example:
			Comparator<Person> nameComparator = (p1, p2) -> p1.name.compareTo(p2.name);
		Used like: new TreeSet<>(nameComparator);

ğŸ”¥ Comparison Table
| Feature                 | Comparable  | Comparator     |
| ----------------------- | ----------- | -------------- |
| Package                 | java.lang   | java.util      |
| Method                  | compareTo() | compare()      |
| Defined in              | Same class  | Separate class |
| Ordering                | Natural     | Custom         |
| Multiple sorting logic  | No          | Yes            |
| Modifies original class | Yes         | No             |

ğŸ”¥ What Happens If compareTo() Returns 0?
	In sorted collections like: TreeSet, TreeMap
	If: compareTo() == 0, ğŸ‘‰ The two elements are considered equal
	Even if: equals() == false
	ğŸ”¹ Example
			TreeSet<Integer> set = new TreeSet<>();
			set.add(10);
			set.add(10);
		Second insertion rejected because: compareTo() returns 0

	ğŸ”¹ Dangerous Case
				class Person implements Comparable<Person> {
					int age;
					String name;

					public int compareTo(Person p) {
						return this.age - p.age;
					}
				}
		Now:
			Person p1 = new Person(25, "John");
			Person p2 = new Person(25, "David");
			set.add(p1);
			set.add(p2);
		ğŸ‘‰Second one will NOT be inserted.
		Why? Because: compareTo() returns 0 (same age), Even though names are different.

ğŸ”¥ Important Interview Insight
	In TreeSet and TreeMap: ğŸ‘‰ Equality is based on comparison logic NOT equals()
		So if compareTo() returns 0: Element treated as duplicate, Not inserted

ğŸ”¥ Why This Is Dangerous? 
	If compareTo() is inconsistent with equals():
		Violates general contract, Leads to strange behavior
	Java recommends: compareTo should be consistent with equals.

=============================================================================================================
ğŸ”¹ Q30. TreeSet vs HashSet performance comparison
=============================================================================================================
ğŸ“Œ 1ï¸âƒ£ Internal Data Structure
	ğŸ”¹ HashSet: Backed by HashMap, Uses Hash table, Bucket-based storage
		Java 8+: Linked list â†’ Red-Black Tree (if collisions > 8)
	
	ğŸ”¹ TreeSet: Backed by TreeMap, Uses Red-Black Tree, Self-balancing binary search tree, Always sorted

2ï¸âƒ£ Time Complexity Comparison
| Operation       | HashSet      | TreeSet      |
| --------------- | ------------ | ------------ |
| add()           | O(1) average | O(log n)     |
| remove()        | O(1) average | O(log n)     |
| contains()      | O(1) average | O(log n)     |
| Worst case      | O(log n)*    | O(log n)     |
| Iteration order | No order     | Sorted order |

3ï¸âƒ£ Why HashSet Is Faster?
	Because: Uses hashing, Direct bucket lookup, No tree traversal, No sorting overhead
	Average lookup: O(1)

4ï¸âƒ£ Why TreeSet Is Slower?
	Because: Must maintain sorted order
	Every insert requires: Tree traversal, Rebalancing (Red-Black Tree), Comparison-based insertion
	So: O(log n)

ğŸ”¥ 5ï¸âƒ£ Memory Usage
| Collection | Memory Usage |
| ---------- | ------------ |
| HashSet    | Lower        |
| TreeSet    | Higher       |

6ï¸âƒ£ When to Use Which?
	âœ… Use HashSet When:
		You only need uniqueness, Performance is priority, Order does not matter, High throughput systems
		Example: Removing duplicates, Fast membership checks
	
	âœ… Use TreeSet When:
		You need sorted data, You need range queries, You need first(), last(), higher(), lower()
		Example: Leaderboard, Ranking system, Time-based ordering

7ï¸âƒ£ Real Production Insight
	In real-world systems:
		ğŸ‘‰ HashSet is used much more frequently
		ğŸ‘‰ TreeSet only when sorting is required
	Because: Sorting adds overhead, Tree traversal slower than hashing
	
=============================================================================================================
ğŸ”¹ Q31. How does HashMap internally work (Java 8+)? Difference between HashMap in Java 7 vs Java 8?
=============================================================================================================
1ï¸âƒ£ Core Data Structure
	HashMap uses: Array of Node<K,V>
		transient Node<K,V>[] table;
		Each bucket contains: Single node OR Linked list OR Red-Black Tree (Java 8+)

2ï¸âƒ£ What Happens During put(key, value)?
	Step 1ï¸âƒ£ â†’ Calculate Hash
			int hash = key.hashCode();
			hash = hash ^ (hash >>> 16); This spreads higher bits to reduce collision.
	
	Step 2ï¸âƒ£ â†’ Find Bucket Index
			index = (n - 1) & hash;
			Where: n = table length (always power of 2)
	
	Step 3ï¸âƒ£ â†’ Insert Logic
			Case 1: Bucket empty: Insert directly.
			Case 2: Bucket not empty: Check first node:
					if (key.equals(existingKey))
						replace value
					Else: Traverse linked list OR traverse tree (if treeified)
	
	Step 4ï¸âƒ£ â†’ Treeification (Java 8 Feature)
		If: Bucket size > 8 AND Table size â‰¥ 64 Linked list converts to: ğŸ‘‰ Red-Black Tree
		Improves worst-case performance.

3ï¸âƒ£ Resize Mechanism
	Triggered when: size > capacity * loadFactor
	Default: Initial capacity = 16
	Load factor = 0.75
	Resize: Capacity doubles
	Rehashing occurs. Nodes redistributed

ğŸ”¥ Why Load Factor 0.75?
		Balance between: Memory usage, Performance
		Higher load factor â†’ fewer resizes but more collisions
		Lower load factor â†’ more memory but fewer collisions

ğŸ”¥ Difference Between HashMap Java 7 vs Java 8
| Feature               | Java 7                         | Java 8                 |
| --------------------- | ------------------------------ | ---------------------- |
| Collision handling    | Linked List only               | Linked List â†’ Tree     |
| Worst-case complexity | O(n)                           | O(log n)               |
| Tree structure        | âŒ No                           | âœ… Yes                  |
| Hash function         | Basic spread                   | Improved bit spreading |
| Resize transfer       | Head insertion (reverse order) | Maintains order        |

ğŸ”¥ Java 7 Problem
	In Java 7: All collisions stored in linked list, Worst case â†’ O(n)
	Under heavy collision â†’ performance degradation
	In concurrent resize â†’ could cause infinite loop (rare bug)

ğŸ”¥ Java 8 Improvement
	Java 8 introduced: ğŸ‘‰ Tree bins (Red-Black Tree)
	When collision count > 8: Linked list â†’ converted to tree, Search becomes O(log n)
	When count drops below 6: Tree â†’ converted back to linked list

ğŸ”¥ Important Internal Classes (Java 8)
		static class Node<K,V>
		static final class TreeNode<K,V> extends LinkedHashMap.Entry<K,V>
		TreeNode represents Red-Black tree nodes.

ğŸ”¥ Time Complexity (Java 8)
| Operation | Average | Worst Case |
| --------- | ------- | ---------- |
| put()     | O(1)    | O(log n)   |
| get()     | O(1)    | O(log n)   |
| remove()  | O(1)    | O(log n)   |
Before Java 8 â†’ worst case O(n)

 
=============================================================================================================
ğŸ”¹ Q36. Can HashMap contain mutable keys? Why HashMap is not thread-safe?
=============================================================================================================
1ï¸âƒ£  Technically yes, but it is strongly discouraged.
	ğŸ‘‰ Mutable keys can break HashMap behavior.
	ğŸ” Why Is It Dangerous?
		HashMap uses:
			1ï¸âƒ£ hashCode() â†’ to determine bucket
			2ï¸âƒ£ equals() â†’ to compare keys inside bucket
			If key state changes after insertion and that state affects hashCode() or equals():ğŸ‘‰ The key becomes unreachable.

			ğŸ“Œ Example
				class Person {
					String name;

					public int hashCode() {
						return name.hashCode();
					}

					public boolean equals(Object o) {
						return this.name.equals(((Person)o).name);
					}
				}
			Now:
				Person p = new Person();
				p.name = "John";
				map.put(p, "Data");
				p.name = "David";  // Mutation
				map.get(p);  // âŒ returns null

			Step-by-step:
				1ï¸âƒ£ Insert â†’ bucket determined using hashCode("John")
				2ï¸âƒ£ After mutation â†’ hashCode("David")
				3ï¸âƒ£ get() searches wrong bucket
				4ï¸âƒ£ Key not found
			
			ğŸš¨ Consequences: containsKey() returns false, remove() fails, Memory leak (entry still exists but unreachable)
					Data corruption
			
			ğŸ”¥ Best Practice: ğŸ‘‰ Always use immutable keys in HashMap.
			Examples of good keys: String, Integer, UUID, Custom immutable objects

ğŸ”¥ 2ï¸âƒ£ Why HashMap Is NOT Thread-Safe?
	Because its internal structure is not synchronized. 
	Multiple threads modifying it concurrently can cause: Race conditions, Data corruption, Infinite loops (Java 7 resize bug), Lost updates
	
	ğŸ” Internal Reason
		HashMap has: Node<K,V>[] table; int size;
		Operations like put() involve:
				1ï¸âƒ£ Compute hash
				2ï¸âƒ£ Find bucket
				3ï¸âƒ£ Insert node
				4ï¸âƒ£ Update size
				5ï¸âƒ£ Possibly resize
		None of these steps are atomic.

	ğŸ“Œ Race Condition Example
		Thread 1: map.put("A", 1);
		Thread 2: map.put("B", 2);
		Possible issues: Both read same size, Both write to same bucket, Overwrite each other, Size becomes inconsistent
	
	ğŸ“Œ Resize Problem (Java 7 Classic Issue)
		During resize: Table doubles, Entries rehashed
		If two threads resize simultaneously: ğŸ‘‰ Bucket links can form a cycle, ğŸ‘‰ Infinite loop during traversal
		This was a serious concurrency bug in Java 7.

ğŸ”¥ Why Read Operations Also Not Safe?
	Even: map.get(key); 
	is unsafe during concurrent write because: table reference may change, structure may be inconsistent

ğŸ”¥ What to Use Instead?
| Scenario            | Use                           |
| ------------------- | ----------------------------- |
| Basic thread-safety | Collections.synchronizedMap() |
| High concurrency    | ConcurrentHashMap             |
| Read-heavy          | ConcurrentHashMap             |

=============================================================================================================
ğŸ”¹ Q37. Difference between HashMap and LinkedHashMap? How access-order works in LinkedHashMap?
=============================================================================================================
ğŸ”¥ Difference Between HashMap and LinkedHashMap
	1ï¸âƒ£ Internal Data Structure
		ğŸ”¹ HashMap: 
				Uses hash table
				Each bucket contains: Node, Linked list Or Red-Black Tree (Java 8+)
				No ordering maintained
		
		ğŸ”¹ LinkedHashMap:
				Extends HashMap
				Uses: Hash table (like HashMap), PLUS a doubly linked list connecting all entries
				This linked list maintains order.

	2ï¸âƒ£ Feature Comparison
	| Feature      | HashMap            | LinkedHashMap       |
	| ------------ | ------------------ | ------------------- |
	| Order        | No order           | Maintains order     |
	| Order Type   | â€”                  | Insertion or Access |
	| Performance  | Slightly faster    | Slightly slower     |
	| Memory usage | Lower              | Higher              |
	| Null allowed | 1 key, many values | Same                |

	3ï¸âƒ£ How LinkedHashMap Maintains Order?
		Each entry is linked in a doubly linked list: head â†’ entry1 â†’ entry2 â†’ entry3 â†’ tail
		When new entry added:
			Insert into bucket (like HashMap)
			Append to tail of linked list
			Iteration follows linked list order.
	
ğŸ”¥ What is Access-Order in LinkedHashMap?
	By default: new LinkedHashMap<>();
	Maintains: ğŸ‘‰ Insertion Order
	But if you use: new LinkedHashMap<>(16, 0.75f, true);
		The third parameter = accessOrder
	If true: ğŸ‘‰ Maintains Access Order

ğŸ”¥ How Access-Order Works
	When: map.get(key); That entry moves to the end of the linked list.
	Example:
		Initial insertion: A â†’ B â†’ C
		If you access: map.get("A");
		Order becomes: B â†’ C â†’ A
		Recently accessed moves to tail.

ğŸ”¥ Why Access-Order Is Important?
	Used for: ğŸ‘‰ LRU Cache (Least Recently Used), Oldest accessed entry is at head.

ğŸ”¥ LRU Implementation Example
		LinkedHashMap<String, String> lru =  new LinkedHashMap<>(16, 0.75f, true) {
				protected boolean removeEldestEntry(Map.Entry eldest) {
					return size() > 100;
				}
			};
		This automatically removes least recently used entry.

ğŸ”¥ Performance Consideration
| Operation | HashMap | LinkedHashMap            |
| --------- | ------- | ------------------------ |
| put()     | O(1)    | O(1)                     |
| get()     | O(1)    | O(1)                     |
| remove()  | O(1)    | O(1)                     |
| Iteration | O(n)    | O(n) (predictable order) |

LinkedHashMap slightly slower due to linked list maintenance.

=============================================================================================================
ğŸ”¹ Q38. How to implement LRU cache using LinkedHashMap? Real-time use cases of LinkedHashMap?
=============================================================================================================
ğŸ”¥ How to Implement LRU Cache Using LinkedHashMap
	ğŸ“Œ Why LinkedHashMap? 
		Because it supports: new LinkedHashMap<>(capacity, loadFactor, true); The third parameter = accessOrder
		When true: Recently accessed entries move to the end, Least recently used stays at the head
		Perfect for LRU logic.

ğŸ”¥ Basic LRU Implementation
				import java.util.LinkedHashMap;
				import java.util.Map;

				public class LRUCache<K, V> extends LinkedHashMap<K, V> {

					private final int capacity;

					public LRUCache(int capacity) {
						super(capacity, 0.75f, true);  // accessOrder = true
						this.capacity = capacity;
					}

					@Override
					protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
						return size() > capacity;
					}
					
					public V getValue(K key) {
						return super.get(key);
					}

					public void putValue(K key, V value) {
						super.put(key, value);
					}
				}

	ğŸ”¥ How It Works Internally
		1ï¸âƒ£ Insertion: Entry added to hash table, Appended to tail of doubly linked list
		2ï¸âƒ£ Access (get): Entry moved to tail (most recently used)
		3ï¸âƒ£ When size exceeds capacity: removeEldestEntry() removes head (least recently used)

	ğŸ”¥ Example Usage
		LRUCache<String, String> cache = new LRUCache<>(3);
		cache.put("A", "Apple");
		cache.put("B", "Banana");
		cache.put("C", "Cherry");
		cache.get("A");   // A becomes most recently used
		cache.put("D", "Date");  // Removes B (least recently used)

	ğŸ”¥ Time Complexity
| Operation | Complexity |
| --------- | ---------- |
| get()     | O(1)       |
| put()     | O(1)       |
| remove()  | O(1)       |

âš  Important: Thread-Safety: LinkedHashMap is NOT thread-safe.
	For multi-threaded usage: Use Collections.synchronizedMap() Or use ConcurrentHashMap + custom LRU logic
	Or use libraries like Caffeine (production-grade)

ğŸ”¥ Real-Time Use Cases of LinkedHashMap
	1ï¸âƒ£ LRU Cache: API response caching, DB query caching, Session storage, Recently viewed products
	2ï¸âƒ£ Preserving Insertion Order
		Example: Configuration loading, Maintaining predictable iteration, JSON parsing order preservation
	
	3ï¸âƒ£ Access-Order Based Tracking: Tracking recently accessed users, Rate limiting systems, Analytics (top recent events)
	4ï¸âƒ£ Lightweight In-Memory Cache
		Small microservices where: No distributed caching required, Simple eviction logic sufficient
	
	5ï¸âƒ£ Framework Usage
		Used internally in: Spring, Hibernate, Various caching mechanisms

ğŸ”¥ Why Not Always Use LinkedHashMap for Cache?
	Because: âŒ Not thread-safe, âŒ No expiry time support, âŒ No size-based eviction tuning, âŒ No statistics

For production caching: Use Caffeine, Use Redis, Use EHCache

=============================================================================================================
ğŸ”¹ Q39. How TreeMap internally works? TreeMap vs HashMap vs LinkedHashMap?
=============================================================================================================
ğŸ”¥ How TreeMap Internally Works
ğŸ“Œ 1ï¸âƒ£ Core Data Structure
		TreeMap is implemented using a: ğŸ‘‰ Red-Black Tree (self-balancing binary search tree)
		Internally:
					static final class Entry<K,V> {
						K key;
						V value;
						Entry<K,V> left;
						Entry<K,V> right;
						Entry<K,V> parent;
						boolean color;  // RED or BLACK
					}

			It maintains: transient Entry<K,V> root;

ğŸ”¥ 2ï¸âƒ£ How Insertion Works
		When you do: map.put(key, value);
			Step 1ï¸âƒ£ â†’ Comparison 
				TreeMap uses: compareTo() (if key implements Comparable) OR Provided Comparator
				To determine: Go left (if smaller), Go right (if larger)
			
			Step 2ï¸âƒ£ â†’ Insert as leaf: Insert at correct BST position.
			Step 3ï¸âƒ£ â†’ Rebalance Tree: 
				Red-Black Tree rules enforced: 
					Root must be black, No two red nodes in a row
					Equal black height on all paths, Rotations and recoloring are done.

ğŸ”¥ 3ï¸âƒ£ How Search Works
	Search follows BST traversal: Compare â†’ go left or right
	Time complexity: O(log n)
	
ğŸ”¥ Why TreeMap Does NOT Use hashCode?
	Because: No buckets, No hashing, Position decided by comparison

ğŸ”¥ TreeMap vs HashMap vs LinkedHashMap
| Feature         | HashMap    | LinkedHashMap                   | TreeMap        |
| --------------- | ---------- | ------------------------------- | -------------- |
| Data structure  | Hash table | Hash table + Doubly linked list | Red-Black Tree |
| Ordering        | No order   | Insertion / Access order        | Sorted order   |
| Time complexity | O(1) avg   | O(1) avg                        | O(log n)       |
| Null key        | 1 allowed  | 1 allowed                       | âŒ Not allowed  |
| Memory usage    | Lower      | Medium                          | Higher         |

ğŸ”¥ When to Use Which?
	âœ… Use HashMap: Fast lookup required, Order doesnâ€™t matter, High throughput systems
		Example: Caching, Indexing, Frequency counting
	
	âœ… Use LinkedHashMap: Need predictable iteration order, Implementing LRU cache, Config loading where order matters
	
	âœ… Use TreeMap: Need sorted data, Range queries, firstKey(), lastKey(), ceilingKey(), Leaderboards / ranking systems

ğŸ”¥ Performance Insight
	HashMap fastest (O(1))
	TreeMap slower (O(log n)) because tree traversal
	LinkedHashMap slightly slower than HashMap due to list maintenance

ğŸ”¥ What Happens If compareTo() Returns 0 in TreeMap?
	If compareTo() returns 0, the two keys are considered equal in TreeMap.
		ğŸ‘‰ The new value replaces the existing value. The key is NOT inserted as a new entry.
	
	ğŸ“Œ Why? : TreeMap is backed by a Red-Black Tree (BST).
		Insertion logic:
			1ï¸âƒ£ Compare new key with existing node
			2ï¸âƒ£ If result < 0 â†’ go left
			3ï¸âƒ£ If result > 0 â†’ go right
			4ï¸âƒ£ If result == 0 â†’ treat as duplicate key
		So equality in TreeMap is determined by: compareTo() == 0, NOT by equals().
		
=============================================================================================================
ğŸ”¹ Q41. How Comparator affects TreeMap behavior?
=============================================================================================================
1ï¸âƒ£ TreeMap Uses Comparator for Ordering
	When you create a TreeMap: TreeMap<K,V> map = new TreeMap<>(comparator);
	TreeMap will:
		ğŸ‘‰ Use comparator.compare(k1, k2), NOT use compareTo(), NOT use hashCode(), NOT use equals()
	
	If no comparator is provided: It uses natural ordering (Comparable)

2ï¸âƒ£ Comparator Controls Sorting Order
	Example: TreeMap<Integer, String> map = new TreeMap<>((a, b) -> b - a);  // descending order
	Now: Keys sorted in reverse order
	Comparator completely defines how keys are arranged in tree.

3ï¸âƒ£ Comparator Defines Key Equality
	In TreeMap: compare(k1, k2) == 0
	ğŸ‘‰ Means keys are considered equal, Second insertion replaces first
		Even if: k1.equals(k2) == false
	Example:
		Comparator<String> comp = (a, b) -> a.length() - b.length();
		TreeMap<String, String> map = new TreeMap<>(comp);
		map.put("AA", "First");
		map.put("BB", "Second");
		
		Both length = 2, Comparator returns 0
		Result: "BB" replaces "AA", Only one entry exists.

4ï¸âƒ£ If Comparator Is Inconsistent with equals()
	You get strange behavior: Duplicates disappear, Lookups behave unexpectedly, containsKey() may behave strangely
	Java recommends: Comparator should be consistent with equals.
	Meaning: compare(a,b) == 0 â†’ a.equals(b) should be true

5ï¸âƒ£ If Comparator Violates Transitivity
	Example:
		A > B
		B > C
		C > A   âŒ
	
	Tree becomes invalid. Possible outcomes: Infinite loops, IllegalArgumentException, Corrupted tree structure

6ï¸âƒ£ Performance Impact
	Comparator is called during: Every insertion, Every lookup, Every removal
	So: ğŸ‘‰ Slow comparator â†’ slow TreeMap
	Example: Heavy string processing, Complex DB calls inside comparator (bad practice)

ğŸ”¥ Summary Table
| Behavior            | Controlled By Comparator |
| ------------------- | ------------------------ |
| Sorting order       | Yes                      |
| Duplicate detection | Yes                      |
| Equality in TreeMap | Yes                      |
| Tree structure      | Yes                      |
| Performance         | Yes                      |

=============================================================================================================
ğŸ”¹ Q42. Why ConcurrentHashMap is faster than Hashtable? Internal working of ConcurrentHashMap (Java 8)?
=============================================================================================================
ğŸ”¥ Why ConcurrentHashMap Is Faster Than Hashtable?
	ğŸ“Œ 1ï¸âƒ£ Hashtable Uses Coarse-Grained Locking
			In Hashtable, every method is: public synchronized V put(K key, V value)
			That means: Entire map locked for every operation, Only one thread can access at a time
			Result: High contention, Poor scalability, Low throughput
	
	ğŸ“Œ 2ï¸âƒ£ ConcurrentHashMap Uses Fine-Grained Locking
			Instead of locking entire map: Locks only bucket (bin) level, Uses CAS for many operations, Allows concurrent reads and writes
			Result: Better parallelism, Higher throughput, Reduced contention

ğŸ”¥ Internal Working of ConcurrentHashMap (Java 8)
	Java 8 significantly changed CHM.
	ğŸ“Œ 1ï¸âƒ£ Data Structure: transient volatile Node<K,V>[] table;
		Same idea as HashMap: Array of buckets
		Each bucket contains: Node OR TreeNode (Red-Black Tree if collisions high)
	
	ğŸ“Œ 2ï¸âƒ£ No Segments (Java 8 Change)
		Java 7:
			Used: Segment[] (each segment was separate lock)
		Java 8: Removed segments.
			Now: Single table, Lock per bucket, Uses CAS

ğŸ”¥ How put() Works in Java 8 CHM
	Step 1ï¸âƒ£ â†’ Calculate hash: int hash = spread(key.hashCode());
	Step 2ï¸âƒ£ â†’ Find bucket: index = (n - 1) & hash;
	Step 3ï¸âƒ£ â†’ Insert Logic
		Case 1: Bucket empty
			Uses CAS: casTabAt(table, index, null, new Node), No locking needed.
		
		Case 2: Bucket not empty
			Lock only that bucket: synchronized (firstNode), Then: Traverse linked list OR tree

ğŸ“Œ 4ï¸âƒ£ Treeification
	Like HashMap: If bucket size > 8, Convert to Red-Black Tree, Improves worst-case from O(n) to O(log n)

ğŸ”¥ 5ï¸âƒ£ Why Reads Are Fast?
	get() method: Does NOT lock, Uses volatile reads, Safe due to memory visibility guarantees
				  Very high performance for read-heavy systems.

ğŸ”¥ 6ï¸âƒ£ Why ConcurrentHashMap Does NOT Allow Null?
	Because:
		If: map.get(key) == null, We cannot distinguish: Key not present OR Key mapped to null
		In concurrent environment, this ambiguity is dangerous.
		So null keys and values are not allowed.

ğŸ”¥ 7ï¸âƒ£ Resize in ConcurrentHashMap
	Resize is: Cooperative, Multiple threads can help transfer, Uses ForwardingNode to mark moved bins, Very sophisticated implementation.

ğŸ”¥ Time Complexity
| Operation  | Complexity |
| ---------- | ---------- |
| get()      | O(1)       |
| put()      | O(1)       |
| remove()   | O(1)       |
| Worst case | O(log n)   |

ğŸ”¥ Summary Table
| Feature          | Hashtable  | ConcurrentHashMap |
| ---------------- | ---------- | ----------------- |
| Locking          | Entire map | Bucket-level      |
| Reads            | Locked     | Non-blocking      |
| Write contention | High       | Low               |
| Null allowed     | No         | No                |
| Performance      | Slow       | High              |


=============================================================================================================
ğŸ”¹ Q43. Difference between synchronizedMap and ConcurrentHashMap?
=============================================================================================================
ğŸ”¥ Difference Between synchronizedMap and ConcurrentHashMap
ğŸ“Œ 1ï¸âƒ£ What is Collections.synchronizedMap()?
		Example: Map<String, String> map = Collections.synchronizedMap(new HashMap<>());
		This is: A wrapper around HashMap, Entire map is synchronized on a single lock
		
		Internally:
					synchronized (mutex) {
						return m.put(key, value);
					}
		So: Every operation acquires same lock

ğŸ“Œ 2ï¸âƒ£ What is ConcurrentHashMap?
		Example: ConcurrentHashMap<String, String> map = new ConcurrentHashMap<>();
		Internally: Uses bucket-level locking, Uses CAS for many operations, Non-blocking reads, No global lock

ğŸ”¥ Detailed Comparison Table
| Feature           | synchronizedMap      | ConcurrentHashMap |
| ----------------- | -------------------- | ----------------- |
| Locking           | Whole map            | Bucket-level      |
| Read operations   | Locked               | Non-blocking      |
| Write concurrency | Single-threaded      | Multi-threaded    |
| Performance       | Lower                | Higher            |
| Iterators         | Fail-fast            | Weakly consistent |
| Null keys         | Allowed (if HashMap) | âŒ Not allowed     |

ğŸ”¥ 3ï¸âƒ£ Iterator Behavior Difference
	ğŸ”¹ synchronizedMap
		Iterator is: Fail-fast, Must manually synchronize while iterating
		
		Example:
				synchronized(map) {
				   for (String key : map.keySet()) { }
				}
		If not â†’ risk of ConcurrentModificationException

	ğŸ”¹ ConcurrentHashMap
		Iterator is: Weakly consistent, Does NOT throw ConcurrentModificationException
				Reflects state at some point during iteration, Better for concurrent systems.

ğŸ”¥ Why ConcurrentHashMap Does NOT Allow Null?
		Imagine: map.get(key); If result is null, what does it mean?
		1ï¸âƒ£ Key not present OR 2ï¸âƒ£ Key present but value is null
		In single-threaded HashMap: You can check containsKey()
		In concurrent environment: Between get() and containsKey(), Another thread may remove entry
		This creates ambiguity.

	ğŸ”¥ Example Problem
			Thread 1: map.get("A") â†’ null
			Thread 2: map.remove("A")
			Now: Is key absent? Or value was null?
			Ambiguity is unsafe in concurrent systems.
			So design decision: Disallow null keys and values.

ğŸ”¥ Why synchronizedMap Allows Null?
	Because: It's just HashMap wrapped, Does not solve ambiguity problem, Does not guarantee atomic check-then-act

ğŸ”¥ Performance Insight
	synchronizedMap: Single lock â†’ bottleneck, Poor scalability under high concurrency
	ConcurrentHashMap: Fine-grained locking, CAS-based operations, Better scalability
	
=============================================================================================================
ğŸ”¹ Q44. How thread-safety is achieved in ConcurrentHashMap? What is CAS in ConcurrentHashMap?
=============================================================================================================
ğŸ”¥ How Thread-Safety Is Achieved in ConcurrentHashMap (Java 8)
	Java 8 redesigned ConcurrentHashMap. 
	It achieves thread-safety using:
		1ï¸âƒ£ Volatile variables
		2ï¸âƒ£ CAS (Compare-And-Swap)
		3ï¸âƒ£ Fine-grained locking (bin-level locking)
		4ï¸âƒ£ Tree bins for high collision
		5ï¸âƒ£ Cooperative resizing

ğŸ“Œ 1ï¸âƒ£ Internal Data Structure
	transient volatile Node<K,V>[] table;
	Important: table is volatile, Ensures visibility across threads
	Each bucket (bin) contains: Node (linked list) OR TreeNode (Red-Black tree)

ğŸ“Œ 2ï¸âƒ£ Read Operations (get)
	map.get(key); Completely non-blocking, No synchronization, Uses volatile reads
	Why safe? table is volatile, Node fields are volatile, Memory visibility guaranteed, This makes reads extremely fast.

ğŸ“Œ 3ï¸âƒ£ Write Operations (put)
		Case A: Bucket is empty
				Uses CAS: casTabAt(table, index, null, new Node(...)), No locking required.
		Case B: Bucket is not empty
				It locks only that specific bin: synchronized (firstNode)
				So: Other buckets remain accessible, Only that bucket is locked, This is called fine-grained locking.

ğŸ“Œ 4ï¸âƒ£ Treeification
		If bucket size > 8: Linked list â†’ Red-Black Tree, Improves worst-case performance
		Still thread-safe: Bin-level synchronization applied

ğŸ“Œ 5ï¸âƒ£ Resize Mechanism
	Resize is: Multi-threaded, Cooperative, Uses ForwardingNode, Multiple threads help transfer entries, Very advanced implementation.

ğŸ”¥ What is CAS in ConcurrentHashMap?
	CAS = Compare-And-Swap, It is a CPU-level atomic instruction.
	ğŸ“Œ How CAS Works
		CAS does: If (currentValue == expectedValue) update to newValue, Else do nothing
		All done atomically.

		ğŸ“Œ Example (Conceptual)
			if (table[index] == null) {
				table[index] = newNode;
			}
		In concurrent world this is unsafe.
		Instead CHM does: casTabAt(table, index, null, newNode)
		Meaning: Insert only if bucket still null, If another thread inserted â†’ CAS fails, Retry, No locking required.

	ğŸ“Œ Why CAS Is Powerful: Lock-free, Non-blocking, Avoids thread contention, Improves scalability
		Used in: AtomicInteger, ConcurrentHashMap, Lock-free algorithms

=============================================================================================================
ğŸ”¹ Q45. Different ways to iterate a collection? Difference between Iterator and ListIterator?
=============================================================================================================
ğŸ”¥ Different Ways to Iterate a Collection
ğŸ“Œ 1ï¸âƒ£ Enhanced For-Each Loop
				for (String s : list) {
					System.out.println(s);
				}
		Internally converted to:
				Iterator<String> it = list.iterator();
				while (it.hasNext()) {
					String s = it.next();
				}

		ğŸ‘‰ Uses Iterator internally, Fail-fast

ğŸ“Œ 2ï¸âƒ£ Using Iterator
			Iterator<String> it = list.iterator();
			while (it.hasNext()) {
				String value = it.next();
			}
		ğŸ‘‰ Works for all Collection types, Supports remove()

ğŸ“Œ 3ï¸âƒ£ Using ListIterator (Only for List)
			ListIterator<String> it = list.listIterator();
			while (it.hasNext()) {
				System.out.println(it.next());
			}

		ğŸ‘‰ Supports bidirectional traversal, Can add(), set(), remove()

ğŸ“Œ 4ï¸âƒ£ Using Traditional For Loop (Index-Based)
			for (int i = 0; i < list.size(); i++) {
				System.out.println(list.get(i));
			}

		ğŸ‘‰ Best for ArrayList, ğŸš« Slow for LinkedList (O(nÂ²))

ğŸ“Œ 5ï¸âƒ£ Using forEach() Method (Java 8+)
			list.forEach(e -> System.out.println(e));
			Uses internal iterator.

ğŸ“Œ 6ï¸âƒ£ Using Stream API
			list.stream().forEach(System.out::println);
		Or parallel:
			list.parallelStream().forEach(System.out::println);

ğŸ“Œ 7ï¸âƒ£ Using Spliterator (Advanced)
			Spliterator<String> sp = list.spliterator();
			sp.forEachRemaining(System.out::println);
			Used internally by Streams.
			
ğŸ”¥ Difference Between Iterator and ListIterator
| Feature   | Iterator                    | ListIterator                                               |
| --------- | --------------------------- | ---------------------------------------------------------- |
| Package   | java.util                   | java.util                                                  |
| Works for | All Collections             | Only List                                                  |
| Direction | Forward only                | Forward & Backward                                         |
| Methods   | hasNext(), next(), remove() | hasNext(), next(), hasPrevious(), previous(), add(), set() |

ğŸ“Œ 2ï¸âƒ£ Traversal Capability
	Iterator: A â†’ B â†’ C â†’ D , Only forward.
	ListIterator: A â‡„ B â‡„ C â‡„ D ,Forward and backward.

ğŸ“Œ 3ï¸âƒ£ Modification Capability
| Operation | Iterator | ListIterator |
| --------- | -------- | ------------ |
| remove()  | Yes      | Yes          |
| add()     | No       | Yes          |
| set()     | No       | Yes          |

ğŸ“Œ 4ï¸âƒ£ Starting Position
		Iterator: Always starts at beginning.
		ListIterator: list.listIterator(2); Can start at specific index.
	
ğŸ”¥ Fail-Fast Behavior
	Both: Are fail-fast, Throw ConcurrentModificationException If collection structurally modified outside iterator.

ğŸ”¥ When to Use What?
	âœ… Use Iterator: Generic collection traversal, Removing elements safely
	âœ… Use ListIterator: Need reverse traversal, Need add() during iteration, Need index-based traversal
	
=============================================================================================================
ğŸ”¹ Q46. Why remove() from collection during iteration causes exception? How to safely remove elements while iterating?
=============================================================================================================
ğŸ”¥ Why Removing from Collection During Iteration Causes Exception?
		When you iterate like this:
			for (String s : list) {
				list.remove(s);   // âŒ
			}
		You get: ConcurrentModificationException
		ğŸ“Œ Reason: Fail-Fast Mechanism
		Most Java collections (ArrayList, HashMap, HashSet, etc.) are: Fail-Fast
		They detect structural modification during iteration.

	ğŸ” How Fail-Fast Works Internally
		Collections maintain a variable: modCount
		Every structural modification: add(), remove(), clear(), resize() ğŸ‘‰ Increments modCount
		When Iterator Is Created
			Iterator stores: expectedModCount = modCount;
			During iteration:
				if (modCount != expectedModCount)
					throw ConcurrentModificationException;
					
ğŸ”¥ What is Structural Modification?
	Structural modification means: Changing size, Adding or removing elements, Rehashing
	Not structural: Updating value via set(), Replacing existing value in Map

ğŸ”¥ Why Direct remove() Causes Exception?
	Example: 
			Iterator<String> it = list.iterator();
			while (it.hasNext()) {
				String s = it.next();
				list.remove(s);   // âŒ
			}
		Because: list.remove() increments modCount, Iterator's expectedModCount not updated, Mismatch detected, Exception thrown

ğŸ”¥ How to Safely Remove Elements While Iterating?
	âœ… 1ï¸âƒ£ Use Iteratorâ€™s remove()
			Iterator<String> it = list.iterator();
			while (it.hasNext()) {
				String s = it.next();
				if (s.equals("A")) {
					it.remove();  // âœ… Safe
				}
			}
		Why safe? Because: Iterator.remove() updates expectedModCount,No mismatch occurs

	âœ… 2ï¸âƒ£ Use removeIf() (Java 8+)
			list.removeIf(s -> s.equals("A"));
		Internally safe.

	âœ… 3ï¸âƒ£ Collect to Separate List
			List<String> toRemove = new ArrayList<>();
			for (String s : list) {
				if (condition) {
					toRemove.add(s);
				}
			}
			list.removeAll(toRemove);
	
	âœ… 4ï¸âƒ£ Use Concurrent Collections
			If multi-threaded: Use ConcurrentHashMap, Use CopyOnWriteArrayList
			These use: Weakly consistent iterators, No ConcurrentModificationException

ğŸ”¥ Important Interview Insight
	Fail-fast is: Best-effort mechanism, Not guaranteed, Used to detect programming bugs, It is NOT thread-safety mechanism.
	
=============================================================================================================
ğŸ”¹ Q47. Why Iterator.remove() Works But for-each Cannot?
=============================================================================================================
Because in for-each: You donâ€™t have reference to iterator, You cannot call iterator.remove() directly
	To safely remove:
		Iterator<String> it = list.iterator();
		while (it.hasNext()) {
			String s = it.next();
			if (condition) {
				it.remove();  // âœ… safe
			}
		}

ğŸ”¥ Why This Design? Fail-fast is designed to: Detect programmer errors early, Avoid inconsistent traversal, Prevent subtle concurrency bugs

ğŸ”¥ For Arrays â€“ Special Case
	For arrays: for (int x : arr) Compiler converts to: for (int i = 0; i < arr.length; i++) No iterator involved.

=============================================================================================================
ğŸ”¹ Q50. Internal working of CopyOnWriteArrayList? 
=============================================================================================================
ğŸ”¥ What is CopyOnWriteArrayList?
	CopyOnWriteArrayList is a thread-safe list implementation where Reads are lock-free, Writes create a new copy of the array, It belongs to java.util.concurrent.

ğŸ”¥ Internal Data Structure
	Internally it maintains:
		private transient volatile Object[] array;
	
	Important: Backed by array (like ArrayList), Array reference is volatile, Ensures visibility across threads

ğŸ”¥ Core Idea: Copy-On-Write
	When you perform: list.add("A");
	It does:
		1ï¸âƒ£ Acquire lock
		2ï¸âƒ£ Create new array (oldSize + 1)
		3ï¸âƒ£ Copy old elements
		4ï¸âƒ£ Add new element
		5ï¸âƒ£ Replace old array reference
		6ï¸âƒ£ Release lock
	Old array remains untouched.
	
ğŸ”¥ How Read Operations Work
		list.get(index);
	It simply reads from: array[index]; No locking.
	Because: Array reference is volatile, Visibility guaranteed, No structural modification on read

ğŸ”¥ Locking Strategy
	Writes use: ReentrantLock
	Simplified internal flow:
				lock.lock();
				try {
				   Object[] newArray = Arrays.copyOf(oldArray, oldArray.length + 1);
				   newArray[last] = element;
				   array = newArray;
				} finally {
				   lock.unlock();
				}

ğŸ”¥ Iterator Behavior (Very Important)
	Iterators are: Fail-safe, Snapshot-based
	When iterator is created: Object[] snapshot = array;
	Iterator iterates over snapshot.
	Even if list changes: Iterator continues safely, No ConcurrentModificationException

ğŸ”¥ Important Insight
	Iterator sees old state.
	Example:
		Iterator it = list.iterator();
		list.add("X");
	Iterator will NOT see "X".

ğŸ”¥ Performance Characteristics
| Operation | Complexity |
| --------- | ---------- |
| get()     | O(1)       |
| add()     | O(n)       |
| remove()  | O(n)       |
| iteration | O(n)       |

ğŸ”¥ When Is It Suitable: Best for: Read-heavy systems, Event listeners, Observer patterns, Subscriber lists, Rare writes
ğŸ”¥ When NOT Suitable: âŒ Write-heavy systems, Large lists, Memory-sensitive environments
	Because: Every write copies entire array, High memory churn, GC pressure

ğŸ”¥ Why No ConcurrentModificationException?
	Because: No modCount, Iterator uses snapshot, Underlying array never modified in-place

ğŸ”¥ Real-World Example
	Used in: Event dispatch systems, Configuration lists, Plugin lists, Listeners in frameworks

=============================================================================================================
ğŸ”¹ Q51. How to make custom collection thread-safe?
=============================================================================================================
ğŸ”¥ There is no single way. You choose strategy based on:
	Read vs write ratio, Performance requirements, Memory constraints, Consistency needs

âœ… Approach 1: Use Synchronized Methods (Coarse-Grained Locking) Simplest approach.

		public class MySafeList<E> {
			private final List<E> list = new ArrayList<>();

			public synchronized void add(E e) {
				list.add(e);
			}

			public synchronized E get(int index) {
				return list.get(index);
			}

			public synchronized int size() {
				return list.size();
			}
		}
	Pros: Simple, Easy to implement
	Cons: Entire object locked, Low scalability, High contention
	
âœ… Approach 2: Use Explicit Lock (ReentrantLock) Better control.
		import java.util.concurrent.locks.ReentrantLock;
		public class MySafeList<E> {
			private final List<E> list = new ArrayList<>();
			private final ReentrantLock lock = new ReentrantLock();

			public void add(E e) {
				lock.lock();
				try {
					list.add(e);
				} finally {
					lock.unlock();
				}
			}
		}
	
	Pros: More flexible, Fair locking option, TryLock support
	Cons: More complex

âœ… Approach 3: Use ReadWriteLock (Best for Read-Heavy Systems)
		import java.util.concurrent.locks.ReentrantReadWriteLock;
		public class MySafeList<E> {

			private final List<E> list = new ArrayList<>();
			private final ReentrantReadWriteLock lock =	new ReentrantReadWriteLock();

			public void add(E e) {
				lock.writeLock().lock();
				try {
					list.add(e);
				} finally {
					lock.writeLock().unlock();
				}
			}

			public E get(int index) {
				lock.readLock().lock();
				try {
					return list.get(index);
				} finally {
					lock.readLock().unlock();
				}
			}
		}
	Pros: Multiple readers allowed, Better scalability
	Cons: More complex logic

âœ… Approach 4: Use Concurrent Data Structures Internally
	Instead of building from scratch: 
				private final ConcurrentHashMap<K,V> map;
			OR
				private final CopyOnWriteArrayList<E> list;
		
		Let Java handle concurrency. Best approach in most cases.

âœ… Approach 5: Use Immutable Design
	Make collection immutable: No modification allowed, Safe by design
	Example:
		List<E> list = List.copyOf(original);
		Immutable collections are naturally thread-safe.

ğŸ”¥ Advanced Strategy: Lock-Free (Using CAS)
	Very advanced.
	Use: AtomicReference, CAS operations
	Example idea:
		AtomicReference<List<E>> ref;
	Mostly used in high-performance systems.

ğŸ”¥ Important Considerations
	When designing custom thread-safe collection:
		1ï¸âƒ£ Avoid exposing internal mutable references
		2ï¸âƒ£ Protect iteration logic
		3ï¸âƒ£ Handle compound operations carefully
		
		Example:
			if (!map.containsKey(k)) {
				map.put(k, v);   // Race condition
			}
		
		Better: map.putIfAbsent(k, v);

ğŸ”¥ Common Mistake
	Just synchronizing add() and get() is not enough if you have compound operations.
	Example:
		public synchronized boolean contains(E e) {
			return list.contains(e);
		}
	
	If caller does: 
		if (!list.contains(e)) {
			list.add(e);
		}
	Still not atomic.

=============================================================================================================
ğŸ”¹ Q52. Difference between immutable and unmodifiable collections? How Collections.unmodifiableList works internally?
=============================================================================================================
ğŸ”¥ Difference Between Immutable and Unmodifiable Collections
	ğŸ“Œ 1ï¸âƒ£ Immutable Collection : Cannot be modified after creation, No structural changes allowed, Backing data cannot change
	Examples:
		List.of(...) (Java 9+), Set.of(...), Map.of(...)
		Once created: No add(), No remove(), No update(), Underlying data fixed

	ğŸ“Œ 2ï¸âƒ£ Unmodifiable Collection: 
		 Read-only view of another collection
		 Structural modification not allowed through wrapper
		 But underlying collection CAN change
		Example:
			List<String> original = new ArrayList<>();
			original.add("A");
			List<String> unmodifiable = Collections.unmodifiableList(original);
		
		If: original.add("B"); Then: unmodifiable
		Also reflects "B". Because: Itâ€™s just a wrapper.

ğŸ”¥ Key Differences Table
| Feature                  | Immutable   | Unmodifiable                   |
| ------------------------ | ----------- | ------------------------------ |
| Can modify structure?    | âŒ No        | âŒ No (via wrapper)             |
| Underlying data changes? | âŒ No        | âœ… Yes                          |
| Thread-safe by design?   | Usually yes | Not necessarily                |
| Defensive copy required? | No          | Often yes                      |
| Example                  | List.of()   | Collections.unmodifiableList() |

ğŸ”¥ How Collections.unmodifiableList() Works Internally
	When you call: List<String> list = Collections.unmodifiableList(original);
	Java creates: new Collections.UnmodifiableList<>(original);

ğŸ“Œ Internal Structure (Simplified)
		static class UnmodifiableList<E> 
				extends UnmodifiableCollection<E>
				implements List<E> {

			final List<? extends E> list;

			UnmodifiableList(List<? extends E> list) {
				this.list = list;
			}

			public E get(int index) {
				return list.get(index);
			}

			public E set(int index, E element) {
				throw new UnsupportedOperationException();
			}

			public void add(int index, E element) {
				throw new UnsupportedOperationException();
			}

			public E remove(int index) {
				throw new UnsupportedOperationException();
			}
		}

ğŸ“Œ Important Points: Delegates read methods to original list, Overrides modifying methods, Throws UnsupportedOperationException

ğŸ”¥ Why It Is Not Truly Immutable?
	Because: original.add("C"); Changes visible in wrapper.
	So it is: A read-only view, Not a defensive copy

ğŸ”¥ If You Want True Immutability
	Use: 
		List<String> immutable = List.copyOf(original);
	Or:
		List<String> immutable = List.of("A", "B");

	These create: Independent copy, No backing reference

ğŸ”¥ Thread-Safety Insight
	Immutable collections: Naturally thread-safe, No synchronization required
	Unmodifiable wrapper: Not thread-safe if original list is mutable and shared
	
ğŸ”¥ Why Unmodifiable Collection Can Be Modified Indirectly?
	When you create:
		List<String> original = new ArrayList<>();
		original.add("A");
		List<String> unmodifiable =     Collections.unmodifiableList(original);
		
	What happens? unmodifiable is just a wrapper, It holds a reference to original, It does NOT create a copy

ğŸ“Œ If Original Changes: 
	original.add("B");
	System.out.println(unmodifiable);
	Output: [A, B]
	
	Even though: unmodifiable.add("C"); // âŒ UnsupportedOperationException
	So: 
		Direct modification via wrapper â†’ âŒ Not allowed
		Indirect modification via original â†’ âœ… Allowed

ğŸ”¥ So What Is Actually Protected?
	unmodifiableList protects: add(), remove(), clear(), set()
	But it does NOT protect: Changes in original collection, Mutation of elements inside collection

ğŸ”¥ How to Prevent Indirect Modification?
	âœ… Option 1: Defensive Copy
		List<String> safe = Collections.unmodifiableList(new ArrayList<>(original));
		Now: Wrapper over new copy, Original changes wonâ€™t affect safe list

	âœ… Option 2: Use Immutable Collection (Java 9+)
			List<String> immutable = List.copyOf(original);
		OR
			List<String> immutable = List.of("A", "B");
		These are truly immutable.

ğŸ”¥ Key Interview Insight
	Unmodifiable â‰  Immutable
	Unmodifiable: Read-only view, Backing data can change
	Immutable: Data cannot change, No reference to mutable structure
	
=============================================================================================================
ğŸ”¹ Q55. Choose correct collection for: High read, low write, Frequent insert/delete, Sorted data, Thread-safe access
=============================================================================================================
1ï¸âƒ£ High Read, Low Write
	âœ… Best Choice: CopyOnWriteArrayList
	âœ… Alternative: ConcurrentHashMap (if key-value structure needed)

2ï¸âƒ£ Frequent Insert/Delete
	âœ… Best Choice: LinkedList (Single-threaded)
	âœ… If multi-threaded: ConcurrentLinkedQueue, LinkedBlockingQueue
	
3ï¸âƒ£ Sorted Data
	âœ… Best Choice: TreeSet / TreeMap
	âœ… If Concurrent Sorted Data: ConcurrentSkipListMap, ConcurrentSkipListSet

4ï¸âƒ£ Thread-Safe Access
	Depends on workload.
	ğŸ”¹ High Concurrency (Best Overall):
		âœ… ConcurrentHashMap: Fine-grained locking, CAS operations, Non-blocking reads
	
	ğŸ”¹ Moderate Concurrency:
		âœ… Collections.synchronizedList(): Single lock, Simpler but slower

	ğŸ”¹ Read-Heavy List:
		âœ… CopyOnWriteArrayList
		
=============================================================================================================
ğŸ”¹ Q56. Why Big-O matters while choosing collections? Memory overhead comparison of List, Set, Map?
=============================================================================================================
ğŸ”¥ Why Big-O Matters While Choosing Collections?
	Big-O tells you how performance scales as data grows.
	Example:
		If you store: 
			100 elements â†’ everything feels fast
			10 million elements â†’ wrong choice becomes disaster
			
ğŸ“Œ Example 1: ArrayList vs LinkedList
| Operation     | ArrayList | LinkedList |
| ------------- | --------- | ---------- |
| get(index)    | O(1)      | O(n)       |
| add at end    | O(1)*     | O(1)       |
| insert middle | O(n)      | O(n)       |

If your app: 
	Frequently reads by index â†’ ArrayList wins
	Rare random access â†’ LinkedList maybe ok
	Choosing wrong one can turn: 10ms â†’ 5 seconds

ğŸ“Œ Example 2: HashMap vs TreeMap
| Operation | HashMap  | TreeMap  |
| --------- | -------- | -------- |
| get()     | O(1) avg | O(log n) |
| add()     | O(1) avg | O(log n) |
| Sorted    | No       | Yes      |
If sorting not required â†’ TreeMap is unnecessary overhead.

Notice: 
	O(1) grows flat
	O(log n) grows slowly
	O(n) grows steeply

ğŸ”¥ Memory Overhead Comparison
	Time complexity is not everything. Memory overhead also matters.
	ğŸ“Œ 1ï¸âƒ£ List Implementations
		ğŸ”¹ ArrayList: Backed by Object[], Stores references only, Extra unused capacity
			Memory per element: Reference only (~8 bytes), Most memory-efficient list.
		
		ğŸ”¹ LinkedList: Each element stored in Node: Object data, Node next, Node prev
			Memory per element: 3 references + object header, Much higher memory overhead.
	
	ğŸ“Œ 2ï¸âƒ£ Set Implementations
		ğŸ”¹ HashSet: Internally: HashMap, Node object, hash, key reference, next reference, Higher memory than ArrayList.
		
		ğŸ”¹ TreeSet: Red-Black Tree node, key, left/right/parent references, color flag, More memory than HashSet.
		
	ğŸ“Œ 3ï¸âƒ£ Map Implementations
		ğŸ”¹ HashMap: Each entry stores: key, value, hash, next pointer, Memory moderate.
		
		ğŸ”¹ LinkedHashMap: Everything in HashMap, PLUS before/after pointers, Higher memory than HashMap.
		
		ğŸ”¹ TreeMap: Each entry stores:, key, value, left/right/parent, color, Highest memory among these three.
		
=============================================================================================================
ğŸ”¹ Q57. Why Map lookup is generally faster than List search? Why TreeMap is slower than HashMap?
=============================================================================================================
ğŸ”¥ Why Map Lookup Is Generally Faster Than List Search?
ğŸ“Œ 1ï¸âƒ£ List Search (Linear Search)
		If you search in a List: list.contains("A");
		Internally: Starts from index 0, Compares element one by one, Stops when found
		Time complexity: O(n)
		Worst case: Element at end Or not present
	
ğŸ“Œ 2ï¸âƒ£ Map Lookup (HashMap)
		When you do: map.get(key);
		Internally:
			1ï¸âƒ£ Calculate hashCode()
			2ï¸âƒ£ Compute bucket index
			3ï¸âƒ£ Go directly to that bucket
			4ï¸âƒ£ Compare only within that bucket
			Time complexity (average): O(1)
			Because: Direct index calculation, No full traversal	

ğŸ”¥ Why Map Is Faster?
	Because Map (HashMap): Uses hashing, Jumps directly to bucket, Avoids scanning entire structure
	While List: Must scan elements sequentially

ğŸ”¥ Important Exception
	If: You use TreeMap Or HashMap has extreme collisions, Then complexity changes.

ğŸ”¥ Why TreeMap Is Slower Than HashMap?
	ğŸ“Œ 1ï¸âƒ£ TreeMap Uses Red-Black Tree
		TreeMap stores elements in: Self-balancing binary search tree
		Every operation: Traverse tree, Compare keys, Rebalance tree
		Time complexity: O(log n)
		
	ğŸ“Œ 2ï¸âƒ£ HashMap Uses Hashing
		HashMap: No tree traversal (except high collision case), Direct bucket access
		Time complexity: O(1) average
		
=============================================================================================================
ğŸ”¹ Q59. How would you detect duplicate objects using collections?
=============================================================================================================
ğŸ”¥ How to Detect Duplicate Objects Using Collections?
	The best approach depends on: Data size, Object type, Performance requirement, Whether duplicates must be preserved
	âœ… Approach 1: Using HashSet (Most Efficient)
		Because: HashSet does NOT allow duplicates, Backed by HashMap, O(1) average lookup
		ğŸ“Œ Example
				Set<Person> set = new HashSet<>();
				Set<Person> duplicates = new HashSet<>();

				for (Person p : list) {
					if (!set.add(p)) {
						duplicates.add(p);
					}
				}
			If add() returns false â†’ duplicate found.

	ğŸ”¥ Important: equals() and hashCode() Must Be Correct
		If you donâ€™t override properly: HashSet may fail to detect duplicates
		Rule: If equals() returns true â†’ hashCode must be same

	âœ… Approach 2: Using Map with Counting
		If you need frequency count:
			Map<Person, Integer> map = new HashMap<>();
			for (Person p : list) {
				map.put(p, map.getOrDefault(p, 0) + 1);
			}
		Then: if (map.get(p) > 1) You found duplicate.

	âœ… Approach 3: Using Stream (Java 8+)
			Set<Person> seen = new HashSet<>();

			List<Person> duplicates =
				list.stream()
					.filter(p -> !seen.add(p))
					.collect(Collectors.toList());

	âœ… Approach 4: Sorting + Comparison (Less Efficient)
		If objects are Comparable:
			Collections.sort(list);
			for (int i = 1; i < list.size(); i++) {
				if (list.get(i).equals(list.get(i - 1))) {
					System.out.println("Duplicate found");
				}
			}

		Time complexity: O(n log n), Slower than HashSet approach.

ğŸ”¥ Performance Comparison
| Approach         | Time Complexity |
| ---------------- | --------------- |
| HashSet          | O(n)            |
| HashMap counting | O(n)            |
| Sorting          | O(n log n)      |
| Nested loop      | O(nÂ²) âŒ         |

ğŸ”¥ Real-World Scenarios
	If millions of records: Use HashSet / HashMap
	If memory constraint: Consider streaming / external sorting
	If DB-level duplicates: Use SQL GROUP BY

=============================================================================================================
ğŸ”¹ Q61. How to sort a Map by value?
=============================================================================================================
ğŸ”¥ How to Sort a Map by Value?
	Important: HashMap is not sorted, We must convert entries to a List, Then sort, Then collect into LinkedHashMap

âœ… Approach 1: Using Java 8 Stream (Recommended)
		Map<String, Integer> map = new HashMap<>();
		map.put("A", 3); 
		map.put("B", 1);
		map.put("C", 2);

		Map<String, Integer> sorted =
			map.entrySet()
			   .stream()
			   .sorted(Map.Entry.comparingByValue()) //.sorted(Map.Entry.comparingByValue().reversed()) => Descending Order
			   .collect(Collectors.toMap(
				   Map.Entry::getKey,
				   Map.Entry::getValue,
				   (e1, e2) -> e1,
				   LinkedHashMap::new
			   ));
	
		ğŸ‘‰ LinkedHashMap is used to preserve sorted order.

âœ… Approach 2: Using List + Collections.sort (Pre-Java 8)
		List<Map.Entry<String, Integer>> list =  new ArrayList<>(map.entrySet());
		Collections.sort(list, (e1, e2) -> e1.getValue().compareTo(e2.getValue()));
		
		Map<String, Integer> sorted = new LinkedHashMap<>();
		for (Map.Entry<String, Integer> entry : list) {
			sorted.put(entry.getKey(), entry.getValue());
		}
	
	ğŸ”¥ Time Complexity 
		Sorting requires: O(n log n)
		Because: We are sorting n entries.

ğŸ”¥ Why Not Use TreeMap?
	TreeMap sorts by key, not by value.
	So: new TreeMap<>(map) Will sort by key only.

ğŸ”¥ Alternative: Using TreeSet with Custom Comparator
	Less common approach:
		TreeSet<Map.Entry<String, Integer>> set = new TreeSet<>(Comparator.comparing(Map.Entry::getValue));
	
	âš  Risk: If two values are equal â†’ comparator returns 0 â†’ One entry may be lost.
	Better to include key comparison too:
		Comparator.comparing(Map.Entry::getValue).thenComparing(Map.Entry::getKey)

ğŸ”¥ Senior-Level Insight
	If: Sorting frequently required, Data large
	Better to: Use PriorityQueue Or maintain sorted structure incrementally Instead of sorting every time.
	

=============================================================================================================
ğŸ”¹ Q63. How to make a collection read-only?
=============================================================================================================
âœ… 1ï¸âƒ£ Using Collections.unmodifiableXXX() (Wrapper Approach)
	Example: 
		List<String> list = new ArrayList<>();
		list.add("A");
		List<String> readOnly = Collections.unmodifiableList(list);
		Now: readOnly.add("B");  // âŒ UnsupportedOperationException

	ğŸ“Œ How It Works Internally
		It returns a wrapper: Delegates read methods to original list, Overrides modifying methods, Throws UnsupportedOperationException
	âš  Important If original list changes: list.add("C");
		Read-only view reflects change.
		So this is: Read-only view, NOT truly immutable

âœ… 2ï¸âƒ£ Using Java 9+ Immutable Factory Methods (True Read-Only)
		List<String> list = List.of("A", "B", "C");
	OR
		List<String> list = List.copyOf(existingList);
	
	Now: No modification allowed, No backing reference exposed, Truly immutable, Difference from unmodifiableList
	List.of(): Does NOT wrap existing list, Creates immutable collection, Underlying structure cannot change

âœ… 3ï¸âƒ£ Defensive Copy + Unmodifiable Wrapper (Best Practice)
	If you want to return a read-only collection from a method:
		public List<String> getData() {
			return Collections.unmodifiableList(new ArrayList<>(internalList));
		}
	
	Why? Prevents caller from modifying original, Prevents indirect modification
	
ğŸ”¥ What About Set and Map?
	Same methods available:
		Collections.unmodifiableSet(set);
		Collections.unmodifiableMap(map);
		Set.of(...);
		Map.of(...);

ğŸ”¥ Thread-Safety Insight
	Immutable collections: Naturally thread-safe, No synchronization required
	Unmodifiable wrapper: Not thread-safe if original is mutable

ğŸ”¥ When to Use Which?
| Scenario                      | Best Option      |
| ----------------------------- | ---------------- |
| Return read-only view         | unmodifiableList |
| True immutability needed      | List.of / copyOf |
| Prevent external modification | Defensive copy   |
| Concurrent read-heavy         | Immutable        |


=============================================================================================================
ğŸ”¹ Q64. How to handle NullPointerException in collections?
=============================================================================================================
ğŸ”¥ Why NullPointerException Happens in Collections?
	Common scenarios:
		1ï¸âƒ£ Collection reference is null
		2ï¸âƒ£ Null element inside collection
		3ï¸âƒ£ Null key in Map
		4ï¸âƒ£ Calling method on null value from map
		5ï¸âƒ£ Using stream on null collection
	
	Example:
		List<String> list = null;
		list.add("A");   // âŒ NPE

ğŸ”¥ 1ï¸âƒ£ Always Initialize Collections
	Instead of: List<String> list = null;
	Use: List<String> list = new ArrayList<>(); OR List<String> list = Collections.emptyList();
	Best practice: Never return null collections from methods.

ğŸ”¥ 2ï¸âƒ£ Handle Null Elements Safely
| Collection        | Null Allowed?       |
| ----------------- | ------------------- |
| ArrayList         | âœ… Yes               |
| HashSet           | âœ… Yes (one null)    |
| HashMap           | âœ… One null key      |
| TreeMap           | âŒ No null key       |
| ConcurrentHashMap | âŒ No null key/value |

Be careful with: list.contains(null);
And with: map.get(key).toString();  // âŒ if value is null

ğŸ”¥ 4ï¸âƒ£ Safe Map Access Pattern
	Instead of: map.get(key).length();   // âŒ risk
	Use:
		String value = map.get(key);
		if (value != null) {
			value.length();
		}

	Or: map.getOrDefault(key, "default");
	Or: Optional.ofNullable(map.get(key)).ifPresent(v -> System.out.println(v));

ğŸ”¥ 5ï¸âƒ£ Using Optional for Safer Handling
	Optional.ofNullable(list)
        .orElse(Collections.emptyList())
        .forEach(System.out::println);
	
	Good for stream pipelines.

ğŸ”¥ 6ï¸âƒ£ Avoid Null Keys/Values in Design
	Better practice: Avoid storing null in collections, Use Optional as value, Use default object instead of null
	Example: Map<String, Optional<String>> map;

ğŸ”¥ 7ï¸âƒ£ Defensive Checks Before Iteration
	Instead of: for (String s : list) { }
	Use:
		if (list != null) {
			for (String s : list) { }
		}
	Or better:
			for (String s : Optional.ofNullable(list)
									.orElse(Collections.emptyList())) {
			}

ğŸ”¥ 8ï¸âƒ£ Stream Safe Handling
	Instead of: list.stream()
	Use:
		Optional.ofNullable(list)
        .orElse(Collections.emptyList())
        .stream()

ğŸ”¥ Real Production Best Practices
	âœ… Never return null collections
	âœ… Prefer empty collections
	âœ… Avoid null elements
	âœ… Use getOrDefault()
	âœ… Use Optional for APIs
	âœ… Validate inputs
	
=============================================================================================================
ğŸ”¹ Q65. How to avoid memory leaks caused by collections?
=============================================================================================================
ğŸ”¥ Why Collections Can Cause Memory Leaks?
	Java has GC â€” so how can memory leak happen?
	Because: Objects are still reachable, Collection still holds reference, GC cannot clean them
	A memory leak in Java means: Objects are no longer used but still strongly referenced.

ğŸ”¥ 1ï¸âƒ£ Not Removing Unused Objects from Collections
	Example:
		List<Object> cache = new ArrayList<>();
		public void add(Object o) {
			cache.add(o);
		}
	If never removed: List grows forever, Heap increases, Eventually OutOfMemoryError
	
	âœ… Fix: Remove unused objects, Clear collection when done
		cache.clear();

ğŸ”¥ 2ï¸âƒ£ Static Collections (Very Common Leak)
		private static List<User> users = new ArrayList<>();
	
	Static variables: Live for entire application lifetime, GC never collects them
		If you keep adding â†’ permanent memory growth.
	
	âœ… Fix: Avoid static collections, Use proper lifecycle management, Use bounded cache

ğŸ”¥ 3ï¸âƒ£ Using Unbounded Cache
	Bad: Map<String, Object> cache = new HashMap<>();
	If used as cache: No eviction, Grows forever
	
	âœ… Fix: Use LRU Cache, LinkedHashMap with removeEldestEntry()
		OR use: Caffeine, Redis

4ï¸âƒ£ Listener / Callback Leaks
	Example: listeners.add(this);
	If never removed: Object remains referenced, GC cannot clean it
	Very common in: GUI apps, Event-driven systems
	
	âœ… Fix: Always remove listeners, Use WeakReference, Use WeakHashMap

ğŸ”¥ 5ï¸âƒ£ ThreadLocal Memory Leak
	ThreadLocal<User> userContext = new ThreadLocal<>();
	If using thread pool: Thread never dies, ThreadLocal value remains, Memory leak
	
	âœ… Fix: Always:
		try {
			userContext.set(user);
		} finally {
			userContext.remove();
		}

ğŸ”¥ 6ï¸âƒ£ Using WeakHashMap for Auto Cleanup
	WeakHashMap: Keys stored as weak references, If key not referenced elsewhere, Entry automatically removed
	Good for: Metadata mapping, Cache with GC support

ğŸ”¥ 7ï¸âƒ£ Clearing Large Collections After Use
	Example:
		List<BigObject> list = new ArrayList<>();
		process(list);
		list.clear();  // Important
	
	Better: list = null; If object no longer needed.

ğŸ”¥ 8ï¸âƒ£ Avoid Retaining References Accidentally
	Example: List<LargeObject> temp = largeList.subList(0, 10);
	subList keeps reference to entire original list. Memory of full list retained.
	
	âœ… Fix: new ArrayList<>(largeList.subList(0, 10)); Creates independent copy.

ğŸ”¥ 9ï¸âƒ£ Profiling & Detection Tools
	In real systems use: VisualVM, JProfiler, Eclipse MAT, Heap dump analysis
	Look for: Large collections, Growing maps, Static references
	
=============================================================================================================
ğŸ”¹ Q66. Why WeakHashMap exists? Use cases of WeakHashMap in real systems?
=============================================================================================================
ğŸ”¥ Why WeakHashMap Exists?
	In normal HashMap: Keys are stored using strong references, As long as key exists in map â†’ GC cannot remove it
	This can cause memory leaks if: You forget to remove entries, Keys are no longer used elsewhere
	
	ğŸ“Œ Problem with HashMap
		Map<Object, String> map = new HashMap<>();
		Object key = new Object();
		map.put(key, "Data");
		key = null;   // Remove external reference
	
	Question: will entry be garbage collected?
		âŒ No. Because: HashMap still strongly references key. So entry stays forever.

ğŸ”¥ How WeakHashMap Solves This
	WeakHashMap stores keys as: Weak references
	If key is not referenced anywhere else: GC clears weak reference, Entry automatically removed

ğŸ”¥ How WeakHashMap Internally Works
	Internally:
		static class Entry<K,V> extends WeakReference<K> {
			V value;
		}
	
	Important: Keys wrapped in WeakReference, Uses ReferenceQueue, When GC collects key â†’ entry removed

ğŸ”¥ When GC Runs
	If:
		Object key = new Object();
		weakMap.put(key, "Value");
		key = null;
		System.gc();
	
	Now: Key has no strong reference, GC clears weak reference, Entry removed automatically

ğŸ”¥ Real Use Cases of WeakHashMap
	âœ… 1ï¸âƒ£ Caching Metadata
		Example:
			Framework storing metadata about objects: WeakHashMap<Object, Metadata>
			If object destroyed: Metadata automatically removed
			Used in: Reflection caching, ORM frameworks, Class metadata storage

	âœ… 2ï¸âƒ£ Avoid Memory Leaks in Listeners
		Store listeners as weak keys: WeakHashMap<Listener, Object>
		If listener object no longer used: Automatically removed
	
	âœ… 3ï¸âƒ£ Canonical Mapping / Interning
		Store computed values associated with object identity: WeakHashMap<Object, ComputedValue>
		If object gone â†’ computed value removed

	âœ… 4ï¸âƒ£ ClassLoader-Based Caching
		Very important in application servers.
		If: ClassLoader unloaded, But cached strongly in map â†’ Memory leak
		WeakHashMap avoids this.

ğŸ”¥ Important Limitations
	âš  Only keys are weak, Values are still strong references, Cleanup happens only during GC, Not deterministic

ğŸ”¥ When NOT to Use WeakHashMap
	âŒ If you need stable cache, If you need deterministic eviction, For large performance-critical caches
	Better use: Caffeine, SoftReference cache, LRU

ğŸ”¥ Weak vs Soft Reference (Quick Insight)
| Type   | GC Behavior                                    |
| ------ | ---------------------------------------------- |
| Strong | Never collected while referenced               |
| Weak   | Collected immediately when no strong reference |
| Soft   | Collected only under memory pressure           |

=============================================================================================================
ğŸ”¹ Q67. What is IdentityHashMap? Difference between HashMap and IdentityHashMap?
=============================================================================================================
ğŸ”¥ IdentityHashMap is a special Map implementation where Keys are compared using == (reference equality) NOT using equals()
	It also uses: System.identityHashCode(), NOT key.hashCode()
	
ğŸ”¥ Normal HashMap Behavior
	In HashMap: Two keys are considered equal if: key1.equals(key2) == true And: key1.hashCode() == key2.hashCode()

ğŸ”¥ IdentityHashMap Behavior
	In IdentityHashMap: Two keys are equal only if: key1 == key2
	Meaning: They must be the exact same object in memory.

ğŸ”¥ Example
	String a = new String("hello");
	String b = new String("hello");

	HashMap<String, Integer> map1 = new HashMap<>();
	map1.put(a, 1);
	map1.put(b, 2);
	System.out.println(map1.size());  // 1
	
	Because: a.equals(b) == true
	
	Now with IdentityHashMap:
		IdentityHashMap<String, Integer> map2 = new IdentityHashMap<>();
		map2.put(a, 1);
		map2.put(b, 2);
		System.out.println(map2.size());  // 2
	
	Because: a == b  // false

ğŸ”¥ Internal Working of IdentityHashMap
	Unlike HashMap: Does NOT use buckets + linked list, Uses flat array structure
	Stores key-value pairs in alternating slots: [ key1, value1, key2, value2, ... ]
	Comparison uses: == 
	Hashing uses: System.identityHashCode(key)

ğŸ”¥ Key Differences Table
| Feature          | HashMap         | IdentityHashMap          |
| ---------------- | --------------- | ------------------------ |
| Equality check   | equals()        | ==                       |
| Hash function    | hashCode()      | identityHashCode()       |
| Logical equality | Yes             | No                       |
| Use case         | General purpose | Object identity tracking |
| Null key         | 1 allowed       | 1 allowed                |

ğŸ”¥ When to Use IdentityHashMap?
	Very specific scenarios:
		âœ… 1ï¸âƒ£ Object Graph Traversal
			During: Serialization, Deep cloning, Object mapping, You must track visited objects by identity.
			Example: Map<Object, Boolean> visited = new IdentityHashMap<>();
		
		âœ… 2ï¸âƒ£ Proxy / Framework Internals
			Used in: JVM internals, Serialization frameworks, ORM tools, Dependency injection frameworks
		
		âœ… 3ï¸âƒ£ Caching Based on Object Instance
			When you care about: Specific object instance, Not logical equality

ğŸ”¥ When NOT to Use IdentityHashMap
	âŒ General business logic, When logical equality matters, For typical key-value storage
	Because: It violates Map contract expectations.
	
=============================================================================================================
ğŸ”¹ Q68. EnumMap use cases? Why EnumMap is faster than HashMap?
=============================================================================================================
ğŸ”¥ EnumMap is a specialized Map implementation designed only for enum keys.
	Example:
		enum Status {
				NEW, PROCESSING, COMPLETED, FAILED
			}
		EnumMap<Status, String> map = new EnumMap<>(Status.class);
		
		ğŸ‘‰ Keys must be enum type, Extremely efficient

ğŸ”¥ Why EnumMap Is Faster Than HashMap?
ğŸ“Œ 1ï¸âƒ£ Internal Data Structure
	Unlike HashMap: No hashing, No buckets, No linked lists, No tree bins
	Instead: Internally backed by an array, Index of enum is determined by: enum.ordinal()
	
ğŸ“Œ 2ï¸âƒ£ Direct Index Access
	Example: map.put(Status.NEW, "Task Created");
	Internally: array[Status.NEW.ordinal()] = "Task Created";
	So lookup is: O(1) with direct index, No hash calculation required.

ğŸ“Œ 3ï¸âƒ£ Memory Efficient
	HashMap Entry stores: key, value, hash, next pointer
	EnumMap stores: Only array of values, Keys implied by enum order, Much lower memory overhead.

ğŸ”¥ Performance Comparison
| Feature         | HashMap            | EnumMap                |
| --------------- | ------------------ | ---------------------- |
| Key type        | Any object         | Only enum              |
| Lookup          | O(1) avg (hashing) | O(1) direct index      |
| Memory usage    | Higher             | Lower                  |
| Null key        | 1 allowed          | âŒ Not allowed          |
| Iteration order | No order           | Enum declaration order |

ğŸ”¥ Why Itâ€™s Faster in Practice?
	Because:
		1ï¸âƒ£ No hash computation
		2ï¸âƒ£ No collision handling
		3ï¸âƒ£ No linked list traversal
		4ï¸âƒ£ No rehashing
		5ï¸âƒ£ Better cache locality
	Array access is extremely fast.

ğŸ”¥ Use Cases of EnumMap
	âœ… 1ï¸âƒ£ State Machine Implementation
		Example: EnumMap<State, Handler> handlers;
		Very common in: Workflow engines, Order processing systems
	
	âœ… 2ï¸âƒ£ Role-Based Logic
		EnumMap<Role, Permission> permissions;
		Instead of: if (role == ADMIN) { }
		Cleaner and faster.
	
	âœ… 3ï¸âƒ£ Configuration by Enum Type
		Example: EnumMap<DayOfWeek, Schedule> schedule;

	âœ… 4ï¸âƒ£ Replacing Switch Statements
		Instead of:
		switch(status) {
			case NEW:
		}
		
		Use: handlers.get(status).execute(); More extensible.

ğŸ”¥ When NOT to Use EnumMap
	âŒ If keys are not enum, If dynamic keys required, If enum type not fixed

ğŸ”¥ Real Production Insight
	EnumMap is widely used in: Compilers, JVM internals, Spring framework, State-driven systems

=============================================================================================================
ğŸ”¹ Q69. What is NavigableMap? Difference between SortedMap and NavigableMap?
=============================================================================================================
ğŸ”¥ NavigableMap is an extension of SortedMap that provides:
	 Navigation methods, Closest match lookup, Reverse order traversal, Range queries, It belongs to java.util.
	
	Main implementation: TreeMap, ConcurrentSkipListMap
	
ğŸ”¥ Interface Hierarchy
			Map
			  â†“
			SortedMap
			  â†“
			NavigableMap

ğŸ”¥ What is SortedMap? It ensures: Keys are sorted, Natural order or Comparator, Provides range view methods
	Key methods: firstKey(), lastKey(), headMap(), tailMap(), subMap()

ğŸ”¥ What Extra Does NavigableMap Provide?
NavigableMap adds powerful navigation methods.
	1ï¸âƒ£ Closest Match Methods
		| Method        | Meaning          |
		| ------------- | ---------------- |
		| lowerKey(k)   | Greatest key < k |
		| floorKey(k)   | Greatest key â‰¤ k |
		| ceilingKey(k) | Smallest key â‰¥ k |
		| higherKey(k)  | Smallest key > k |
	
		Very useful for: Range searching, Nearest match lookup, Time-series queries
	
	2ï¸âƒ£ Entry Variants: provides: lowerEntry(), floorEntry(), ceilingEntry(), higherEntry()
	
	3ï¸âƒ£ Reverse Order Support: descendingMap(), descendingKeySet()

ğŸ”¥ Difference Between SortedMap and NavigableMap
| Feature               | SortedMap | NavigableMap |
| --------------------- | --------- | ------------ |
| Sorted keys           | âœ… Yes     | âœ… Yes        |
| Range views           | âœ… Yes     | âœ… Yes        |
| Closest match methods | âŒ No      | âœ… Yes        |
| Reverse traversal     | âŒ No      | âœ… Yes        |
| Entry navigation      | âŒ No      | âœ… Yes        |
| Implementation        | TreeMap   | TreeMap      |

ğŸ”¥ Practical Example
		TreeMap<Integer, String> map = new TreeMap<>();
		map.put(10, "A");
		map.put(20, "B");
		map.put(30, "C");
		map.floorKey(25);   // 20
		map.ceilingKey(25); // 30
	Very powerful for: Leaderboards, Time-series data, Scheduling systems

ğŸ”¥ When to Use NavigableMap?
	Use when you need: Range queries, Nearest value lookup, Sorted order with navigation, Time-based search, Ranking systems

ğŸ”¥ Concurrent Version
	If concurrent access required: ConcurrentSkipListMap (implements NavigableMap)

=============================================================================================================
ğŸ”¹ Q70. Explain subMap(), headMap(), tailMap()
=============================================================================================================
ğŸ”¥ These methods provide range views of a sorted map.
	They are available in: SortedMap, NavigableMap (with inclusive/exclusive options)
	Main implementation: TreeMap, ConcurrentSkipListMap
	
	1ï¸âƒ£ headMap(): Returns portion of map whose keys are less than given key.
		SortedMap version: headMap(toKey)
		ğŸ‘‰ Keys < toKey, ğŸ‘‰ toKey is exclusive
		
		NavigableMap version: headMap(toKey, inclusive)
		Example: map.headMap(20, true);	ğŸ‘‰ Includes key 20
		
		ğŸ“Œ Example: 
					TreeMap<Integer, String> map = new TreeMap<>();
					map.put(10, "A");
					map.put(20, "B");
					map.put(30, "C");
					map.headMap(20);

					Result:{10=A}

	2ï¸âƒ£ tailMap(): Returns portion whose keys are greater than or equal to given key.

		SortedMap version: tailMap(fromKey), keys >= fromKey

		NavigableMap version: tailMap(fromKey, inclusive)
		Example: map.tailMap(20, false); 	ğŸ‘‰ Excludes 20

		ğŸ“Œ Example
				map.tailMap(20);
				
				Result: {20=B, 30=C}

	3ï¸âƒ£ subMap(): Returns portion between two keys.
		SortedMap version: subMap(fromKey, toKey)
			ğŸ‘‰ fromKey inclusive
			ğŸ‘‰ toKey exclusive

		NavigableMap version: subMap(fromKey, fromInclusive, toKey, toInclusive)
		Full control over inclusivity.
			Example: map.subMap(10, 30);
			Result: {10=A, 20=B}
			
=============================================================================================================
ğŸ”¹ Q71. Internal working of PriorityQueue? PriorityQueue vs TreeSet? Why PriorityQueue is not sorted iteration-wise?
=============================================================================================================
ğŸ”¥ PriorityQueue in Java is implemented using: Binary Heap, Backed by array
	By default: Min-Heap, Smallest element at root
	
	Internal Structure
		Internally:
			transient Object[] queue;
			int size;
			Comparator<? super E> comparator;
		It uses a complete binary tree stored in an array.

ğŸ”¥ Heap Index Formula
	If element at index i:
		Left child â†’ 2*i + 1
		Right child â†’ 2*i + 2
		Parent â†’ (i - 1) / 2

ğŸ”¥ Operations: 
	1ï¸âƒ£ add() / offer()
		Steps: Insert at end of array, Perform heapify-up (sift-up)
		Time complexity: O(log n)

	2ï¸âƒ£ poll()
		Steps: Remove root, Replace with last element, Perform heapify-down (sift-down)
		Time complexity: O(log n)
	
	3ï¸âƒ£ peek(): Return root. Time complexity: O(1)

ğŸ”¥ Why PriorityQueue Is NOT Sorted Iteration-Wise?
	Because:
		Heap guarantees: Parent â‰¤ Children
		It does NOT guarantee: Left subtree < Right subtree, Full sorted order
	The internal array is a heap structure, not a sorted array.
	So: for (Integer i : pq)	Will NOT give sorted order.
	To get sorted order:
		while (!pq.isEmpty()) {
			System.out.println(pq.poll());
		}

ğŸ”¥ Example
	Insert: 5, 1, 3, 2
	Heap may look like:
		  1
		/   \
	   2     3
	  /
	 5

	Array: [1, 2, 3, 5]
	But if you insert different order, structure changes. Only root guaranteed smallest.

ğŸ”¥ PriorityQueue vs TreeSet
| Feature          | PriorityQueue | TreeSet        |
| ---------------- | ------------- | -------------- |
| Structure        | Binary Heap   | Red-Black Tree |
| Sorted globally  | âŒ No          | âœ… Yes          |
| Root min/max     | âœ… Yes         | Yes            |
| Iteration sorted | âŒ No          | âœ… Yes          |

ğŸ”¥ Time Complexity Comparison
| Operation        | PriorityQueue | TreeSet  |
| ---------------- | ------------- | -------- |
| add              | O(log n)      | O(log n) |
| remove           | O(log n)      | O(log n) |
| peek/min         | O(1)          | O(log n) |
| contains         | O(n)          | O(log n) |
| Iteration sorted | âŒ No          | âœ… Yes    |

ğŸ”¥ Important Differences
	1ï¸âƒ£ Duplicate Elements
			PriorityQueue â†’ allows duplicates
				TreeSet â†’ does NOT allow duplicates

	2ï¸âƒ£ Lookup Performance
			PriorityQueue â†’ contains() is O(n)
			TreeSet â†’ contains() is O(log n)

	3ï¸âƒ£ Use Case
		Use PriorityQueue when:	Need fast min/max extraction, Implement Dijkstra, Task scheduling, 	Top-K problems
		Use TreeSet when: Need sorted set, Need range queries, Need floor/ceiling operations

ğŸ”¥ Why PriorityQueue Is Faster for Min Retrieval?
	Because: Root always smallest, No tree traversal needed, Direct array access
	
=============================================================================================================
ğŸ”¹ Q72. Difference between Collection and Stream? How streams internally process collections?
=============================================================================================================
1ï¸âƒ£ Conceptual Difference
| Feature          | Collection         | Stream                      |
| ---------------- | ------------------ | --------------------------- |
| Purpose          | Store data         | Process data                |
| Mutable          | Yes                | No (does not modify source) |
| Reusable         | Yes                | No (one-time use)           |
| Traversal        | External iteration | Internal iteration          |
| Eager/Lazy       | Eager              | Lazy (intermediate ops)     |
| Parallel support | Manual             | Built-in                    |

ğŸ”¹ Collection
	Example:
		List<String> list = new ArrayList<>();
		list.add("A");
		ğŸ‘‰ Holds data, You control iteration

ğŸ”¹ Stream
	Example: 
		list.stream()
			.filter(s -> s.startsWith("A"))
			.forEach(System.out::println);

		ğŸ‘‰ Does not store data, Processes data in pipeline

ğŸ”¥ External vs Internal Iteration
	ğŸ”¹External Iteration (Collection)
		for (String s : list) {
			...
		}
		ğŸ‘‰ Developer controls loop, Pull-based model

	
	ğŸ”¹Internal Iteration (Stream)
		list.stream().forEach(...)
		ğŸ‘‰ Stream controls iteration, Push-based model

ğŸ”¥ How Streams Internally Process Collections?
	Now letâ€™s go deeper. When you write:
		list.stream()
			.filter(x -> x > 5)
			.map(x -> x * 2)
			.forEach(System.out::println);

	Internally:
		Step 1ï¸âƒ£ â†’ Stream Creation: list.stream()
			Collection provides: Spliterator, Stream gets a Spliterator from collection.
		
		Step 2ï¸âƒ£ â†’ Build Pipeline
			Each intermediate operation: filter(), map(), sorted()
				Creates a new stream stage. No processing happens yet. ğŸ‘‰ Lazy evaluation.
		
		Step 3ï¸âƒ£ â†’ Terminal Operation Triggers Execution
			When: forEach()
				Stream starts processing.

ğŸ”¥ Internal Architecture
	Internally Stream API uses: AbstractPipeline, ReferencePipeline, Sink, Spliterator
	Data flows through chained stages.

ğŸ”¥ Important Concepts
	ğŸ“Œ 1ï¸âƒ£ Lazy Evaluation
			list.stream()
				.filter(...)
				.map(...)

		Nothing happens until: terminal operation
		
ğŸ“Œ 2ï¸âƒ£ Vertical Processing (Not Horizontal)
	Instead of: Filter entire list, Then map entire list
	Stream does:
		Element1 â†’ filter â†’ map â†’ output
		Element2 â†’ filter â†’ map â†’ output
		Improves performance.

ğŸ“Œ 3ï¸âƒ£ Spliterator
		Collection provides: Spliterator<E> spliterator()
		Spliterator: Traverses elements, Splits for parallel processing
		Used in: parallelStream()

ğŸ”¥ How Parallel Stream Works?
	Spliterator splits data, ForkJoinPool used, Multiple threads process chunks

ğŸ”¥ Performance Comparison
| Operation           | Collection        | Stream          |
| ------------------- | ----------------- | --------------- |
| Simple loop         | Faster            | Slight overhead |
| Complex chaining    | Verbose           | Cleaner         |
| Parallel processing | Manual threads    | Built-in        |
| Memory usage        | Mutable structure | No storage      |

ğŸ”¥ Important Limitation
	Stream: Cannot be reused, Consumed once
		Stream s = list.stream();
		s.forEach(...);
		s.forEach(...);  // âŒ IllegalStateException
		
=============================================================================================================
ğŸ”¹ Q75. Parallel streams with collections â€“ risks?
=============================================================================================================
ğŸ”¥ What Are Parallel Streams?
			list.parallelStream()
				.filter(x -> x > 10)
				.map(x -> x * 2)
				.forEach(System.out::println);
		
		Internally: Uses ForkJoinPool.commonPool(), Splits data using Spliterator, Processes chunks in parallel threads

ğŸ”¥ Risks of Parallel Streams
	âš  1ï¸âƒ£ Shared Mutable State (Race Conditions)
		Bad example:
				List<Integer> result = new ArrayList<>();
				list.parallelStream()
					.forEach(result::add);  // âŒ Not thread-safe

		Problem: Multiple threads modify same list, Data corruption possible
		âœ… Fix:
				list.parallelStream()
					.collect(Collectors.toList());
			Use proper collectors.

	âš  2ï¸âƒ£ Performance Overhead
		Parallel streams are NOT always faster.
		Overhead includes: Thread management, Task splitting, Context switching, Synchronization
		For small datasets: ğŸ‘‰ Sequential is faster.
	
	âš  3ï¸âƒ£ Blocking Operations Inside Parallel Stream
		Example: list.parallelStream().forEach(x -> callExternalAPI(x));
		If API call blocks: Threads get blocked, ForkJoinPool threads exhausted, Other parallel tasks starve
		ForkJoinPool is designed for: CPU-bound tasks, Not I/O-bound tasks.

	âš  4ï¸âƒ£ Using Parallel Streams in Application Servers
		ForkJoinPool.commonPool() is shared globally.
		If you use parallel streams heavily: You block common pool, Other framework tasks suffer, Spring / Servlet container impact possible.

	âš  5ï¸âƒ£ Non-Associative Operations
		Reduction operations must be associative.
		Safe: .reduce(0, Integer::sum);
		Unsafe: .reduce(0, (a, b) -> a - b);  // âŒ Not associative
		Parallel results unpredictable.
	
	âš  6ï¸âƒ£ Order-Sensitive Operations
		Parallel stream does NOT guarantee order unless: forEachOrdered() But forEachOrdered() reduces performance.

	âš  7ï¸âƒ£ Poor Splitting Performance
		Collections like: 
			ArrayList â†’ Splits well
			LinkedList â†’ Poor splitting
			I/O streams â†’ Cannot split
		Parallelism efficiency depends on Spliterator.

ğŸ”¥ When Parallel Streams Make Sense
	âœ… Large dataset
	âœ… CPU-intensive tasks
	âœ… Stateless operations
	âœ… No shared mutable state
	âœ… Associative reductions

	Example: IntStream.range(1, 1_000_000).parallel().sum();

ğŸ”¥ When NOT to Use
	âŒ Small datasets, I/O operations, Database calls, Logging inside stream, Shared mutable state, Transactional code

ğŸ”¥ Senior-Level Insight
	Parallel streams are NOT a replacement for: ExecutorService, CompletableFuture, Reactive programming
	They are best for: ğŸ‘‰ Data processing workloads.

=============================================================================================================
ğŸ”¹ Q76. Why using mutable objects as Map keys is dangerous?
=============================================================================================================
ğŸ”¥ Because HashMap relies on: 1ï¸âƒ£ hashCode(), 2ï¸âƒ£ equals()
	If key state changes after insertion â†’ map structure breaks.

ğŸ”¥ The Problem with Mutable Keys
	Consider:
		class User {
			String email;

			@Override
			public int hashCode() {
				return email.hashCode();
			}

			@Override
			public boolean equals(Object o) {
				return this.email.equals(((User)o).email);
			}
		}

	Step 1ï¸âƒ£ Insert
			User user = new User("a@gmail.com");
			map.put(user, "Data");
			Hash computed based on "a@gmail.com".

	Step 2ï¸âƒ£ Modify Key
			user.email = "b@gmail.com";
			Now: hashCode changed, equals behavior changed
	
	Step 3ï¸âƒ£ Try to Retrieve
			map.get(user);
		HashMap: Computes new hash (based on new email), Looks in different bucket, Doesnâ€™t find entry
		Result: null, Even though entry exists!

ğŸ”¥ Worse: Entry Becomes â€œLostâ€
		The old entry: Still in old bucket, But no way to retrieve it
		This causes: Logical corruption, Memory leak, Hard-to-debug production issues

ğŸ”¥ What Exactly Breaks?
	HashMap assumes: hashCode and equals are stable
	Contract: If a key is in a Map, its hashCode must not change while it is in the Map.
	If violated: Bucket index mismatch, equals comparison fails, Retrieval fails

ğŸ”¥ Even TreeMap Is Affected
	If using TreeMap: If compareTo() depends on mutable field:
	After mutation: Tree structure becomes invalid, Search fails

ğŸ”¥ Best Practices
	âœ… 1ï¸âƒ£ Use Immutable Keys
		Examples: String, Integer, UUID
		Custom immutable class
	
	âœ… 2ï¸âƒ£ Make Fields Final
		class User {
			private final String email;
		}
	
	âœ… 3ï¸âƒ£ Do NOT Include Mutable Fields in hashCode/equals
		Bad: List<String> roles;  // mutable
		If roles used in hashCode â†’ dangerous.

ğŸ”¥ Real Production Example
	Common bug: Using List or Date as key, Date modified later, Map lookup fails, Very hard to debug.
	
=============================================================================================================
ğŸ”¹ Q78. Why ConcurrentModificationException is unchecked?
=============================================================================================================
ğŸ”¥ First: What is ConcurrentModificationException?
	It is thrown when: A collection is structurally modified, While being iterated, Outside the iterator
	Example:
			List<String> list = new ArrayList<>();
			list.add("A");
			list.add("B");

			for (String s : list) {
				list.remove(s);   // âŒ ConcurrentModificationException
			}

ğŸ”¥ Why Is It Unchecked (RuntimeException)?
	Because: It indicates a programming error, not a recoverable condition.

ğŸ”¥ Checked vs Unchecked Philosophy
	âœ… Checked Exception
		Used for: Recoverable conditions, External failures, I/O issues, Network failures
		Example: IOException, SQLException
		You are expected to handle them.
	
	âŒ Unchecked Exception
		Used for: Programming mistakes, Logical errors, API misuse, Violated contract
		Examples: NullPointerException, IllegalArgumentException, ConcurrentModificationException
		You are NOT expected to recover.

ğŸ”¥ Why CME Is a Programming Error?
	Because: 
		You violated iterator contract:Do not structurally modify collection during iteration except via Iterator.remove().
		Itâ€™s not: Thread-safety exception, Concurrency control mechanism
		It is: ğŸ‘‰ A fail-fast detection mechanism.

ğŸ”¥ Fail-Fast Is Best-Effort
	Internally:
		Collections maintain: modCount
		Iterator stores: expectedModCount
		If mismatch detected: throw new ConcurrentModificationException();

ğŸ”¥ Why Not Checked?
	If CME were checked: Every loop would need try-catch, Code becomes noisy, It encourages wrong recovery patterns
	You cannot meaningfully recover:
		try {
		   for (...) { }
		} catch (ConcurrentModificationException e) {
		   // What would you do here?
		}
	It usually indicates: Your logic is wrong.
	Important: CME Is NOT About Multi-Threading Only
	Common misconception: It can happen even in single thread:
		for (String s : list) {
			list.add("X");
		}
	No concurrency involved.
	So: It is about contract violation, not thread collision.

ğŸ”¥ Design Principle
	Java designers made it unchecked because: It represents API misuse, Itâ€™s a developer bug ,It should fail fast
	It should not force boilerplate handling
	
=============================================================================================================
ğŸ”¹ Q79. Why HashSet size may appear incorrect?
=============================================================================================================
ğŸ”¥When someone says: â€œHashSet size is incorrectâ€
It usually means: Duplicates are not removed, Or expected duplicates are still present, Or unexpected removals happened

ğŸ”¥ 1ï¸âƒ£ equals() and hashCode() Not Implemented Properly
	This is the most common reason.
		Remember: HashSet internally uses HashMap.
		Duplicate detection depends on: hashCode(), equals()

		ğŸ“Œ Example (Wrong Behavior)
				class User {
					String email;
				}

			User u1 = new User("a@gmail.com");
			User u2 = new User("a@gmail.com");
			Set<User> set = new HashSet<>();
			set.add(u1);
			set.add(u2);
			System.out.println(set.size());  // 2 âŒ

			Because: Default equals() compares reference
			Different objects â†’ treated as different

		âœ… Fix
			Override properly:
					@Override
					public boolean equals(Object o) {
						return this.email.equals(((User)o).email);
					}

					@Override
					public int hashCode() {
						return email.hashCode();
					}
ğŸ”¥ 2ï¸âƒ£ Mutable Objects Used in HashSet
	Another very dangerous scenario.
	If key fields change after insertion: 
		user.email = "new@gmail.com";
	Hash code changes.
	Now: Object is stored in wrong bucket, Contains() may return false, Removal may fail
	Set size looks correct but behavior broken.

ğŸ”¥ 3ï¸âƒ£ Inconsistent equals() and hashCode()
	Rule: If: equals() == true Then: hashCode() must be same
	If violated: Duplicate detection fails, Set size increases unexpectedly

ğŸ”¥ 4ï¸âƒ£ Using Custom Comparator in TreeSet (Not HashSet)
	Sometimes confusion happens:
		In TreeSet: Duplicate detection depends on: compareTo() == 0 Not equals().
		If compareTo inconsistent: Size may seem wrong

ğŸ”¥ 5ï¸âƒ£ Concurrency Issues
	If using HashSet in multi-threaded environment: set.add(x);
	Without synchronization: Race conditions, Corrupted structure, Incorrect size, HashSet is NOT thread-safe.

ğŸ”¥ 6ï¸âƒ£ Hash Collision Edge Cases
	If many objects return same hashCode(): return 1 => Performance degrades. But size remains correct.
	Still may cause confusion in debugging.

ğŸ”¥ Real Production Example
	Common bug:
		Set<Order> processedOrders = new HashSet<>();
		Order object includes: mutable status, mutable timestamp, If those fields used in hashCode:
		After update â†’ set broken

ğŸ”¥ Debugging Checklist
	When HashSet size looks wrong:
		1ï¸âƒ£ Check equals() implementation
		2ï¸âƒ£ Check hashCode() implementation
		3ï¸âƒ£ Check immutability of fields
		4ï¸âƒ£ Check multi-threaded access
		5ï¸âƒ£ Check null handling
=============================================================================================================
ğŸ”¹ Q80. Why TreeSet may silently drop elements?
=============================================================================================================
ğŸ”¥ Why TreeSet May Silently Drop Elements?
	Because: TreeSet does NOT use equals() to determine duplicates, It uses compareTo() (or Comparator)
	If: compare(a, b) == 0
		TreeSet treats them as duplicates
		Even if: a.equals(b) == false
		So second element is silently ignored.

ğŸ”¥ How TreeSet Internally Works
	TreeSet is backed by: TreeMap
	Internally:
		private transient NavigableMap<E,Object> m;
		Insertion logic: if (compare(key, existingKey) == 0) do not insert
		
	ğŸ”¥ Example 1: Wrong compareTo()
			class Person implements Comparable<Person> {
				int age;
				String name;

				public int compareTo(Person p) {
					return this.age - p.age;
				}
			}

		Now:
			Person p1 = new Person(25, "John");
			Person p2 = new Person(25, "David");
			Set<Person> set = new TreeSet<>();
			set.add(p1);
			set.add(p2);
			System.out.println(set.size());  // 1 âŒ
		
		Why?
			Because: compareTo() returns 0
			TreeSet treats them as same element.
			Even though: p1.equals(p2) == false

	ğŸ”¥ Example 2: Bad Comparator
		Comparator<String> comp = (a, b) -> a.length() - b.length();
		TreeSet<String> set = new TreeSet<>(comp);
		set.add("AA");
		set.add("BB");
		System.out.println(set.size());  // 1 âŒ
		Because both length = 2 â†’ comparator returns 0.
	
	ğŸ”¥ Why It Happens Silently?
		TreeSet: Calls compare(), If result == 0, Simply ignores insertion, No exception thrown, This is by design.
		
ğŸ”¥ Important Rule
	Java docs say: Comparator should be consistent with equals.
	Meaning: compare(a,b) == 0  â†’  a.equals(b) should be true
	If violated: Silent drops, Logical corruption, Unexpected behavior

ğŸ”¥ Difference from HashSet
| Feature          | HashSet           | TreeSet                      |
| ---------------- | ----------------- | ---------------------------- |
| Duplicate check  | equals()          | compareTo()/Comparator       |
| Silent drop risk | Rare (bad equals) | Very common (bad comparator) |

ğŸ”¥ Real Production Example
	Very common mistake:
		Sorting by: Comparator.comparing(Person::getAge)
		But two persons same age but different ID.
		TreeSet will drop one.
		Better comparator:
			Comparator.comparing(Person::getAge).thenComparing(Person::getId)

ğŸ”¥ How to Fix
	Always ensure:
		1ï¸âƒ£ Comparator includes all fields defining uniqueness
		2ï¸âƒ£ compareTo consistent with equals
		3ï¸âƒ£ No mutable fields used in comparison